{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MeanIoU\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#unet collection\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#from keras_unet_collection import models\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#import tensorflow as tf\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "# Use DL_Track_US env\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "#unet collection\n",
    "#from keras_unet_collection import models\n",
    "from models import *\n",
    "#import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with GPU support: True\n",
      "Number of GPUs available: 1\n",
      "GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow is built with GPU support\n",
    "print(\"Built with GPU support:\", tf.test.is_built_with_cuda())\n",
    "\n",
    "# List available physical devices (CPU/GPU)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    print(f\"Number of GPUs available: {len(physical_devices)}\")\n",
    "    for i, gpu in enumerate(physical_devices):\n",
    "        print(f\"GPU {i}: {gpu}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3080 Ti Laptop GPU, compute capability 8.6\n",
      "Mixed precision policy set to: <Policy \"mixed_float16\">\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Set the mixed precision policy to 'mixed_float16'\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Print the current policy to confirm\n",
    "print(\"Mixed precision policy set to:\", mixed_precision.global_policy())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977\n",
      "1977\n"
     ]
    }
   ],
   "source": [
    "#define directory where images and masks are located on local disk\n",
    "image_directory = 'C:/Users/admin/Documents/DeepACSA/temporary/BFRHL_model/apo_image_csa_VL/insert_images/' ##VGG16 needs seperate induction of path\n",
    "mask_directory = 'C:/Users/admin/Documents/DeepACSA/temporary/BFRHL_model/apo_masks_csa_VL/insert_masks/'   ##VGG16 need seperate induction of path\n",
    "#define the properties and empty list for resized images and masks\n",
    "SIZE = 256\n",
    "image_dataset = []\n",
    "mask_dataset = []\n",
    "\n",
    "#define custom function\n",
    "def IoU(y_true, y_pred, dtype=tf.float32):\n",
    "    y_pred = tf.cast(y_pred, dtype)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    y_pred = tf.squeeze(y_pred)\n",
    "    y_true = tf.squeeze(y_true)\n",
    "\n",
    "    y_true_pos = tf.reshape(y_true, [-1])\n",
    "    y_pred_pos = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    area_intersect = tf.reduce_sum(tf.multiply(y_true_pos, y_pred_pos))\n",
    "    \n",
    "    area_true = tf.reduce_sum(y_true_pos)\n",
    "    area_pred = tf.reduce_sum(y_pred_pos)\n",
    "    area_union = area_true + area_pred - area_intersect\n",
    "    \n",
    "    # Return the IoU score\n",
    "    return tf.math.divide_no_nan(area_intersect, area_union)\n",
    "\n",
    "def iou_seg(y_true, y_pred, dtype=tf.float32):\n",
    "    y_pred = tf.cast(y_pred, dtype)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    y_pred = tf.squeeze(y_pred)\n",
    "    y_true = tf.squeeze(y_true)\n",
    "\n",
    "    y_true_pos = tf.reshape(y_true, [-1])\n",
    "    y_pred_pos = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    area_intersect = tf.reduce_sum(tf.multiply(y_true_pos, y_pred_pos))\n",
    "    \n",
    "    area_true = tf.reduce_sum(y_true_pos)\n",
    "    area_pred = tf.reduce_sum(y_pred_pos)\n",
    "    area_union = area_true + area_pred - area_intersect\n",
    "    \n",
    "    # IoU score\n",
    "    iou = tf.math.divide_no_nan(area_intersect, area_union)\n",
    "\n",
    "    # IoU loss (1 - IoU)\n",
    "    return 1 - iou\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1 - score\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \n",
    "    return 1 - dice_score(y_true, y_pred)\n",
    "\n",
    "def dice_bce_score(y_true, y_pred, smooth=1):    \n",
    "    \n",
    "    Dice_BCE = K.binary_crossentropy(y_true, y_pred)   + dice_loss(y_true, y_pred)\n",
    "    \n",
    "    return Dice_BCE\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.8, gamma=2):    \n",
    "      \n",
    "    BCE = K.binary_crossentropy(y_true, y_pred)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    return focal_loss\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n",
    "    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n",
    "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
    "\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
    "    tv = tversky(y_true, y_pred)\n",
    "    return K.pow((1 - tv), gamma)\n",
    "\n",
    "# Plot sample of model prediction\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(30, 20))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='Greys_r')\n",
    "    \n",
    "    ax[0].set_title('US-image', c=\"white\" )\n",
    "    ax[0].grid(False)\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), cmap='Greys_r')\n",
    "    ax[1].set_title('Aponeurosis', c=\"white\")\n",
    "    ax[1].grid(False)\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[2].set_title('Apo-Predicted', c=\"white\")\n",
    "    ax[2].grid(False)\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=0.5, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[3].set_title('Apo-Picture binary', c=\"white\")\n",
    "    ax[3].grid(False)\n",
    "    \n",
    "    plt.savefig(str(ix)+\"Pred_area.tif\")\n",
    "\n",
    "# Save all predictions on disk \n",
    "def save_pred_area(binary_preds): \n",
    "    for i in range(len(binary_preds)): \n",
    "        fig, (ax1)= plt.subplots(1, 1, figsize = (15, 15))\n",
    "        ax1.imshow(binary_preds[i], cmap=\"Greys_r\", interpolation=\"bilinear\")\n",
    "        ax1.set_title(\"Predicted Area\")\n",
    "        plt.savefig(str(i)+\"Pred_area.tif\") # Saves images to directory of notebook\n",
    "\n",
    "#enumerate and resize images/masks\n",
    "masks = os.listdir(mask_directory)\n",
    "for i, image_name in enumerate(masks):\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        image = cv2.imread(mask_directory+image_name, 0)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        image = np.array(image).astype('float32')\n",
    "        mask_dataset.append(np.array(image))\n",
    "\n",
    "images = os.listdir(image_directory)\n",
    "for i, image_name in enumerate(images):    #enumerate method adds a counter and returns the enumerate object\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        #print(image_directory+image_name)\n",
    "        image = cv2.imread(image_directory+image_name, 1)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        image = np.array(image).astype('float32')\n",
    "        image_dataset.append(np.array(image))\n",
    "\n",
    "\n",
    "\n",
    "#define some hyperparameters\n",
    "num_labels = 1  #Binary classificaion\n",
    "batch_size = 1  #keep it smaller than 3\n",
    "epochs = 60 #60 \n",
    "num_folds = 5   #define the number of folds (usually 5-10 folds)\n",
    "\n",
    "#normalize images\n",
    "image_dataset = np.array(image_dataset)/255\n",
    "print(len(image_dataset))\n",
    "#do not normalize masks, just rescale to 0 to 1. Add RGB-Chanel (3) to mask.\n",
    "mask_dataset = np.expand_dims((np.array(mask_dataset)),3)/255\n",
    "print(len(mask_dataset))\n",
    "\n",
    "#define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, random_state= 42 ,shuffle=True)\n",
    "\n",
    "#define per-fold score containers \n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "IoU_per_fold = []\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold and create csv file\n",
    "def best_fold(searchterm):\n",
    "    fo_path = os.getcwd() \n",
    "    max_val_iou = 0.0\n",
    "    fold = \"\"\n",
    "    for file_name in os.listdir(fo_path):\n",
    "        if searchterm in file_name and file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(fo_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(file_path)\n",
    "            if \"val_IoU\" in df.columns:\n",
    "                val_iou = df[\"val_IoU\"].max()\n",
    "            if val_iou > max_val_iou:\n",
    "                max_val_iou = val_iou\n",
    "                fold = file_name\n",
    "\n",
    "    #save the results to a CSV file\n",
    "    results = pd.DataFrame({\"fold\": [fold], \"max_val_iou\": [max_val_iou]})\n",
    "    results.to_csv(f\"{searchterm}results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to plot a random image with its corresponding mask overlay\n",
    "def plot_random_image_with_mask_overlay(x_train, y_train):\n",
    "    # Flatten the list of all images and masks for easy random selection\n",
    "\n",
    "    random_index = random.randint(min(range(len(x_train))), max(range(len(x_train))))     \n",
    "    # Retrieve the corresponding image and mask\n",
    "    random_image = x_train[random_index]  # Select the middle frame from the stack\n",
    "    random_mask = y_train[random_index]  # Assume a single channel mask\n",
    "    \n",
    "    # Plot the image with its mask overlay\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(random_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(random_image, cmap='gray')\n",
    "    plt.imshow(random_mask, cmap='gray', alpha=0.3)  # Overlay the mask with some transparency\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot an image with its mask overlay\n",
    "plot_random_image_with_mask_overlay(image_dataset, mask_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input image & mask folder variables and prefix for saved augmented images\n",
    "# Change before augmentation\n",
    "input_img_folder = 'C:/Users/admin/Documents/DeepACSA/temporary/BFRHL_model/apo_image_csa_VL/insert_images/'\n",
    "input_mask_folder = 'C:/Users/admin/Documents/DeepACSA/temporary/BFRHL_model/apo_masks_csa_VL/insert_masks/'\n",
    "img_aug_prefix = \"aug\"\n",
    "\n",
    "\n",
    "# Creating image augmentation function\n",
    "gen = ImageDataGenerator(rotation_range=10, \n",
    "                        width_shift_range=0.01, \n",
    "                        height_shift_range=0.01,\n",
    "                        zoom_range=0.05,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "ids = os.listdir(input_img_folder)\n",
    "seed = 42\n",
    "batch_size = 2\n",
    "num_aug_images = 2 # Number of images added from augmented dataset. \n",
    "\n",
    "\n",
    "for i in range(int(len(ids))):\n",
    "    \n",
    "    # Choose image & mask that should be augmented \n",
    "    # Directory structur: \"root/some_dorectory/\"\n",
    "    chosen_image = ids[i] \n",
    "    image_path = input_img_folder + chosen_image \n",
    "    mask_path = input_mask_folder + chosen_image\n",
    "    image = np.expand_dims(plt.imread(image_path),0)# Read and expand image dimensions\n",
    "    if image.ndim < 4: \n",
    "        image = np.expand_dims(image,-1)\n",
    "    mask = np.expand_dims(plt.imread(mask_path),0)\n",
    "    if mask.ndim < 4: \n",
    "        mask = np.expand_dims(mask,-1)\n",
    "\n",
    "    # Augment images \n",
    "    aug_image = gen.flow(image, batch_size=batch_size, seed=seed, save_to_dir=input_img_folder, save_prefix=img_aug_prefix+str(i), save_format=\"tif\")\n",
    "    aug_mask = gen.flow(mask, batch_size=batch_size, seed=seed, save_to_dir=input_mask_folder, save_prefix=img_aug_prefix+str(i), save_format=\"tif\")\n",
    "    seed = seed + 1 \n",
    "     \n",
    "    # Add images to folder\n",
    "    for i in range(num_aug_images):\n",
    "        next(aug_image)[0].astype(np.uint8)\n",
    "        next(aug_mask)[0].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class PredictionVisualizer(Callback):\n",
    "    def __init__(self, validation_data, n_images=5):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.n_images = n_images\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_images, val_masks = self.validation_data\n",
    "        predictions = self.model.predict(val_images)\n",
    "\n",
    "        # Select a few random samples\n",
    "        indices = np.random.choice(len(val_images), self.n_images, replace=False)\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        for i, idx in enumerate(indices):\n",
    "            plt.subplot(3, self.n_images, i + 1)\n",
    "            plt.imshow(val_images[idx], cmap='gray')\n",
    "            plt.title(\"Input Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(3, self.n_images, i + 1 + self.n_images)\n",
    "            plt.imshow(val_masks[idx, ..., 0], cmap='gray')\n",
    "            plt.title(\"Ground Truth Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(3, self.n_images, i + 1 + 2 * self.n_images)\n",
    "            plt.imshow(predictions[idx, ..., 0], cmap='gray')\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'epoch_{epoch + 1}_predictions.png')\n",
    "        plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with unet_2plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works now but needs at least 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras_unet_collection\\_backbone_zoo.py:45: UserWarning: \n",
      "\n",
      "Backbone VGG19 does not use batch norm, but other layers received batch_norm=True\n",
      "  warnings.warn(param_mismatch);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9337 - IoU: 0.2873\n",
      "Epoch 1: val_loss improved from inf to 0.12234, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 69s 81ms/step - loss: 0.2483 - accuracy: 0.9337 - IoU: 0.2873 - val_loss: 0.1223 - val_accuracy: 0.9688 - val_IoU: 0.4115 - lr: 1.0000e-05\n",
      "Epoch 2/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9690 - IoU: 0.4178\n",
      "Epoch 2: val_loss improved from 0.12234 to 0.10261, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 64s 81ms/step - loss: 0.1195 - accuracy: 0.9690 - IoU: 0.4178 - val_loss: 0.1026 - val_accuracy: 0.9718 - val_IoU: 0.4493 - lr: 1.0000e-05\n",
      "Epoch 3/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9725 - IoU: 0.4697\n",
      "Epoch 3: val_loss improved from 0.10261 to 0.09062, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 64s 81ms/step - loss: 0.0966 - accuracy: 0.9725 - IoU: 0.4697 - val_loss: 0.0906 - val_accuracy: 0.9735 - val_IoU: 0.4911 - lr: 1.0000e-05\n",
      "Epoch 4/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9751 - IoU: 0.5145\n",
      "Epoch 4: val_loss improved from 0.09062 to 0.08215, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 64s 81ms/step - loss: 0.0819 - accuracy: 0.9751 - IoU: 0.5145 - val_loss: 0.0822 - val_accuracy: 0.9747 - val_IoU: 0.5300 - lr: 1.0000e-05\n",
      "Epoch 5/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9773 - IoU: 0.5567\n",
      "Epoch 5: val_loss improved from 0.08215 to 0.07409, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 65s 83ms/step - loss: 0.0704 - accuracy: 0.9773 - IoU: 0.5567 - val_loss: 0.0741 - val_accuracy: 0.9758 - val_IoU: 0.5688 - lr: 1.0000e-05\n",
      "Epoch 6/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9795 - IoU: 0.5988\n",
      "Epoch 6: val_loss improved from 0.07409 to 0.06732, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 66s 83ms/step - loss: 0.0604 - accuracy: 0.9795 - IoU: 0.5988 - val_loss: 0.0673 - val_accuracy: 0.9767 - val_IoU: 0.6045 - lr: 1.0000e-05\n",
      "Epoch 7/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 0.9815 - IoU: 0.6387\n",
      "Epoch 7: val_loss improved from 0.06732 to 0.06496, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0520 - accuracy: 0.9815 - IoU: 0.6387 - val_loss: 0.0650 - val_accuracy: 0.9768 - val_IoU: 0.6334 - lr: 1.0000e-05\n",
      "Epoch 8/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9831 - IoU: 0.6754\n",
      "Epoch 8: val_loss improved from 0.06496 to 0.05975, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 66s 83ms/step - loss: 0.0451 - accuracy: 0.9831 - IoU: 0.6754 - val_loss: 0.0598 - val_accuracy: 0.9776 - val_IoU: 0.6605 - lr: 1.0000e-05\n",
      "Epoch 9/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9840 - IoU: 0.7052\n",
      "Epoch 9: val_loss did not improve from 0.05975\n",
      "791/791 [==============================] - 66s 83ms/step - loss: 0.0401 - accuracy: 0.9840 - IoU: 0.7052 - val_loss: 0.0634 - val_accuracy: 0.9768 - val_IoU: 0.6704 - lr: 1.0000e-05\n",
      "Epoch 10/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9846 - IoU: 0.7305\n",
      "Epoch 10: val_loss did not improve from 0.05975\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0362 - accuracy: 0.9846 - IoU: 0.7305 - val_loss: 0.0621 - val_accuracy: 0.9765 - val_IoU: 0.6826 - lr: 1.0000e-05\n",
      "Epoch 11/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0331 - accuracy: 0.9850 - IoU: 0.7518\n",
      "Epoch 11: val_loss improved from 0.05975 to 0.05647, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0331 - accuracy: 0.9850 - IoU: 0.7519 - val_loss: 0.0565 - val_accuracy: 0.9771 - val_IoU: 0.6862 - lr: 1.0000e-05\n",
      "Epoch 12/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9851 - IoU: 0.7674\n",
      "Epoch 12: val_loss improved from 0.05647 to 0.05213, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0311 - accuracy: 0.9851 - IoU: 0.7674 - val_loss: 0.0521 - val_accuracy: 0.9781 - val_IoU: 0.7119 - lr: 1.0000e-05\n",
      "Epoch 13/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9858 - IoU: 0.7883\n",
      "Epoch 13: val_loss improved from 0.05213 to 0.04980, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0280 - accuracy: 0.9858 - IoU: 0.7883 - val_loss: 0.0498 - val_accuracy: 0.9787 - val_IoU: 0.7291 - lr: 1.0000e-05\n",
      "Epoch 14/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0248 - accuracy: 0.9867 - IoU: 0.8099\n",
      "Epoch 14: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 66s 83ms/step - loss: 0.0248 - accuracy: 0.9867 - IoU: 0.8100 - val_loss: 0.0514 - val_accuracy: 0.9783 - val_IoU: 0.7365 - lr: 1.0000e-05\n",
      "Epoch 15/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0232 - accuracy: 0.9869 - IoU: 0.8228\n",
      "Epoch 15: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0232 - accuracy: 0.9869 - IoU: 0.8228 - val_loss: 0.0530 - val_accuracy: 0.9779 - val_IoU: 0.7456 - lr: 1.0000e-05\n",
      "Epoch 16/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9870 - IoU: 0.8332\n",
      "Epoch 16: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0220 - accuracy: 0.9870 - IoU: 0.8332 - val_loss: 0.0519 - val_accuracy: 0.9783 - val_IoU: 0.7640 - lr: 1.0000e-05\n",
      "Epoch 17/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9873 - IoU: 0.8428\n",
      "Epoch 17: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0207 - accuracy: 0.9873 - IoU: 0.8428 - val_loss: 0.0503 - val_accuracy: 0.9790 - val_IoU: 0.7736 - lr: 1.0000e-05\n",
      "Epoch 18/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9875 - IoU: 0.8519\n",
      "Epoch 18: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0196 - accuracy: 0.9875 - IoU: 0.8519 - val_loss: 0.0561 - val_accuracy: 0.9780 - val_IoU: 0.7721 - lr: 1.0000e-05\n",
      "Epoch 19/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9877 - IoU: 0.8601\n",
      "Epoch 19: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0185 - accuracy: 0.9877 - IoU: 0.8602 - val_loss: 0.0527 - val_accuracy: 0.9786 - val_IoU: 0.7867 - lr: 1.0000e-05\n",
      "Epoch 20/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9881 - IoU: 0.8706\n",
      "Epoch 20: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0171 - accuracy: 0.9881 - IoU: 0.8706 - val_loss: 0.0632 - val_accuracy: 0.9765 - val_IoU: 0.7744 - lr: 1.0000e-05\n",
      "Epoch 21/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9881 - IoU: 0.8745\n",
      "Epoch 21: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0167 - accuracy: 0.9881 - IoU: 0.8745 - val_loss: 0.0595 - val_accuracy: 0.9770 - val_IoU: 0.7779 - lr: 1.0000e-05\n",
      "Epoch 22/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9885 - IoU: 0.8827\n",
      "Epoch 22: val_loss did not improve from 0.04980\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0156 - accuracy: 0.9885 - IoU: 0.8827 - val_loss: 0.0660 - val_accuracy: 0.9760 - val_IoU: 0.7696 - lr: 1.0000e-05\n",
      "Epoch 23/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9884 - IoU: 0.8852\n",
      "Epoch 23: val_loss improved from 0.04980 to 0.04770, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0154 - accuracy: 0.9884 - IoU: 0.8852 - val_loss: 0.0477 - val_accuracy: 0.9798 - val_IoU: 0.8030 - lr: 1.0000e-05\n",
      "Epoch 24/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9885 - IoU: 0.8875\n",
      "Epoch 24: val_loss did not improve from 0.04770\n",
      "791/791 [==============================] - 66s 83ms/step - loss: 0.0152 - accuracy: 0.9885 - IoU: 0.8875 - val_loss: 0.0508 - val_accuracy: 0.9794 - val_IoU: 0.8022 - lr: 1.0000e-05\n",
      "Epoch 25/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9886 - IoU: 0.8918\n",
      "Epoch 25: val_loss improved from 0.04770 to 0.04691, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0146 - accuracy: 0.9886 - IoU: 0.8918 - val_loss: 0.0469 - val_accuracy: 0.9803 - val_IoU: 0.8109 - lr: 1.0000e-05\n",
      "Epoch 26/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9887 - IoU: 0.8937\n",
      "Epoch 26: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 66s 83ms/step - loss: 0.0144 - accuracy: 0.9887 - IoU: 0.8937 - val_loss: 0.0474 - val_accuracy: 0.9806 - val_IoU: 0.8158 - lr: 1.0000e-05\n",
      "Epoch 27/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9888 - IoU: 0.8977\n",
      "Epoch 27: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0138 - accuracy: 0.9888 - IoU: 0.8977 - val_loss: 0.0526 - val_accuracy: 0.9799 - val_IoU: 0.8075 - lr: 1.0000e-05\n",
      "Epoch 28/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9890 - IoU: 0.9014\n",
      "Epoch 28: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0134 - accuracy: 0.9890 - IoU: 0.9014 - val_loss: 0.0518 - val_accuracy: 0.9803 - val_IoU: 0.8155 - lr: 1.0000e-05\n",
      "Epoch 29/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9890 - IoU: 0.9019\n",
      "Epoch 29: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0133 - accuracy: 0.9890 - IoU: 0.9019 - val_loss: 0.0501 - val_accuracy: 0.9803 - val_IoU: 0.8205 - lr: 1.0000e-05\n",
      "Epoch 30/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9891 - IoU: 0.9052\n",
      "Epoch 30: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0128 - accuracy: 0.9891 - IoU: 0.9052 - val_loss: 0.0530 - val_accuracy: 0.9800 - val_IoU: 0.8188 - lr: 1.0000e-05\n",
      "Epoch 31/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9891 - IoU: 0.9054\n",
      "Epoch 31: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0129 - accuracy: 0.9891 - IoU: 0.9054 - val_loss: 0.0514 - val_accuracy: 0.9799 - val_IoU: 0.8168 - lr: 1.0000e-05\n",
      "Epoch 32/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9892 - IoU: 0.9069\n",
      "Epoch 32: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0126 - accuracy: 0.9892 - IoU: 0.9069 - val_loss: 0.0510 - val_accuracy: 0.9803 - val_IoU: 0.8171 - lr: 1.0000e-05\n",
      "Epoch 33/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9894 - IoU: 0.9117\n",
      "Epoch 33: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0120 - accuracy: 0.9894 - IoU: 0.9117 - val_loss: 0.0516 - val_accuracy: 0.9802 - val_IoU: 0.8190 - lr: 1.0000e-05\n",
      "Epoch 34/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9894 - IoU: 0.9134\n",
      "Epoch 34: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0118 - accuracy: 0.9894 - IoU: 0.9134 - val_loss: 0.0509 - val_accuracy: 0.9804 - val_IoU: 0.8202 - lr: 1.0000e-05\n",
      "Epoch 35/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9896 - IoU: 0.9168\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.04691\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0112 - accuracy: 0.9896 - IoU: 0.9168 - val_loss: 0.0563 - val_accuracy: 0.9800 - val_IoU: 0.8148 - lr: 1.0000e-05\n",
      "Epoch 36/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9894 - IoU: 0.9130\n",
      "Epoch 36: val_loss improved from 0.04691 to 0.04300, saving model to K-foldno1-unet2plus-VL-V1-BFRHL.h5\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0119 - accuracy: 0.9894 - IoU: 0.9130 - val_loss: 0.0430 - val_accuracy: 0.9822 - val_IoU: 0.8372 - lr: 1.0000e-06\n",
      "Epoch 37/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9900 - IoU: 0.9218\n",
      "Epoch 37: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0102 - accuracy: 0.9900 - IoU: 0.9218 - val_loss: 0.0439 - val_accuracy: 0.9823 - val_IoU: 0.8410 - lr: 1.0000e-06\n",
      "Epoch 38/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9902 - IoU: 0.9280\n",
      "Epoch 38: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0093 - accuracy: 0.9902 - IoU: 0.9280 - val_loss: 0.0451 - val_accuracy: 0.9824 - val_IoU: 0.8439 - lr: 1.0000e-06\n",
      "Epoch 39/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9904 - IoU: 0.9329\n",
      "Epoch 39: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0087 - accuracy: 0.9904 - IoU: 0.9329 - val_loss: 0.0463 - val_accuracy: 0.9825 - val_IoU: 0.8461 - lr: 1.0000e-06\n",
      "Epoch 40/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9906 - IoU: 0.9369\n",
      "Epoch 40: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0081 - accuracy: 0.9906 - IoU: 0.9369 - val_loss: 0.0475 - val_accuracy: 0.9825 - val_IoU: 0.8477 - lr: 1.0000e-06\n",
      "Epoch 41/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9907 - IoU: 0.9403\n",
      "Epoch 41: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0077 - accuracy: 0.9907 - IoU: 0.9403 - val_loss: 0.0486 - val_accuracy: 0.9826 - val_IoU: 0.8492 - lr: 1.0000e-06\n",
      "Epoch 42/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9908 - IoU: 0.9433\n",
      "Epoch 42: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0073 - accuracy: 0.9908 - IoU: 0.9433 - val_loss: 0.0497 - val_accuracy: 0.9826 - val_IoU: 0.8503 - lr: 1.0000e-06\n",
      "Epoch 43/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9909 - IoU: 0.9459\n",
      "Epoch 43: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0069 - accuracy: 0.9909 - IoU: 0.9459 - val_loss: 0.0508 - val_accuracy: 0.9826 - val_IoU: 0.8514 - lr: 1.0000e-06\n",
      "Epoch 44/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9909 - IoU: 0.9482\n",
      "Epoch 44: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0066 - accuracy: 0.9909 - IoU: 0.9482 - val_loss: 0.0519 - val_accuracy: 0.9826 - val_IoU: 0.8522 - lr: 1.0000e-06\n",
      "Epoch 45/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9910 - IoU: 0.9502\n",
      "Epoch 45: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0064 - accuracy: 0.9910 - IoU: 0.9502 - val_loss: 0.0529 - val_accuracy: 0.9826 - val_IoU: 0.8530 - lr: 1.0000e-06\n",
      "Epoch 46/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9910 - IoU: 0.9520\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0061 - accuracy: 0.9910 - IoU: 0.9520 - val_loss: 0.0540 - val_accuracy: 0.9826 - val_IoU: 0.8535 - lr: 1.0000e-06\n",
      "Epoch 47/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9909 - IoU: 0.9515\n",
      "Epoch 47: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0064 - accuracy: 0.9909 - IoU: 0.9515 - val_loss: 0.0534 - val_accuracy: 0.9827 - val_IoU: 0.8550 - lr: 1.0000e-07\n",
      "Epoch 48/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9910 - IoU: 0.9521\n",
      "Epoch 48: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0062 - accuracy: 0.9910 - IoU: 0.9521 - val_loss: 0.0535 - val_accuracy: 0.9827 - val_IoU: 0.8551 - lr: 1.0000e-07\n",
      "Epoch 49/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9910 - IoU: 0.9524\n",
      "Epoch 49: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 68s 85ms/step - loss: 0.0061 - accuracy: 0.9910 - IoU: 0.9524 - val_loss: 0.0535 - val_accuracy: 0.9827 - val_IoU: 0.8551 - lr: 1.0000e-07\n",
      "Epoch 50/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9910 - IoU: 0.9527\n",
      "Epoch 50: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 66s 84ms/step - loss: 0.0061 - accuracy: 0.9910 - IoU: 0.9527 - val_loss: 0.0536 - val_accuracy: 0.9827 - val_IoU: 0.8552 - lr: 1.0000e-07\n",
      "Epoch 51/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9910 - IoU: 0.9529\n",
      "Epoch 51: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0060 - accuracy: 0.9910 - IoU: 0.9529 - val_loss: 0.0537 - val_accuracy: 0.9827 - val_IoU: 0.8552 - lr: 1.0000e-07\n",
      "Epoch 52/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9910 - IoU: 0.9531\n",
      "Epoch 52: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0060 - accuracy: 0.9910 - IoU: 0.9531 - val_loss: 0.0537 - val_accuracy: 0.9827 - val_IoU: 0.8553 - lr: 1.0000e-07\n",
      "Epoch 53/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9910 - IoU: 0.9534\n",
      "Epoch 53: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0060 - accuracy: 0.9910 - IoU: 0.9534 - val_loss: 0.0538 - val_accuracy: 0.9827 - val_IoU: 0.8553 - lr: 1.0000e-07\n",
      "Epoch 54/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9910 - IoU: 0.9536\n",
      "Epoch 54: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0059 - accuracy: 0.9910 - IoU: 0.9536 - val_loss: 0.0539 - val_accuracy: 0.9827 - val_IoU: 0.8554 - lr: 1.0000e-07\n",
      "Epoch 55/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9910 - IoU: 0.9538\n",
      "Epoch 55: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 85ms/step - loss: 0.0059 - accuracy: 0.9910 - IoU: 0.9538 - val_loss: 0.0541 - val_accuracy: 0.9827 - val_IoU: 0.8555 - lr: 1.0000e-07\n",
      "Epoch 56/200\n",
      "790/791 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9910 - IoU: 0.9540\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.04300\n",
      "791/791 [==============================] - 67s 84ms/step - loss: 0.0059 - accuracy: 0.9910 - IoU: 0.9540 - val_loss: 0.0542 - val_accuracy: 0.9827 - val_IoU: 0.8555 - lr: 1.0000e-07\n",
      "Epoch 56: early stopping\n",
      "13/13 [==============================] - 14s 510ms/step - loss: 0.0542 - accuracy: 0.9827 - IoU: 0.8576\n",
      "13/13 [==============================] - 4s 290ms/step - loss: 0.0542 - accuracy: 0.9827 - IoU: 0.8576\n",
      "13/13 [==============================] - 4s 286ms/step - loss: 0.0542 - accuracy: 0.9827 - IoU: 0.8576\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/200\n",
      "791/791 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.9121 - IoU: 0.2553\n",
      "Epoch 1: val_loss improved from inf to 0.16172, saving model to K-foldno2-unet2plus-VL-V1-BFRHL.h5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't decrement id ref count (unable to extend file properly)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#fit model to data\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m Unet_plus_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#append evaluation values for every fold to a list\u001b[39;00m\n\u001b[0;32m     45\u001b[0m acc_per_fold\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mevaluate(image_dataset[test], mask_dataset[test])[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\h5py\\_hl\\files.py:586\u001b[0m, in \u001b[0;36mFile.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;66;03m# We have to explicitly murder all open objects related to the file\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# Close file-resident objects first, then the files.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;66;03m# Otherwise we get errors in MPI mode.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39m_close_open_objects(h5f\u001b[38;5;241m.\u001b[39mOBJ_LOCAL \u001b[38;5;241m|\u001b[39m \u001b[38;5;241m~\u001b[39mh5f\u001b[38;5;241m.\u001b[39mOBJ_FILE)\n\u001b[1;32m--> 586\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_close_open_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_LOCAL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJ_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    589\u001b[0m     _objects\u001b[38;5;241m.\u001b[39mnonlocal_close()\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:360\u001b[0m, in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't decrement id ref count (unable to extend file properly)"
     ]
    }
   ],
   "source": [
    "from keras_unet_collection.losses import focal_tversky\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "architecture = \"unet2plus_\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "       \n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=20, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00000001, verbose=1),\n",
    "    ModelCheckpoint(filepath=f\"K-foldno{fold_no}-unet2plus-VL-V1-BFRHL.h5\", verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f\"K-foldno{fold_no}-unet2plus-VL-V1-BFRHL.csv\", separator=\",\", append=False)]\n",
    "\n",
    "  #define the model architecture\n",
    "  #unet_plus_2d requires a Backbone\n",
    "  model = unet_plus_2d((256, 256, 3), filter_num=[64, 128, 256, 512, 1024], \n",
    "                           n_labels=1, \n",
    "                           stack_num_down=2, stack_num_up=2, \n",
    "                           activation=\"ReLU\", \n",
    "                           output_activation=\"Sigmoid\", \n",
    "                           batch_norm=True, pool=False, unpool=False, \n",
    "                           backbone=\"VGG19\", weights=\"imagenet\", \n",
    "                           freeze_backbone=True, freeze_batch_norm=True, \n",
    "                           name=\"xnet\")\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate= 1e-5), \n",
    "              metrics=[\"accuracy\", IoU])\n",
    "  \n",
    "  #generate a print\n",
    "  print(\"------------------------------------------------------------------------\")\n",
    "  print(f\"Training for fold {fold_no} ...\")\n",
    "\n",
    "  #fit model to data\n",
    "  Unet_plus_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = 2,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "  \n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "  clear_session()\n",
    "\n",
    "fold_no = 1\n",
    "#determine best fold\n",
    "best_fold(architecture)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with unet_3plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of convolution filters after each full-scale skip connection: filter_num_skip = [64, 64, 64, 64]\n",
      "\tNumber of channels of full-scale aggregated feature maps: filter_num_aggregate = 320\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tunet3plus_output_sup0_activation\n",
      "\tunet3plus_output_sup1_activation\n",
      "\tunet3plus_output_sup2_activation\n",
      "\tunet3plus_output_sup3_activation\n",
      "\tunet3plus_output_final_activation\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 2.4256 - unet3plus_output_sup0_activation_loss: 0.4570 - unet3plus_output_sup1_activation_loss: 0.6218 - unet3plus_output_sup2_activation_loss: 0.6476 - unet3plus_output_sup3_activation_loss: 0.6507 - unet3plus_output_final_activation_loss: 0.0486 - unet3plus_output_sup0_activation_accuracy: 0.7364 - unet3plus_output_sup0_activation_IoU: 0.1480 - unet3plus_output_sup1_activation_accuracy: 0.7284 - unet3plus_output_sup1_activation_IoU: 0.0865 - unet3plus_output_sup2_activation_accuracy: 0.8845 - unet3plus_output_sup2_activation_IoU: 0.0791 - unet3plus_output_sup3_activation_accuracy: 0.9088 - unet3plus_output_sup3_activation_IoU: 0.0777 - unet3plus_output_final_activation_accuracy: 0.9761 - unet3plus_output_final_activation_IoU: 0.7305\n",
      "Epoch 1: val_loss improved from inf to 2.16942, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 329s 206ms/step - loss: 2.4256 - unet3plus_output_sup0_activation_loss: 0.4570 - unet3plus_output_sup1_activation_loss: 0.6218 - unet3plus_output_sup2_activation_loss: 0.6476 - unet3plus_output_sup3_activation_loss: 0.6507 - unet3plus_output_final_activation_loss: 0.0486 - unet3plus_output_sup0_activation_accuracy: 0.7364 - unet3plus_output_sup0_activation_IoU: 0.1480 - unet3plus_output_sup1_activation_accuracy: 0.7284 - unet3plus_output_sup1_activation_IoU: 0.0865 - unet3plus_output_sup2_activation_accuracy: 0.8845 - unet3plus_output_sup2_activation_IoU: 0.0791 - unet3plus_output_sup3_activation_accuracy: 0.9088 - unet3plus_output_sup3_activation_IoU: 0.0777 - unet3plus_output_final_activation_accuracy: 0.9761 - unet3plus_output_final_activation_IoU: 0.7305 - val_loss: 2.1694 - val_unet3plus_output_sup0_activation_loss: 0.3473 - val_unet3plus_output_sup1_activation_loss: 0.5450 - val_unet3plus_output_sup2_activation_loss: 0.6063 - val_unet3plus_output_sup3_activation_loss: 0.6175 - val_unet3plus_output_final_activation_loss: 0.0533 - val_unet3plus_output_sup0_activation_accuracy: 0.7403 - val_unet3plus_output_sup0_activation_IoU: 0.2029 - val_unet3plus_output_sup1_activation_accuracy: 0.7689 - val_unet3plus_output_sup1_activation_IoU: 0.1031 - val_unet3plus_output_sup2_activation_accuracy: 0.8913 - val_unet3plus_output_sup2_activation_IoU: 0.0830 - val_unet3plus_output_sup3_activation_accuracy: 0.9107 - val_unet3plus_output_sup3_activation_IoU: 0.0787 - val_unet3plus_output_final_activation_accuracy: 0.9778 - val_unet3plus_output_final_activation_IoU: 0.7995 - lr: 1.0000e-04\n",
      "Epoch 2/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.8273 - unet3plus_output_sup0_activation_loss: 0.2029 - unet3plus_output_sup1_activation_loss: 0.4434 - unet3plus_output_sup2_activation_loss: 0.5668 - unet3plus_output_sup3_activation_loss: 0.5871 - unet3plus_output_final_activation_loss: 0.0271 - unet3plus_output_sup0_activation_accuracy: 0.8415 - unet3plus_output_sup0_activation_IoU: 0.3578 - unet3plus_output_sup1_activation_accuracy: 0.8465 - unet3plus_output_sup1_activation_IoU: 0.1300 - unet3plus_output_sup2_activation_accuracy: 0.8920 - unet3plus_output_sup2_activation_IoU: 0.0848 - unet3plus_output_sup3_activation_accuracy: 0.9136 - unet3plus_output_sup3_activation_IoU: 0.0784 - unet3plus_output_final_activation_accuracy: 0.9841 - unet3plus_output_final_activation_IoU: 0.8320\n",
      "Epoch 2: val_loss improved from 2.16942 to 1.48932, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 1.8273 - unet3plus_output_sup0_activation_loss: 0.2029 - unet3plus_output_sup1_activation_loss: 0.4434 - unet3plus_output_sup2_activation_loss: 0.5668 - unet3plus_output_sup3_activation_loss: 0.5871 - unet3plus_output_final_activation_loss: 0.0271 - unet3plus_output_sup0_activation_accuracy: 0.8415 - unet3plus_output_sup0_activation_IoU: 0.3578 - unet3plus_output_sup1_activation_accuracy: 0.8465 - unet3plus_output_sup1_activation_IoU: 0.1300 - unet3plus_output_sup2_activation_accuracy: 0.8920 - unet3plus_output_sup2_activation_IoU: 0.0848 - unet3plus_output_sup3_activation_accuracy: 0.9136 - unet3plus_output_sup3_activation_IoU: 0.0784 - unet3plus_output_final_activation_accuracy: 0.9841 - unet3plus_output_final_activation_IoU: 0.8320 - val_loss: 1.4893 - val_unet3plus_output_sup0_activation_loss: 0.0588 - val_unet3plus_output_sup1_activation_loss: 0.3076 - val_unet3plus_output_sup2_activation_loss: 0.5299 - val_unet3plus_output_sup3_activation_loss: 0.5602 - val_unet3plus_output_final_activation_loss: 0.0328 - val_unet3plus_output_sup0_activation_accuracy: 0.9813 - val_unet3plus_output_sup0_activation_IoU: 0.7335 - val_unet3plus_output_sup1_activation_accuracy: 0.9494 - val_unet3plus_output_sup1_activation_IoU: 0.1858 - val_unet3plus_output_sup2_activation_accuracy: 0.8931 - val_unet3plus_output_sup2_activation_IoU: 0.0876 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0783 - val_unet3plus_output_final_activation_accuracy: 0.9828 - val_unet3plus_output_final_activation_IoU: 0.8320 - lr: 1.0000e-04\n",
      "Epoch 3/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.3632 - unet3plus_output_sup0_activation_loss: 0.0361 - unet3plus_output_sup1_activation_loss: 0.2746 - unet3plus_output_sup2_activation_loss: 0.4927 - unet3plus_output_sup3_activation_loss: 0.5360 - unet3plus_output_final_activation_loss: 0.0237 - unet3plus_output_sup0_activation_accuracy: 0.9843 - unet3plus_output_sup0_activation_IoU: 0.7780 - unet3plus_output_sup1_activation_accuracy: 0.9521 - unet3plus_output_sup1_activation_IoU: 0.2078 - unet3plus_output_sup2_activation_accuracy: 0.9185 - unet3plus_output_sup2_activation_IoU: 0.0918 - unet3plus_output_sup3_activation_accuracy: 0.9149 - unet3plus_output_sup3_activation_IoU: 0.0780 - unet3plus_output_final_activation_accuracy: 0.9855 - unet3plus_output_final_activation_IoU: 0.8516\n",
      "Epoch 3: val_loss improved from 1.48932 to 1.32542, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 1.3632 - unet3plus_output_sup0_activation_loss: 0.0361 - unet3plus_output_sup1_activation_loss: 0.2746 - unet3plus_output_sup2_activation_loss: 0.4927 - unet3plus_output_sup3_activation_loss: 0.5360 - unet3plus_output_final_activation_loss: 0.0237 - unet3plus_output_sup0_activation_accuracy: 0.9843 - unet3plus_output_sup0_activation_IoU: 0.7780 - unet3plus_output_sup1_activation_accuracy: 0.9521 - unet3plus_output_sup1_activation_IoU: 0.2078 - unet3plus_output_sup2_activation_accuracy: 0.9185 - unet3plus_output_sup2_activation_IoU: 0.0918 - unet3plus_output_sup3_activation_accuracy: 0.9149 - unet3plus_output_sup3_activation_IoU: 0.0780 - unet3plus_output_final_activation_accuracy: 0.9855 - unet3plus_output_final_activation_IoU: 0.8516 - val_loss: 1.3254 - val_unet3plus_output_sup0_activation_loss: 0.0423 - val_unet3plus_output_sup1_activation_loss: 0.2724 - val_unet3plus_output_sup2_activation_loss: 0.4659 - val_unet3plus_output_sup3_activation_loss: 0.5137 - val_unet3plus_output_final_activation_loss: 0.0311 - val_unet3plus_output_sup0_activation_accuracy: 0.9816 - val_unet3plus_output_sup0_activation_IoU: 0.7588 - val_unet3plus_output_sup1_activation_accuracy: 0.9490 - val_unet3plus_output_sup1_activation_IoU: 0.2174 - val_unet3plus_output_sup2_activation_accuracy: 0.9214 - val_unet3plus_output_sup2_activation_IoU: 0.0948 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0777 - val_unet3plus_output_final_activation_accuracy: 0.9821 - val_unet3plus_output_final_activation_IoU: 0.8112 - lr: 1.0000e-04\n",
      "Epoch 4/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.2300 - unet3plus_output_sup0_activation_loss: 0.0253 - unet3plus_output_sup1_activation_loss: 0.2488 - unet3plus_output_sup2_activation_loss: 0.4448 - unet3plus_output_sup3_activation_loss: 0.4922 - unet3plus_output_final_activation_loss: 0.0190 - unet3plus_output_sup0_activation_accuracy: 0.9863 - unet3plus_output_sup0_activation_IoU: 0.8396 - unet3plus_output_sup1_activation_accuracy: 0.9532 - unet3plus_output_sup1_activation_IoU: 0.2246 - unet3plus_output_sup2_activation_accuracy: 0.9225 - unet3plus_output_sup2_activation_IoU: 0.0956 - unet3plus_output_sup3_activation_accuracy: 0.9150 - unet3plus_output_sup3_activation_IoU: 0.0773 - unet3plus_output_final_activation_accuracy: 0.9869 - unet3plus_output_final_activation_IoU: 0.8772\n",
      "Epoch 4: val_loss improved from 1.32542 to 1.22268, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 1.2300 - unet3plus_output_sup0_activation_loss: 0.0253 - unet3plus_output_sup1_activation_loss: 0.2488 - unet3plus_output_sup2_activation_loss: 0.4448 - unet3plus_output_sup3_activation_loss: 0.4922 - unet3plus_output_final_activation_loss: 0.0190 - unet3plus_output_sup0_activation_accuracy: 0.9863 - unet3plus_output_sup0_activation_IoU: 0.8396 - unet3plus_output_sup1_activation_accuracy: 0.9532 - unet3plus_output_sup1_activation_IoU: 0.2246 - unet3plus_output_sup2_activation_accuracy: 0.9225 - unet3plus_output_sup2_activation_IoU: 0.0956 - unet3plus_output_sup3_activation_accuracy: 0.9150 - unet3plus_output_sup3_activation_IoU: 0.0773 - unet3plus_output_final_activation_accuracy: 0.9869 - unet3plus_output_final_activation_IoU: 0.8772 - val_loss: 1.2227 - val_unet3plus_output_sup0_activation_loss: 0.0424 - val_unet3plus_output_sup1_activation_loss: 0.2481 - val_unet3plus_output_sup2_activation_loss: 0.4283 - val_unet3plus_output_sup3_activation_loss: 0.4726 - val_unet3plus_output_final_activation_loss: 0.0314 - val_unet3plus_output_sup0_activation_accuracy: 0.9827 - val_unet3plus_output_sup0_activation_IoU: 0.8058 - val_unet3plus_output_sup1_activation_accuracy: 0.9511 - val_unet3plus_output_sup1_activation_IoU: 0.2204 - val_unet3plus_output_sup2_activation_accuracy: 0.9218 - val_unet3plus_output_sup2_activation_IoU: 0.0951 - val_unet3plus_output_sup3_activation_accuracy: 0.9146 - val_unet3plus_output_sup3_activation_IoU: 0.0769 - val_unet3plus_output_final_activation_accuracy: 0.9839 - val_unet3plus_output_final_activation_IoU: 0.8373 - lr: 1.0000e-04\n",
      "Epoch 5/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.1319 - unet3plus_output_sup0_activation_loss: 0.0217 - unet3plus_output_sup1_activation_loss: 0.2290 - unet3plus_output_sup2_activation_loss: 0.4098 - unet3plus_output_sup3_activation_loss: 0.4537 - unet3plus_output_final_activation_loss: 0.0177 - unet3plus_output_sup0_activation_accuracy: 0.9870 - unet3plus_output_sup0_activation_IoU: 0.8607 - unet3plus_output_sup1_activation_accuracy: 0.9535 - unet3plus_output_sup1_activation_IoU: 0.2355 - unet3plus_output_sup2_activation_accuracy: 0.9226 - unet3plus_output_sup2_activation_IoU: 0.0968 - unet3plus_output_sup3_activation_accuracy: 0.9150 - unet3plus_output_sup3_activation_IoU: 0.0765 - unet3plus_output_final_activation_accuracy: 0.9874 - unet3plus_output_final_activation_IoU: 0.8844\n",
      "Epoch 5: val_loss improved from 1.22268 to 1.10896, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 1.1319 - unet3plus_output_sup0_activation_loss: 0.0217 - unet3plus_output_sup1_activation_loss: 0.2290 - unet3plus_output_sup2_activation_loss: 0.4098 - unet3plus_output_sup3_activation_loss: 0.4537 - unet3plus_output_final_activation_loss: 0.0177 - unet3plus_output_sup0_activation_accuracy: 0.9870 - unet3plus_output_sup0_activation_IoU: 0.8607 - unet3plus_output_sup1_activation_accuracy: 0.9535 - unet3plus_output_sup1_activation_IoU: 0.2355 - unet3plus_output_sup2_activation_accuracy: 0.9226 - unet3plus_output_sup2_activation_IoU: 0.0968 - unet3plus_output_sup3_activation_accuracy: 0.9150 - unet3plus_output_sup3_activation_IoU: 0.0765 - unet3plus_output_final_activation_accuracy: 0.9874 - unet3plus_output_final_activation_IoU: 0.8844 - val_loss: 1.1090 - val_unet3plus_output_sup0_activation_loss: 0.0286 - val_unet3plus_output_sup1_activation_loss: 0.2244 - val_unet3plus_output_sup2_activation_loss: 0.3950 - val_unet3plus_output_sup3_activation_loss: 0.4365 - val_unet3plus_output_final_activation_loss: 0.0245 - val_unet3plus_output_sup0_activation_accuracy: 0.9854 - val_unet3plus_output_sup0_activation_IoU: 0.8551 - val_unet3plus_output_sup1_activation_accuracy: 0.9525 - val_unet3plus_output_sup1_activation_IoU: 0.2390 - val_unet3plus_output_sup2_activation_accuracy: 0.9221 - val_unet3plus_output_sup2_activation_IoU: 0.0972 - val_unet3plus_output_sup3_activation_accuracy: 0.9147 - val_unet3plus_output_sup3_activation_IoU: 0.0763 - val_unet3plus_output_final_activation_accuracy: 0.9856 - val_unet3plus_output_final_activation_IoU: 0.8714 - lr: 1.0000e-04\n",
      "Epoch 6/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.0439 - unet3plus_output_sup0_activation_loss: 0.0183 - unet3plus_output_sup1_activation_loss: 0.2108 - unet3plus_output_sup2_activation_loss: 0.3791 - unet3plus_output_sup3_activation_loss: 0.4201 - unet3plus_output_final_activation_loss: 0.0157 - unet3plus_output_sup0_activation_accuracy: 0.9879 - unet3plus_output_sup0_activation_IoU: 0.8814 - unet3plus_output_sup1_activation_accuracy: 0.9540 - unet3plus_output_sup1_activation_IoU: 0.2478 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0981 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0756 - unet3plus_output_final_activation_accuracy: 0.9881 - unet3plus_output_final_activation_IoU: 0.8965\n",
      "Epoch 6: val_loss did not improve from 1.10896\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 1.0439 - unet3plus_output_sup0_activation_loss: 0.0183 - unet3plus_output_sup1_activation_loss: 0.2108 - unet3plus_output_sup2_activation_loss: 0.3791 - unet3plus_output_sup3_activation_loss: 0.4201 - unet3plus_output_final_activation_loss: 0.0157 - unet3plus_output_sup0_activation_accuracy: 0.9879 - unet3plus_output_sup0_activation_IoU: 0.8814 - unet3plus_output_sup1_activation_accuracy: 0.9540 - unet3plus_output_sup1_activation_IoU: 0.2478 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0981 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0756 - unet3plus_output_final_activation_accuracy: 0.9881 - unet3plus_output_final_activation_IoU: 0.8965 - val_loss: 1.1221 - val_unet3plus_output_sup0_activation_loss: 0.0744 - val_unet3plus_output_sup1_activation_loss: 0.2229 - val_unet3plus_output_sup2_activation_loss: 0.3688 - val_unet3plus_output_sup3_activation_loss: 0.4056 - val_unet3plus_output_final_activation_loss: 0.0503 - val_unet3plus_output_sup0_activation_accuracy: 0.9778 - val_unet3plus_output_sup0_activation_IoU: 0.7583 - val_unet3plus_output_sup1_activation_accuracy: 0.9490 - val_unet3plus_output_sup1_activation_IoU: 0.2275 - val_unet3plus_output_sup2_activation_accuracy: 0.9214 - val_unet3plus_output_sup2_activation_IoU: 0.0947 - val_unet3plus_output_sup3_activation_accuracy: 0.9146 - val_unet3plus_output_sup3_activation_IoU: 0.0747 - val_unet3plus_output_final_activation_accuracy: 0.9788 - val_unet3plus_output_final_activation_IoU: 0.7754 - lr: 1.0000e-04\n",
      "Epoch 7/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.9714 - unet3plus_output_sup0_activation_loss: 0.0168 - unet3plus_output_sup1_activation_loss: 0.1957 - unet3plus_output_sup2_activation_loss: 0.3527 - unet3plus_output_sup3_activation_loss: 0.3911 - unet3plus_output_final_activation_loss: 0.0152 - unet3plus_output_sup0_activation_accuracy: 0.9882 - unet3plus_output_sup0_activation_IoU: 0.8905 - unet3plus_output_sup1_activation_accuracy: 0.9542 - unet3plus_output_sup1_activation_IoU: 0.2589 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0990 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0745 - unet3plus_output_final_activation_accuracy: 0.9884 - unet3plus_output_final_activation_IoU: 0.9009\n",
      "Epoch 7: val_loss improved from 1.10896 to 0.95518, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.9714 - unet3plus_output_sup0_activation_loss: 0.0168 - unet3plus_output_sup1_activation_loss: 0.1957 - unet3plus_output_sup2_activation_loss: 0.3527 - unet3plus_output_sup3_activation_loss: 0.3911 - unet3plus_output_final_activation_loss: 0.0152 - unet3plus_output_sup0_activation_accuracy: 0.9882 - unet3plus_output_sup0_activation_IoU: 0.8905 - unet3plus_output_sup1_activation_accuracy: 0.9542 - unet3plus_output_sup1_activation_IoU: 0.2589 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0990 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0745 - unet3plus_output_final_activation_accuracy: 0.9884 - unet3plus_output_final_activation_IoU: 0.9009 - val_loss: 0.9552 - val_unet3plus_output_sup0_activation_loss: 0.0219 - val_unet3plus_output_sup1_activation_loss: 0.1925 - val_unet3plus_output_sup2_activation_loss: 0.3418 - val_unet3plus_output_sup3_activation_loss: 0.3785 - val_unet3plus_output_final_activation_loss: 0.0205 - val_unet3plus_output_sup0_activation_accuracy: 0.9866 - val_unet3plus_output_sup0_activation_IoU: 0.8748 - val_unet3plus_output_sup1_activation_accuracy: 0.9531 - val_unet3plus_output_sup1_activation_IoU: 0.2601 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.0983 - val_unet3plus_output_sup3_activation_accuracy: 0.9147 - val_unet3plus_output_sup3_activation_IoU: 0.0738 - val_unet3plus_output_final_activation_accuracy: 0.9868 - val_unet3plus_output_final_activation_IoU: 0.8810 - lr: 1.0000e-04\n",
      "Epoch 8/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.9099 - unet3plus_output_sup0_activation_loss: 0.0157 - unet3plus_output_sup1_activation_loss: 0.1830 - unet3plus_output_sup2_activation_loss: 0.3303 - unet3plus_output_sup3_activation_loss: 0.3663 - unet3plus_output_final_activation_loss: 0.0146 - unet3plus_output_sup0_activation_accuracy: 0.9883 - unet3plus_output_sup0_activation_IoU: 0.8957 - unet3plus_output_sup1_activation_accuracy: 0.9543 - unet3plus_output_sup1_activation_IoU: 0.2699 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0998 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0732 - unet3plus_output_final_activation_accuracy: 0.9885 - unet3plus_output_final_activation_IoU: 0.9030\n",
      "Epoch 8: val_loss improved from 0.95518 to 0.89852, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.9099 - unet3plus_output_sup0_activation_loss: 0.0157 - unet3plus_output_sup1_activation_loss: 0.1830 - unet3plus_output_sup2_activation_loss: 0.3303 - unet3plus_output_sup3_activation_loss: 0.3663 - unet3plus_output_final_activation_loss: 0.0146 - unet3plus_output_sup0_activation_accuracy: 0.9883 - unet3plus_output_sup0_activation_IoU: 0.8957 - unet3plus_output_sup1_activation_accuracy: 0.9543 - unet3plus_output_sup1_activation_IoU: 0.2699 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0998 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0732 - unet3plus_output_final_activation_accuracy: 0.9885 - unet3plus_output_final_activation_IoU: 0.9030 - val_loss: 0.8985 - val_unet3plus_output_sup0_activation_loss: 0.0214 - val_unet3plus_output_sup1_activation_loss: 0.1802 - val_unet3plus_output_sup2_activation_loss: 0.3210 - val_unet3plus_output_sup3_activation_loss: 0.3557 - val_unet3plus_output_final_activation_loss: 0.0201 - val_unet3plus_output_sup0_activation_accuracy: 0.9869 - val_unet3plus_output_sup0_activation_IoU: 0.8845 - val_unet3plus_output_sup1_activation_accuracy: 0.9534 - val_unet3plus_output_sup1_activation_IoU: 0.2718 - val_unet3plus_output_sup2_activation_accuracy: 0.9224 - val_unet3plus_output_sup2_activation_IoU: 0.0998 - val_unet3plus_output_sup3_activation_accuracy: 0.9147 - val_unet3plus_output_sup3_activation_IoU: 0.0726 - val_unet3plus_output_final_activation_accuracy: 0.9871 - val_unet3plus_output_final_activation_IoU: 0.8911 - lr: 1.0000e-04\n",
      "Epoch 9/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.8569 - unet3plus_output_sup0_activation_loss: 0.0147 - unet3plus_output_sup1_activation_loss: 0.1718 - unet3plus_output_sup2_activation_loss: 0.3112 - unet3plus_output_sup3_activation_loss: 0.3453 - unet3plus_output_final_activation_loss: 0.0139 - unet3plus_output_sup0_activation_accuracy: 0.9887 - unet3plus_output_sup0_activation_IoU: 0.9030 - unet3plus_output_sup1_activation_accuracy: 0.9545 - unet3plus_output_sup1_activation_IoU: 0.2818 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.1008 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0720 - unet3plus_output_final_activation_accuracy: 0.9888 - unet3plus_output_final_activation_IoU: 0.9083\n",
      "Epoch 9: val_loss improved from 0.89852 to 0.85598, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.8569 - unet3plus_output_sup0_activation_loss: 0.0147 - unet3plus_output_sup1_activation_loss: 0.1718 - unet3plus_output_sup2_activation_loss: 0.3112 - unet3plus_output_sup3_activation_loss: 0.3453 - unet3plus_output_final_activation_loss: 0.0139 - unet3plus_output_sup0_activation_accuracy: 0.9887 - unet3plus_output_sup0_activation_IoU: 0.9030 - unet3plus_output_sup1_activation_accuracy: 0.9545 - unet3plus_output_sup1_activation_IoU: 0.2818 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.1008 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0720 - unet3plus_output_final_activation_accuracy: 0.9888 - unet3plus_output_final_activation_IoU: 0.9083 - val_loss: 0.8560 - val_unet3plus_output_sup0_activation_loss: 0.0225 - val_unet3plus_output_sup1_activation_loss: 0.1713 - val_unet3plus_output_sup2_activation_loss: 0.3038 - val_unet3plus_output_sup3_activation_loss: 0.3367 - val_unet3plus_output_final_activation_loss: 0.0217 - val_unet3plus_output_sup0_activation_accuracy: 0.9866 - val_unet3plus_output_sup0_activation_IoU: 0.8808 - val_unet3plus_output_sup1_activation_accuracy: 0.9532 - val_unet3plus_output_sup1_activation_IoU: 0.2811 - val_unet3plus_output_sup2_activation_accuracy: 0.9224 - val_unet3plus_output_sup2_activation_IoU: 0.1003 - val_unet3plus_output_sup3_activation_accuracy: 0.9147 - val_unet3plus_output_sup3_activation_IoU: 0.0712 - val_unet3plus_output_final_activation_accuracy: 0.9867 - val_unet3plus_output_final_activation_IoU: 0.8837 - lr: 1.0000e-04\n",
      "Epoch 10/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.8115 - unet3plus_output_sup0_activation_loss: 0.0132 - unet3plus_output_sup1_activation_loss: 0.1623 - unet3plus_output_sup2_activation_loss: 0.2953 - unet3plus_output_sup3_activation_loss: 0.3279 - unet3plus_output_final_activation_loss: 0.0128 - unet3plus_output_sup0_activation_accuracy: 0.9891 - unet3plus_output_sup0_activation_IoU: 0.9110 - unet3plus_output_sup1_activation_accuracy: 0.9547 - unet3plus_output_sup1_activation_IoU: 0.2942 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1019 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0706 - unet3plus_output_final_activation_accuracy: 0.9891 - unet3plus_output_final_activation_IoU: 0.9148\n",
      "Epoch 10: val_loss improved from 0.85598 to 0.82618, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.8115 - unet3plus_output_sup0_activation_loss: 0.0132 - unet3plus_output_sup1_activation_loss: 0.1623 - unet3plus_output_sup2_activation_loss: 0.2953 - unet3plus_output_sup3_activation_loss: 0.3279 - unet3plus_output_final_activation_loss: 0.0128 - unet3plus_output_sup0_activation_accuracy: 0.9891 - unet3plus_output_sup0_activation_IoU: 0.9110 - unet3plus_output_sup1_activation_accuracy: 0.9547 - unet3plus_output_sup1_activation_IoU: 0.2942 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1019 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0706 - unet3plus_output_final_activation_accuracy: 0.9891 - unet3plus_output_final_activation_IoU: 0.9148 - val_loss: 0.8262 - val_unet3plus_output_sup0_activation_loss: 0.0255 - val_unet3plus_output_sup1_activation_loss: 0.1646 - val_unet3plus_output_sup2_activation_loss: 0.2902 - val_unet3plus_output_sup3_activation_loss: 0.3212 - val_unet3plus_output_final_activation_loss: 0.0246 - val_unet3plus_output_sup0_activation_accuracy: 0.9859 - val_unet3plus_output_sup0_activation_IoU: 0.8754 - val_unet3plus_output_sup1_activation_accuracy: 0.9528 - val_unet3plus_output_sup1_activation_IoU: 0.2894 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.1002 - val_unet3plus_output_sup3_activation_accuracy: 0.9147 - val_unet3plus_output_sup3_activation_IoU: 0.0692 - val_unet3plus_output_final_activation_accuracy: 0.9859 - val_unet3plus_output_final_activation_IoU: 0.8771 - lr: 1.0000e-04\n",
      "Epoch 11/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7766 - unet3plus_output_sup0_activation_loss: 0.0128 - unet3plus_output_sup1_activation_loss: 0.1551 - unet3plus_output_sup2_activation_loss: 0.2824 - unet3plus_output_sup3_activation_loss: 0.3137 - unet3plus_output_final_activation_loss: 0.0125 - unet3plus_output_sup0_activation_accuracy: 0.9891 - unet3plus_output_sup0_activation_IoU: 0.9123 - unet3plus_output_sup1_activation_accuracy: 0.9548 - unet3plus_output_sup1_activation_IoU: 0.3055 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1028 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0692 - unet3plus_output_final_activation_accuracy: 0.9892 - unet3plus_output_final_activation_IoU: 0.9150\n",
      "Epoch 11: val_loss improved from 0.82618 to 0.78223, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.7766 - unet3plus_output_sup0_activation_loss: 0.0128 - unet3plus_output_sup1_activation_loss: 0.1551 - unet3plus_output_sup2_activation_loss: 0.2824 - unet3plus_output_sup3_activation_loss: 0.3137 - unet3plus_output_final_activation_loss: 0.0125 - unet3plus_output_sup0_activation_accuracy: 0.9891 - unet3plus_output_sup0_activation_IoU: 0.9123 - unet3plus_output_sup1_activation_accuracy: 0.9548 - unet3plus_output_sup1_activation_IoU: 0.3055 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1028 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0692 - unet3plus_output_final_activation_accuracy: 0.9892 - unet3plus_output_final_activation_IoU: 0.9150 - val_loss: 0.7822 - val_unet3plus_output_sup0_activation_loss: 0.0202 - val_unet3plus_output_sup1_activation_loss: 0.1556 - val_unet3plus_output_sup2_activation_loss: 0.2781 - val_unet3plus_output_sup3_activation_loss: 0.3084 - val_unet3plus_output_final_activation_loss: 0.0199 - val_unet3plus_output_sup0_activation_accuracy: 0.9868 - val_unet3plus_output_sup0_activation_IoU: 0.8868 - val_unet3plus_output_sup1_activation_accuracy: 0.9535 - val_unet3plus_output_sup1_activation_IoU: 0.3052 - val_unet3plus_output_sup2_activation_accuracy: 0.9224 - val_unet3plus_output_sup2_activation_IoU: 0.1024 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0685 - val_unet3plus_output_final_activation_accuracy: 0.9869 - val_unet3plus_output_final_activation_IoU: 0.8886 - lr: 1.0000e-04\n",
      "Epoch 12/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7515 - unet3plus_output_sup0_activation_loss: 0.0135 - unet3plus_output_sup1_activation_loss: 0.1497 - unet3plus_output_sup2_activation_loss: 0.2724 - unet3plus_output_sup3_activation_loss: 0.3026 - unet3plus_output_final_activation_loss: 0.0133 - unet3plus_output_sup0_activation_accuracy: 0.9889 - unet3plus_output_sup0_activation_IoU: 0.9097 - unet3plus_output_sup1_activation_accuracy: 0.9546 - unet3plus_output_sup1_activation_IoU: 0.3160 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1035 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0674 - unet3plus_output_final_activation_accuracy: 0.9890 - unet3plus_output_final_activation_IoU: 0.9118\n",
      "Epoch 12: val_loss improved from 0.78223 to 0.75349, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.7515 - unet3plus_output_sup0_activation_loss: 0.0135 - unet3plus_output_sup1_activation_loss: 0.1497 - unet3plus_output_sup2_activation_loss: 0.2724 - unet3plus_output_sup3_activation_loss: 0.3026 - unet3plus_output_final_activation_loss: 0.0133 - unet3plus_output_sup0_activation_accuracy: 0.9889 - unet3plus_output_sup0_activation_IoU: 0.9097 - unet3plus_output_sup1_activation_accuracy: 0.9546 - unet3plus_output_sup1_activation_IoU: 0.3160 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1035 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0674 - unet3plus_output_final_activation_accuracy: 0.9890 - unet3plus_output_final_activation_IoU: 0.9118 - val_loss: 0.7535 - val_unet3plus_output_sup0_activation_loss: 0.0186 - val_unet3plus_output_sup1_activation_loss: 0.1498 - val_unet3plus_output_sup2_activation_loss: 0.2691 - val_unet3plus_output_sup3_activation_loss: 0.2985 - val_unet3plus_output_final_activation_loss: 0.0175 - val_unet3plus_output_sup0_activation_accuracy: 0.9874 - val_unet3plus_output_sup0_activation_IoU: 0.8948 - val_unet3plus_output_sup1_activation_accuracy: 0.9536 - val_unet3plus_output_sup1_activation_IoU: 0.3180 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1039 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0671 - val_unet3plus_output_final_activation_accuracy: 0.9875 - val_unet3plus_output_final_activation_IoU: 0.8958 - lr: 1.0000e-04\n",
      "Epoch 13/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7248 - unet3plus_output_sup0_activation_loss: 0.0116 - unet3plus_output_sup1_activation_loss: 0.1441 - unet3plus_output_sup2_activation_loss: 0.2641 - unet3plus_output_sup3_activation_loss: 0.2937 - unet3plus_output_final_activation_loss: 0.0114 - unet3plus_output_sup0_activation_accuracy: 0.9895 - unet3plus_output_sup0_activation_IoU: 0.9200 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3293 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1049 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0662 - unet3plus_output_final_activation_accuracy: 0.9896 - unet3plus_output_final_activation_IoU: 0.9216\n",
      "Epoch 13: val_loss improved from 0.75349 to 0.74485, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.7248 - unet3plus_output_sup0_activation_loss: 0.0116 - unet3plus_output_sup1_activation_loss: 0.1441 - unet3plus_output_sup2_activation_loss: 0.2641 - unet3plus_output_sup3_activation_loss: 0.2937 - unet3plus_output_final_activation_loss: 0.0114 - unet3plus_output_sup0_activation_accuracy: 0.9895 - unet3plus_output_sup0_activation_IoU: 0.9200 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3293 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1049 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0662 - unet3plus_output_final_activation_accuracy: 0.9896 - unet3plus_output_final_activation_IoU: 0.9216 - val_loss: 0.7448 - val_unet3plus_output_sup0_activation_loss: 0.0222 - val_unet3plus_output_sup1_activation_loss: 0.1469 - val_unet3plus_output_sup2_activation_loss: 0.2625 - val_unet3plus_output_sup3_activation_loss: 0.2911 - val_unet3plus_output_final_activation_loss: 0.0221 - val_unet3plus_output_sup0_activation_accuracy: 0.9866 - val_unet3plus_output_sup0_activation_IoU: 0.8821 - val_unet3plus_output_sup1_activation_accuracy: 0.9533 - val_unet3plus_output_sup1_activation_IoU: 0.3234 - val_unet3plus_output_sup2_activation_accuracy: 0.9224 - val_unet3plus_output_sup2_activation_IoU: 0.1041 - val_unet3plus_output_sup3_activation_accuracy: 0.9147 - val_unet3plus_output_sup3_activation_IoU: 0.0647 - val_unet3plus_output_final_activation_accuracy: 0.9863 - val_unet3plus_output_final_activation_IoU: 0.8793 - lr: 1.0000e-04\n",
      "Epoch 14/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7102 - unet3plus_output_sup0_activation_loss: 0.0120 - unet3plus_output_sup1_activation_loss: 0.1409 - unet3plus_output_sup2_activation_loss: 0.2583 - unet3plus_output_sup3_activation_loss: 0.2872 - unet3plus_output_final_activation_loss: 0.0118 - unet3plus_output_sup0_activation_accuracy: 0.9895 - unet3plus_output_sup0_activation_IoU: 0.9194 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3398 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1058 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0647 - unet3plus_output_final_activation_accuracy: 0.9895 - unet3plus_output_final_activation_IoU: 0.9208\n",
      "Epoch 14: val_loss improved from 0.74485 to 0.72201, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.7102 - unet3plus_output_sup0_activation_loss: 0.0120 - unet3plus_output_sup1_activation_loss: 0.1409 - unet3plus_output_sup2_activation_loss: 0.2583 - unet3plus_output_sup3_activation_loss: 0.2872 - unet3plus_output_final_activation_loss: 0.0118 - unet3plus_output_sup0_activation_accuracy: 0.9895 - unet3plus_output_sup0_activation_IoU: 0.9194 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3398 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1058 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0647 - unet3plus_output_final_activation_accuracy: 0.9895 - unet3plus_output_final_activation_IoU: 0.9208 - val_loss: 0.7220 - val_unet3plus_output_sup0_activation_loss: 0.0182 - val_unet3plus_output_sup1_activation_loss: 0.1427 - val_unet3plus_output_sup2_activation_loss: 0.2572 - val_unet3plus_output_sup3_activation_loss: 0.2855 - val_unet3plus_output_final_activation_loss: 0.0185 - val_unet3plus_output_sup0_activation_accuracy: 0.9877 - val_unet3plus_output_sup0_activation_IoU: 0.9012 - val_unet3plus_output_sup1_activation_accuracy: 0.9539 - val_unet3plus_output_sup1_activation_IoU: 0.3392 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1050 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0636 - val_unet3plus_output_final_activation_accuracy: 0.9877 - val_unet3plus_output_final_activation_IoU: 0.9025 - lr: 1.0000e-04\n",
      "Epoch 15/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6971 - unet3plus_output_sup0_activation_loss: 0.0113 - unet3plus_output_sup1_activation_loss: 0.1380 - unet3plus_output_sup2_activation_loss: 0.2540 - unet3plus_output_sup3_activation_loss: 0.2826 - unet3plus_output_final_activation_loss: 0.0112 - unet3plus_output_sup0_activation_accuracy: 0.9896 - unet3plus_output_sup0_activation_IoU: 0.9212 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3503 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1068 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0632 - unet3plus_output_final_activation_accuracy: 0.9896 - unet3plus_output_final_activation_IoU: 0.9224\n",
      "Epoch 15: val_loss improved from 0.72201 to 0.71549, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.6971 - unet3plus_output_sup0_activation_loss: 0.0113 - unet3plus_output_sup1_activation_loss: 0.1380 - unet3plus_output_sup2_activation_loss: 0.2540 - unet3plus_output_sup3_activation_loss: 0.2826 - unet3plus_output_final_activation_loss: 0.0112 - unet3plus_output_sup0_activation_accuracy: 0.9896 - unet3plus_output_sup0_activation_IoU: 0.9212 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3503 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1068 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0632 - unet3plus_output_final_activation_accuracy: 0.9896 - unet3plus_output_final_activation_IoU: 0.9224 - val_loss: 0.7155 - val_unet3plus_output_sup0_activation_loss: 0.0196 - val_unet3plus_output_sup1_activation_loss: 0.1411 - val_unet3plus_output_sup2_activation_loss: 0.2538 - val_unet3plus_output_sup3_activation_loss: 0.2818 - val_unet3plus_output_final_activation_loss: 0.0192 - val_unet3plus_output_sup0_activation_accuracy: 0.9875 - val_unet3plus_output_sup0_activation_IoU: 0.9004 - val_unet3plus_output_sup1_activation_accuracy: 0.9537 - val_unet3plus_output_sup1_activation_IoU: 0.3481 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1063 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0624 - val_unet3plus_output_final_activation_accuracy: 0.9876 - val_unet3plus_output_final_activation_IoU: 0.9027 - lr: 1.0000e-04\n",
      "Epoch 16/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6870 - unet3plus_output_sup0_activation_loss: 0.0103 - unet3plus_output_sup1_activation_loss: 0.1357 - unet3plus_output_sup2_activation_loss: 0.2511 - unet3plus_output_sup3_activation_loss: 0.2796 - unet3plus_output_final_activation_loss: 0.0103 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9266 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3609 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1080 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0620 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9273\n",
      "Epoch 16: val_loss improved from 0.71549 to 0.70919, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.6870 - unet3plus_output_sup0_activation_loss: 0.0103 - unet3plus_output_sup1_activation_loss: 0.1357 - unet3plus_output_sup2_activation_loss: 0.2511 - unet3plus_output_sup3_activation_loss: 0.2796 - unet3plus_output_final_activation_loss: 0.0103 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9266 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3609 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1080 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0620 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9273 - val_loss: 0.7092 - val_unet3plus_output_sup0_activation_loss: 0.0191 - val_unet3plus_output_sup1_activation_loss: 0.1396 - val_unet3plus_output_sup2_activation_loss: 0.2516 - val_unet3plus_output_sup3_activation_loss: 0.2795 - val_unet3plus_output_final_activation_loss: 0.0193 - val_unet3plus_output_sup0_activation_accuracy: 0.9876 - val_unet3plus_output_sup0_activation_IoU: 0.9032 - val_unet3plus_output_sup1_activation_accuracy: 0.9538 - val_unet3plus_output_sup1_activation_IoU: 0.3569 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1070 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0611 - val_unet3plus_output_final_activation_accuracy: 0.9875 - val_unet3plus_output_final_activation_IoU: 0.9030 - lr: 1.0000e-04\n",
      "Epoch 17/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6827 - unet3plus_output_sup0_activation_loss: 0.0104 - unet3plus_output_sup1_activation_loss: 0.1347 - unet3plus_output_sup2_activation_loss: 0.2495 - unet3plus_output_sup3_activation_loss: 0.2779 - unet3plus_output_final_activation_loss: 0.0103 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9266 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3691 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1087 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0609 - unet3plus_output_final_activation_accuracy: 0.9898 - unet3plus_output_final_activation_IoU: 0.9272\n",
      "Epoch 17: val_loss improved from 0.70919 to 0.70525, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.6827 - unet3plus_output_sup0_activation_loss: 0.0104 - unet3plus_output_sup1_activation_loss: 0.1347 - unet3plus_output_sup2_activation_loss: 0.2495 - unet3plus_output_sup3_activation_loss: 0.2779 - unet3plus_output_final_activation_loss: 0.0103 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9266 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3691 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1087 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0609 - unet3plus_output_final_activation_accuracy: 0.9898 - unet3plus_output_final_activation_IoU: 0.9272 - val_loss: 0.7052 - val_unet3plus_output_sup0_activation_loss: 0.0181 - val_unet3plus_output_sup1_activation_loss: 0.1389 - val_unet3plus_output_sup2_activation_loss: 0.2507 - val_unet3plus_output_sup3_activation_loss: 0.2784 - val_unet3plus_output_final_activation_loss: 0.0192 - val_unet3plus_output_sup0_activation_accuracy: 0.9877 - val_unet3plus_output_sup0_activation_IoU: 0.9011 - val_unet3plus_output_sup1_activation_accuracy: 0.9537 - val_unet3plus_output_sup1_activation_IoU: 0.3628 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1069 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0602 - val_unet3plus_output_final_activation_accuracy: 0.9877 - val_unet3plus_output_final_activation_IoU: 0.9025 - lr: 1.0000e-04\n",
      "Epoch 18/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6782 - unet3plus_output_sup0_activation_loss: 0.0095 - unet3plus_output_sup1_activation_loss: 0.1337 - unet3plus_output_sup2_activation_loss: 0.2486 - unet3plus_output_sup3_activation_loss: 0.2770 - unet3plus_output_final_activation_loss: 0.0094 - unet3plus_output_sup0_activation_accuracy: 0.9901 - unet3plus_output_sup0_activation_IoU: 0.9306 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3765 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1095 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0601 - unet3plus_output_final_activation_accuracy: 0.9901 - unet3plus_output_final_activation_IoU: 0.9309\n",
      "Epoch 18: val_loss improved from 0.70525 to 0.70299, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6782 - unet3plus_output_sup0_activation_loss: 0.0095 - unet3plus_output_sup1_activation_loss: 0.1337 - unet3plus_output_sup2_activation_loss: 0.2486 - unet3plus_output_sup3_activation_loss: 0.2770 - unet3plus_output_final_activation_loss: 0.0094 - unet3plus_output_sup0_activation_accuracy: 0.9901 - unet3plus_output_sup0_activation_IoU: 0.9306 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3765 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1095 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0601 - unet3plus_output_final_activation_accuracy: 0.9901 - unet3plus_output_final_activation_IoU: 0.9309 - val_loss: 0.7030 - val_unet3plus_output_sup0_activation_loss: 0.0187 - val_unet3plus_output_sup1_activation_loss: 0.1382 - val_unet3plus_output_sup2_activation_loss: 0.2500 - val_unet3plus_output_sup3_activation_loss: 0.2778 - val_unet3plus_output_final_activation_loss: 0.0183 - val_unet3plus_output_sup0_activation_accuracy: 0.9877 - val_unet3plus_output_sup0_activation_IoU: 0.9061 - val_unet3plus_output_sup1_activation_accuracy: 0.9539 - val_unet3plus_output_sup1_activation_IoU: 0.3721 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1082 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0595 - val_unet3plus_output_final_activation_accuracy: 0.9877 - val_unet3plus_output_final_activation_IoU: 0.9062 - lr: 1.0000e-04\n",
      "Epoch 19/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6770 - unet3plus_output_sup0_activation_loss: 0.0094 - unet3plus_output_sup1_activation_loss: 0.1334 - unet3plus_output_sup2_activation_loss: 0.2482 - unet3plus_output_sup3_activation_loss: 0.2766 - unet3plus_output_final_activation_loss: 0.0094 - unet3plus_output_sup0_activation_accuracy: 0.9901 - unet3plus_output_sup0_activation_IoU: 0.9317 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3816 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1100 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0595 - unet3plus_output_final_activation_accuracy: 0.9901 - unet3plus_output_final_activation_IoU: 0.9322\n",
      "Epoch 19: val_loss did not improve from 0.70299\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6770 - unet3plus_output_sup0_activation_loss: 0.0094 - unet3plus_output_sup1_activation_loss: 0.1334 - unet3plus_output_sup2_activation_loss: 0.2482 - unet3plus_output_sup3_activation_loss: 0.2766 - unet3plus_output_final_activation_loss: 0.0094 - unet3plus_output_sup0_activation_accuracy: 0.9901 - unet3plus_output_sup0_activation_IoU: 0.9317 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3816 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1100 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0595 - unet3plus_output_final_activation_accuracy: 0.9901 - unet3plus_output_final_activation_IoU: 0.9322 - val_loss: 0.7100 - val_unet3plus_output_sup0_activation_loss: 0.0212 - val_unet3plus_output_sup1_activation_loss: 0.1396 - val_unet3plus_output_sup2_activation_loss: 0.2504 - val_unet3plus_output_sup3_activation_loss: 0.2778 - val_unet3plus_output_final_activation_loss: 0.0210 - val_unet3plus_output_sup0_activation_accuracy: 0.9869 - val_unet3plus_output_sup0_activation_IoU: 0.8954 - val_unet3plus_output_sup1_activation_accuracy: 0.9534 - val_unet3plus_output_sup1_activation_IoU: 0.3756 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.1093 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0592 - val_unet3plus_output_final_activation_accuracy: 0.9869 - val_unet3plus_output_final_activation_IoU: 0.8955 - lr: 1.0000e-04\n",
      "Epoch 20/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6787 - unet3plus_output_sup0_activation_loss: 0.0102 - unet3plus_output_sup1_activation_loss: 0.1337 - unet3plus_output_sup2_activation_loss: 0.2482 - unet3plus_output_sup3_activation_loss: 0.2765 - unet3plus_output_final_activation_loss: 0.0101 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9273 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3834 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1099 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0590 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9276\n",
      "Epoch 20: val_loss did not improve from 0.70299\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6787 - unet3plus_output_sup0_activation_loss: 0.0102 - unet3plus_output_sup1_activation_loss: 0.1337 - unet3plus_output_sup2_activation_loss: 0.2482 - unet3plus_output_sup3_activation_loss: 0.2765 - unet3plus_output_final_activation_loss: 0.0101 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9273 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3834 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1099 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0590 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9276 - val_loss: 0.7063 - val_unet3plus_output_sup0_activation_loss: 0.0199 - val_unet3plus_output_sup1_activation_loss: 0.1388 - val_unet3plus_output_sup2_activation_loss: 0.2501 - val_unet3plus_output_sup3_activation_loss: 0.2776 - val_unet3plus_output_final_activation_loss: 0.0199 - val_unet3plus_output_sup0_activation_accuracy: 0.9874 - val_unet3plus_output_sup0_activation_IoU: 0.9018 - val_unet3plus_output_sup1_activation_accuracy: 0.9536 - val_unet3plus_output_sup1_activation_IoU: 0.3759 - val_unet3plus_output_sup2_activation_accuracy: 0.9224 - val_unet3plus_output_sup2_activation_IoU: 0.1077 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0583 - val_unet3plus_output_final_activation_accuracy: 0.9874 - val_unet3plus_output_final_activation_IoU: 0.9021 - lr: 1.0000e-04\n",
      "Epoch 21/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6736 - unet3plus_output_sup0_activation_loss: 0.0084 - unet3plus_output_sup1_activation_loss: 0.1327 - unet3plus_output_sup2_activation_loss: 0.2479 - unet3plus_output_sup3_activation_loss: 0.2764 - unet3plus_output_final_activation_loss: 0.0083 - unet3plus_output_sup0_activation_accuracy: 0.9904 - unet3plus_output_sup0_activation_IoU: 0.9373 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3885 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1107 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0589 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9375\n",
      "Epoch 21: val_loss did not improve from 0.70299\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.6736 - unet3plus_output_sup0_activation_loss: 0.0084 - unet3plus_output_sup1_activation_loss: 0.1327 - unet3plus_output_sup2_activation_loss: 0.2479 - unet3plus_output_sup3_activation_loss: 0.2764 - unet3plus_output_final_activation_loss: 0.0083 - unet3plus_output_sup0_activation_accuracy: 0.9904 - unet3plus_output_sup0_activation_IoU: 0.9373 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3885 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1107 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0589 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9375 - val_loss: 0.7037 - val_unet3plus_output_sup0_activation_loss: 0.0185 - val_unet3plus_output_sup1_activation_loss: 0.1380 - val_unet3plus_output_sup2_activation_loss: 0.2498 - val_unet3plus_output_sup3_activation_loss: 0.2775 - val_unet3plus_output_final_activation_loss: 0.0199 - val_unet3plus_output_sup0_activation_accuracy: 0.9879 - val_unet3plus_output_sup0_activation_IoU: 0.9047 - val_unet3plus_output_sup1_activation_accuracy: 0.9541 - val_unet3plus_output_sup1_activation_IoU: 0.3811 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1081 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0579 - val_unet3plus_output_final_activation_accuracy: 0.9877 - val_unet3plus_output_final_activation_IoU: 0.9017 - lr: 1.0000e-04\n",
      "Epoch 22/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6788 - unet3plus_output_sup0_activation_loss: 0.0104 - unet3plus_output_sup1_activation_loss: 0.1337 - unet3plus_output_sup2_activation_loss: 0.2482 - unet3plus_output_sup3_activation_loss: 0.2764 - unet3plus_output_final_activation_loss: 0.0101 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9267 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3867 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1102 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9275\n",
      "Epoch 22: val_loss improved from 0.70299 to 0.70252, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6788 - unet3plus_output_sup0_activation_loss: 0.0104 - unet3plus_output_sup1_activation_loss: 0.1337 - unet3plus_output_sup2_activation_loss: 0.2482 - unet3plus_output_sup3_activation_loss: 0.2764 - unet3plus_output_final_activation_loss: 0.0101 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9267 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3867 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1102 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9275 - val_loss: 0.7025 - val_unet3plus_output_sup0_activation_loss: 0.0186 - val_unet3plus_output_sup1_activation_loss: 0.1376 - val_unet3plus_output_sup2_activation_loss: 0.2496 - val_unet3plus_output_sup3_activation_loss: 0.2775 - val_unet3plus_output_final_activation_loss: 0.0193 - val_unet3plus_output_sup0_activation_accuracy: 0.9880 - val_unet3plus_output_sup0_activation_IoU: 0.9103 - val_unet3plus_output_sup1_activation_accuracy: 0.9540 - val_unet3plus_output_sup1_activation_IoU: 0.3814 - val_unet3plus_output_sup2_activation_accuracy: 0.9226 - val_unet3plus_output_sup2_activation_IoU: 0.1090 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0581 - val_unet3plus_output_final_activation_accuracy: 0.9879 - val_unet3plus_output_final_activation_IoU: 0.9103 - lr: 1.0000e-04\n",
      "Epoch 23/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6725 - unet3plus_output_sup0_activation_loss: 0.0081 - unet3plus_output_sup1_activation_loss: 0.1324 - unet3plus_output_sup2_activation_loss: 0.2478 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0080 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9397 - unet3plus_output_sup1_activation_accuracy: 0.9556 - unet3plus_output_sup1_activation_IoU: 0.3912 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1110 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9401\n",
      "Epoch 23: val_loss did not improve from 0.70252\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6725 - unet3plus_output_sup0_activation_loss: 0.0081 - unet3plus_output_sup1_activation_loss: 0.1324 - unet3plus_output_sup2_activation_loss: 0.2478 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0080 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9397 - unet3plus_output_sup1_activation_accuracy: 0.9556 - unet3plus_output_sup1_activation_IoU: 0.3912 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1110 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9401 - val_loss: 0.7111 - val_unet3plus_output_sup0_activation_loss: 0.0215 - val_unet3plus_output_sup1_activation_loss: 0.1389 - val_unet3plus_output_sup2_activation_loss: 0.2500 - val_unet3plus_output_sup3_activation_loss: 0.2775 - val_unet3plus_output_final_activation_loss: 0.0232 - val_unet3plus_output_sup0_activation_accuracy: 0.9874 - val_unet3plus_output_sup0_activation_IoU: 0.8971 - val_unet3plus_output_sup1_activation_accuracy: 0.9536 - val_unet3plus_output_sup1_activation_IoU: 0.3783 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1089 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0581 - val_unet3plus_output_final_activation_accuracy: 0.9872 - val_unet3plus_output_final_activation_IoU: 0.8967 - lr: 1.0000e-04\n",
      "Epoch 24/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6741 - unet3plus_output_sup0_activation_loss: 0.0086 - unet3plus_output_sup1_activation_loss: 0.1328 - unet3plus_output_sup2_activation_loss: 0.2478 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0085 - unet3plus_output_sup0_activation_accuracy: 0.9903 - unet3plus_output_sup0_activation_IoU: 0.9369 - unet3plus_output_sup1_activation_accuracy: 0.9554 - unet3plus_output_sup1_activation_IoU: 0.3907 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1109 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9370\n",
      "Epoch 24: val_loss did not improve from 0.70252\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6741 - unet3plus_output_sup0_activation_loss: 0.0086 - unet3plus_output_sup1_activation_loss: 0.1328 - unet3plus_output_sup2_activation_loss: 0.2478 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0085 - unet3plus_output_sup0_activation_accuracy: 0.9903 - unet3plus_output_sup0_activation_IoU: 0.9369 - unet3plus_output_sup1_activation_accuracy: 0.9554 - unet3plus_output_sup1_activation_IoU: 0.3907 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1109 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9370 - val_loss: 0.7067 - val_unet3plus_output_sup0_activation_loss: 0.0202 - val_unet3plus_output_sup1_activation_loss: 0.1392 - val_unet3plus_output_sup2_activation_loss: 0.2502 - val_unet3plus_output_sup3_activation_loss: 0.2776 - val_unet3plus_output_final_activation_loss: 0.0195 - val_unet3plus_output_sup0_activation_accuracy: 0.9870 - val_unet3plus_output_sup0_activation_IoU: 0.8888 - val_unet3plus_output_sup1_activation_accuracy: 0.9533 - val_unet3plus_output_sup1_activation_IoU: 0.3786 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.1082 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0578 - val_unet3plus_output_final_activation_accuracy: 0.9872 - val_unet3plus_output_final_activation_IoU: 0.8875 - lr: 1.0000e-04\n",
      "Epoch 25/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6738 - unet3plus_output_sup0_activation_loss: 0.0085 - unet3plus_output_sup1_activation_loss: 0.1327 - unet3plus_output_sup2_activation_loss: 0.2479 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0084 - unet3plus_output_sup0_activation_accuracy: 0.9904 - unet3plus_output_sup0_activation_IoU: 0.9368 - unet3plus_output_sup1_activation_accuracy: 0.9554 - unet3plus_output_sup1_activation_IoU: 0.3908 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1108 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9372\n",
      "Epoch 25: val_loss improved from 0.70252 to 0.70170, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.6738 - unet3plus_output_sup0_activation_loss: 0.0085 - unet3plus_output_sup1_activation_loss: 0.1327 - unet3plus_output_sup2_activation_loss: 0.2479 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0084 - unet3plus_output_sup0_activation_accuracy: 0.9904 - unet3plus_output_sup0_activation_IoU: 0.9368 - unet3plus_output_sup1_activation_accuracy: 0.9554 - unet3plus_output_sup1_activation_IoU: 0.3908 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1108 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9372 - val_loss: 0.7017 - val_unet3plus_output_sup0_activation_loss: 0.0183 - val_unet3plus_output_sup1_activation_loss: 0.1375 - val_unet3plus_output_sup2_activation_loss: 0.2495 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0191 - val_unet3plus_output_sup0_activation_accuracy: 0.9883 - val_unet3plus_output_sup0_activation_IoU: 0.9157 - val_unet3plus_output_sup1_activation_accuracy: 0.9542 - val_unet3plus_output_sup1_activation_IoU: 0.3844 - val_unet3plus_output_sup2_activation_accuracy: 0.9226 - val_unet3plus_output_sup2_activation_IoU: 0.1095 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0583 - val_unet3plus_output_final_activation_accuracy: 0.9883 - val_unet3plus_output_final_activation_IoU: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 26/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6711 - unet3plus_output_sup0_activation_loss: 0.0076 - unet3plus_output_sup1_activation_loss: 0.1321 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0075 - unet3plus_output_sup0_activation_accuracy: 0.9906 - unet3plus_output_sup0_activation_IoU: 0.9426 - unet3plus_output_sup1_activation_accuracy: 0.9556 - unet3plus_output_sup1_activation_IoU: 0.3928 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1113 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9906 - unet3plus_output_final_activation_IoU: 0.9427\n",
      "Epoch 26: val_loss did not improve from 0.70170\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6711 - unet3plus_output_sup0_activation_loss: 0.0076 - unet3plus_output_sup1_activation_loss: 0.1321 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0075 - unet3plus_output_sup0_activation_accuracy: 0.9906 - unet3plus_output_sup0_activation_IoU: 0.9426 - unet3plus_output_sup1_activation_accuracy: 0.9556 - unet3plus_output_sup1_activation_IoU: 0.3928 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1113 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9906 - unet3plus_output_final_activation_IoU: 0.9427 - val_loss: 0.7036 - val_unet3plus_output_sup0_activation_loss: 0.0192 - val_unet3plus_output_sup1_activation_loss: 0.1378 - val_unet3plus_output_sup2_activation_loss: 0.2496 - val_unet3plus_output_sup3_activation_loss: 0.2775 - val_unet3plus_output_final_activation_loss: 0.0195 - val_unet3plus_output_sup0_activation_accuracy: 0.9881 - val_unet3plus_output_sup0_activation_IoU: 0.9133 - val_unet3plus_output_sup1_activation_accuracy: 0.9540 - val_unet3plus_output_sup1_activation_IoU: 0.3778 - val_unet3plus_output_sup2_activation_accuracy: 0.9226 - val_unet3plus_output_sup2_activation_IoU: 0.1084 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0581 - val_unet3plus_output_final_activation_accuracy: 0.9881 - val_unet3plus_output_final_activation_IoU: 0.9135 - lr: 1.0000e-04\n",
      "Epoch 27/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6753 - unet3plus_output_sup0_activation_loss: 0.0091 - unet3plus_output_sup1_activation_loss: 0.1330 - unet3plus_output_sup2_activation_loss: 0.2479 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0089 - unet3plus_output_sup0_activation_accuracy: 0.9902 - unet3plus_output_sup0_activation_IoU: 0.9338 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3904 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1108 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9902 - unet3plus_output_final_activation_IoU: 0.9342\n",
      "Epoch 27: val_loss did not improve from 0.70170\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.6753 - unet3plus_output_sup0_activation_loss: 0.0091 - unet3plus_output_sup1_activation_loss: 0.1330 - unet3plus_output_sup2_activation_loss: 0.2479 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0089 - unet3plus_output_sup0_activation_accuracy: 0.9902 - unet3plus_output_sup0_activation_IoU: 0.9338 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3904 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1108 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9902 - unet3plus_output_final_activation_IoU: 0.9342 - val_loss: 0.7039 - val_unet3plus_output_sup0_activation_loss: 0.0193 - val_unet3plus_output_sup1_activation_loss: 0.1379 - val_unet3plus_output_sup2_activation_loss: 0.2497 - val_unet3plus_output_sup3_activation_loss: 0.2775 - val_unet3plus_output_final_activation_loss: 0.0195 - val_unet3plus_output_sup0_activation_accuracy: 0.9878 - val_unet3plus_output_sup0_activation_IoU: 0.9104 - val_unet3plus_output_sup1_activation_accuracy: 0.9539 - val_unet3plus_output_sup1_activation_IoU: 0.3827 - val_unet3plus_output_sup2_activation_accuracy: 0.9226 - val_unet3plus_output_sup2_activation_IoU: 0.1089 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0578 - val_unet3plus_output_final_activation_accuracy: 0.9878 - val_unet3plus_output_final_activation_IoU: 0.9113 - lr: 1.0000e-04\n",
      "Epoch 28/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6717 - unet3plus_output_sup0_activation_loss: 0.0077 - unet3plus_output_sup1_activation_loss: 0.1323 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0077 - unet3plus_output_sup0_activation_accuracy: 0.9906 - unet3plus_output_sup0_activation_IoU: 0.9421 - unet3plus_output_sup1_activation_accuracy: 0.9556 - unet3plus_output_sup1_activation_IoU: 0.3926 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1112 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9906 - unet3plus_output_final_activation_IoU: 0.9422\n",
      "Epoch 28: val_loss improved from 0.70170 to 0.70087, saving model to B1GC-Kfoldno1-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.6717 - unet3plus_output_sup0_activation_loss: 0.0077 - unet3plus_output_sup1_activation_loss: 0.1323 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0077 - unet3plus_output_sup0_activation_accuracy: 0.9906 - unet3plus_output_sup0_activation_IoU: 0.9421 - unet3plus_output_sup1_activation_accuracy: 0.9556 - unet3plus_output_sup1_activation_IoU: 0.3926 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1112 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9906 - unet3plus_output_final_activation_IoU: 0.9422 - val_loss: 0.7009 - val_unet3plus_output_sup0_activation_loss: 0.0182 - val_unet3plus_output_sup1_activation_loss: 0.1372 - val_unet3plus_output_sup2_activation_loss: 0.2494 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0187 - val_unet3plus_output_sup0_activation_accuracy: 0.9882 - val_unet3plus_output_sup0_activation_IoU: 0.9151 - val_unet3plus_output_sup1_activation_accuracy: 0.9541 - val_unet3plus_output_sup1_activation_IoU: 0.3847 - val_unet3plus_output_sup2_activation_accuracy: 0.9227 - val_unet3plus_output_sup2_activation_IoU: 0.1098 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0584 - val_unet3plus_output_final_activation_accuracy: 0.9882 - val_unet3plus_output_final_activation_IoU: 0.9160 - lr: 1.0000e-04\n",
      "Epoch 29/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6702 - unet3plus_output_sup0_activation_loss: 0.0073 - unet3plus_output_sup1_activation_loss: 0.1320 - unet3plus_output_sup2_activation_loss: 0.2476 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0072 - unet3plus_output_sup0_activation_accuracy: 0.9907 - unet3plus_output_sup0_activation_IoU: 0.9447 - unet3plus_output_sup1_activation_accuracy: 0.9557 - unet3plus_output_sup1_activation_IoU: 0.3938 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1115 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9907 - unet3plus_output_final_activation_IoU: 0.9449\n",
      "Epoch 29: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6702 - unet3plus_output_sup0_activation_loss: 0.0073 - unet3plus_output_sup1_activation_loss: 0.1320 - unet3plus_output_sup2_activation_loss: 0.2476 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0072 - unet3plus_output_sup0_activation_accuracy: 0.9907 - unet3plus_output_sup0_activation_IoU: 0.9447 - unet3plus_output_sup1_activation_accuracy: 0.9557 - unet3plus_output_sup1_activation_IoU: 0.3938 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1115 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9907 - unet3plus_output_final_activation_IoU: 0.9449 - val_loss: 0.7028 - val_unet3plus_output_sup0_activation_loss: 0.0189 - val_unet3plus_output_sup1_activation_loss: 0.1376 - val_unet3plus_output_sup2_activation_loss: 0.2495 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0195 - val_unet3plus_output_sup0_activation_accuracy: 0.9882 - val_unet3plus_output_sup0_activation_IoU: 0.9163 - val_unet3plus_output_sup1_activation_accuracy: 0.9542 - val_unet3plus_output_sup1_activation_IoU: 0.3851 - val_unet3plus_output_sup2_activation_accuracy: 0.9227 - val_unet3plus_output_sup2_activation_IoU: 0.1101 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0586 - val_unet3plus_output_final_activation_accuracy: 0.9883 - val_unet3plus_output_final_activation_IoU: 0.9172 - lr: 1.0000e-04\n",
      "Epoch 30/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6725 - unet3plus_output_sup0_activation_loss: 0.0081 - unet3plus_output_sup1_activation_loss: 0.1325 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0080 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9402 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3921 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1113 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9404\n",
      "Epoch 30: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 323s 204ms/step - loss: 0.6725 - unet3plus_output_sup0_activation_loss: 0.0081 - unet3plus_output_sup1_activation_loss: 0.1325 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0080 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9402 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3921 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1113 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9404 - val_loss: 0.7012 - val_unet3plus_output_sup0_activation_loss: 0.0181 - val_unet3plus_output_sup1_activation_loss: 0.1374 - val_unet3plus_output_sup2_activation_loss: 0.2495 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0188 - val_unet3plus_output_sup0_activation_accuracy: 0.9881 - val_unet3plus_output_sup0_activation_IoU: 0.9127 - val_unet3plus_output_sup1_activation_accuracy: 0.9541 - val_unet3plus_output_sup1_activation_IoU: 0.3841 - val_unet3plus_output_sup2_activation_accuracy: 0.9226 - val_unet3plus_output_sup2_activation_IoU: 0.1098 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0583 - val_unet3plus_output_final_activation_accuracy: 0.9882 - val_unet3plus_output_final_activation_IoU: 0.9143 - lr: 1.0000e-04\n",
      "Epoch 31/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6697 - unet3plus_output_sup0_activation_loss: 0.0071 - unet3plus_output_sup1_activation_loss: 0.1319 - unet3plus_output_sup2_activation_loss: 0.2475 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0070 - unet3plus_output_sup0_activation_accuracy: 0.9907 - unet3plus_output_sup0_activation_IoU: 0.9460 - unet3plus_output_sup1_activation_accuracy: 0.9557 - unet3plus_output_sup1_activation_IoU: 0.3939 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1116 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9908 - unet3plus_output_final_activation_IoU: 0.9461\n",
      "Epoch 31: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.6697 - unet3plus_output_sup0_activation_loss: 0.0071 - unet3plus_output_sup1_activation_loss: 0.1319 - unet3plus_output_sup2_activation_loss: 0.2475 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0070 - unet3plus_output_sup0_activation_accuracy: 0.9907 - unet3plus_output_sup0_activation_IoU: 0.9460 - unet3plus_output_sup1_activation_accuracy: 0.9557 - unet3plus_output_sup1_activation_IoU: 0.3939 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1116 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9908 - unet3plus_output_final_activation_IoU: 0.9461 - val_loss: 0.7238 - val_unet3plus_output_sup0_activation_loss: 0.0291 - val_unet3plus_output_sup1_activation_loss: 0.1394 - val_unet3plus_output_sup2_activation_loss: 0.2502 - val_unet3plus_output_sup3_activation_loss: 0.2776 - val_unet3plus_output_final_activation_loss: 0.0275 - val_unet3plus_output_sup0_activation_accuracy: 0.9865 - val_unet3plus_output_sup0_activation_IoU: 0.8888 - val_unet3plus_output_sup1_activation_accuracy: 0.9534 - val_unet3plus_output_sup1_activation_IoU: 0.3753 - val_unet3plus_output_sup2_activation_accuracy: 0.9224 - val_unet3plus_output_sup2_activation_IoU: 0.1084 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0577 - val_unet3plus_output_final_activation_accuracy: 0.9863 - val_unet3plus_output_final_activation_IoU: 0.8868 - lr: 1.0000e-04\n",
      "Epoch 32/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6716 - unet3plus_output_sup0_activation_loss: 0.0078 - unet3plus_output_sup1_activation_loss: 0.1323 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0077 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9418 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3926 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1114 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9421\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6716 - unet3plus_output_sup0_activation_loss: 0.0078 - unet3plus_output_sup1_activation_loss: 0.1323 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0077 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9418 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3926 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1114 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9421 - val_loss: 0.7054 - val_unet3plus_output_sup0_activation_loss: 0.0203 - val_unet3plus_output_sup1_activation_loss: 0.1382 - val_unet3plus_output_sup2_activation_loss: 0.2498 - val_unet3plus_output_sup3_activation_loss: 0.2775 - val_unet3plus_output_final_activation_loss: 0.0196 - val_unet3plus_output_sup0_activation_accuracy: 0.9873 - val_unet3plus_output_sup0_activation_IoU: 0.9008 - val_unet3plus_output_sup1_activation_accuracy: 0.9538 - val_unet3plus_output_sup1_activation_IoU: 0.3818 - val_unet3plus_output_sup2_activation_accuracy: 0.9225 - val_unet3plus_output_sup2_activation_IoU: 0.1087 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0582 - val_unet3plus_output_final_activation_accuracy: 0.9873 - val_unet3plus_output_final_activation_IoU: 0.9000 - lr: 1.0000e-04\n",
      "Epoch 33/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6702 - unet3plus_output_sup0_activation_loss: 0.0073 - unet3plus_output_sup1_activation_loss: 0.1321 - unet3plus_output_sup2_activation_loss: 0.2476 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0071 - unet3plus_output_sup0_activation_accuracy: 0.9907 - unet3plus_output_sup0_activation_IoU: 0.9446 - unet3plus_output_sup1_activation_accuracy: 0.9557 - unet3plus_output_sup1_activation_IoU: 0.3932 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1113 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9907 - unet3plus_output_final_activation_IoU: 0.9456\n",
      "Epoch 33: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6702 - unet3plus_output_sup0_activation_loss: 0.0073 - unet3plus_output_sup1_activation_loss: 0.1321 - unet3plus_output_sup2_activation_loss: 0.2476 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0071 - unet3plus_output_sup0_activation_accuracy: 0.9907 - unet3plus_output_sup0_activation_IoU: 0.9446 - unet3plus_output_sup1_activation_accuracy: 0.9557 - unet3plus_output_sup1_activation_IoU: 0.3932 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1113 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0587 - unet3plus_output_final_activation_accuracy: 0.9907 - unet3plus_output_final_activation_IoU: 0.9456 - val_loss: 0.7017 - val_unet3plus_output_sup0_activation_loss: 0.0187 - val_unet3plus_output_sup1_activation_loss: 0.1375 - val_unet3plus_output_sup2_activation_loss: 0.2494 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0186 - val_unet3plus_output_sup0_activation_accuracy: 0.9880 - val_unet3plus_output_sup0_activation_IoU: 0.9117 - val_unet3plus_output_sup1_activation_accuracy: 0.9541 - val_unet3plus_output_sup1_activation_IoU: 0.3834 - val_unet3plus_output_sup2_activation_accuracy: 0.9227 - val_unet3plus_output_sup2_activation_IoU: 0.1096 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0582 - val_unet3plus_output_final_activation_accuracy: 0.9880 - val_unet3plus_output_final_activation_IoU: 0.9128 - lr: 1.0000e-05\n",
      "Epoch 34/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6685 - unet3plus_output_sup0_activation_loss: 0.0066 - unet3plus_output_sup1_activation_loss: 0.1318 - unet3plus_output_sup2_activation_loss: 0.2475 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0065 - unet3plus_output_sup0_activation_accuracy: 0.9908 - unet3plus_output_sup0_activation_IoU: 0.9484 - unet3plus_output_sup1_activation_accuracy: 0.9558 - unet3plus_output_sup1_activation_IoU: 0.3941 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1116 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9909 - unet3plus_output_final_activation_IoU: 0.9493\n",
      "Epoch 34: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6685 - unet3plus_output_sup0_activation_loss: 0.0066 - unet3plus_output_sup1_activation_loss: 0.1318 - unet3plus_output_sup2_activation_loss: 0.2475 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0065 - unet3plus_output_sup0_activation_accuracy: 0.9908 - unet3plus_output_sup0_activation_IoU: 0.9484 - unet3plus_output_sup1_activation_accuracy: 0.9558 - unet3plus_output_sup1_activation_IoU: 0.3941 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1116 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9909 - unet3plus_output_final_activation_IoU: 0.9493 - val_loss: 0.7037 - val_unet3plus_output_sup0_activation_loss: 0.0195 - val_unet3plus_output_sup1_activation_loss: 0.1375 - val_unet3plus_output_sup2_activation_loss: 0.2495 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0198 - val_unet3plus_output_sup0_activation_accuracy: 0.9880 - val_unet3plus_output_sup0_activation_IoU: 0.9132 - val_unet3plus_output_sup1_activation_accuracy: 0.9541 - val_unet3plus_output_sup1_activation_IoU: 0.3840 - val_unet3plus_output_sup2_activation_accuracy: 0.9226 - val_unet3plus_output_sup2_activation_IoU: 0.1097 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0583 - val_unet3plus_output_final_activation_accuracy: 0.9880 - val_unet3plus_output_final_activation_IoU: 0.9140 - lr: 1.0000e-05\n",
      "Epoch 35/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6679 - unet3plus_output_sup0_activation_loss: 0.0064 - unet3plus_output_sup1_activation_loss: 0.1316 - unet3plus_output_sup2_activation_loss: 0.2475 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0063 - unet3plus_output_sup0_activation_accuracy: 0.9909 - unet3plus_output_sup0_activation_IoU: 0.9501 - unet3plus_output_sup1_activation_accuracy: 0.9558 - unet3plus_output_sup1_activation_IoU: 0.3946 - unet3plus_output_sup2_activation_accuracy: 0.9234 - unet3plus_output_sup2_activation_IoU: 0.1117 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9909 - unet3plus_output_final_activation_IoU: 0.9509\n",
      "Epoch 35: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6679 - unet3plus_output_sup0_activation_loss: 0.0064 - unet3plus_output_sup1_activation_loss: 0.1316 - unet3plus_output_sup2_activation_loss: 0.2475 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0063 - unet3plus_output_sup0_activation_accuracy: 0.9909 - unet3plus_output_sup0_activation_IoU: 0.9501 - unet3plus_output_sup1_activation_accuracy: 0.9558 - unet3plus_output_sup1_activation_IoU: 0.3946 - unet3plus_output_sup2_activation_accuracy: 0.9234 - unet3plus_output_sup2_activation_IoU: 0.1117 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9909 - unet3plus_output_final_activation_IoU: 0.9509 - val_loss: 0.7041 - val_unet3plus_output_sup0_activation_loss: 0.0197 - val_unet3plus_output_sup1_activation_loss: 0.1375 - val_unet3plus_output_sup2_activation_loss: 0.2495 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0201 - val_unet3plus_output_sup0_activation_accuracy: 0.9881 - val_unet3plus_output_sup0_activation_IoU: 0.9141 - val_unet3plus_output_sup1_activation_accuracy: 0.9541 - val_unet3plus_output_sup1_activation_IoU: 0.3837 - val_unet3plus_output_sup2_activation_accuracy: 0.9226 - val_unet3plus_output_sup2_activation_IoU: 0.1096 - val_unet3plus_output_sup3_activation_accuracy: 0.9148 - val_unet3plus_output_sup3_activation_IoU: 0.0583 - val_unet3plus_output_final_activation_accuracy: 0.9881 - val_unet3plus_output_final_activation_IoU: 0.9152 - lr: 1.0000e-05\n",
      "Epoch 36/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6674 - unet3plus_output_sup0_activation_loss: 0.0062 - unet3plus_output_sup1_activation_loss: 0.1316 - unet3plus_output_sup2_activation_loss: 0.2474 - unet3plus_output_sup3_activation_loss: 0.2761 - unet3plus_output_final_activation_loss: 0.0061 - unet3plus_output_sup0_activation_accuracy: 0.9909 - unet3plus_output_sup0_activation_IoU: 0.9512 - unet3plus_output_sup1_activation_accuracy: 0.9558 - unet3plus_output_sup1_activation_IoU: 0.3948 - unet3plus_output_sup2_activation_accuracy: 0.9234 - unet3plus_output_sup2_activation_IoU: 0.1118 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0589 - unet3plus_output_final_activation_accuracy: 0.9909 - unet3plus_output_final_activation_IoU: 0.9519\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.70087\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6674 - unet3plus_output_sup0_activation_loss: 0.0062 - unet3plus_output_sup1_activation_loss: 0.1316 - unet3plus_output_sup2_activation_loss: 0.2474 - unet3plus_output_sup3_activation_loss: 0.2761 - unet3plus_output_final_activation_loss: 0.0061 - unet3plus_output_sup0_activation_accuracy: 0.9909 - unet3plus_output_sup0_activation_IoU: 0.9512 - unet3plus_output_sup1_activation_accuracy: 0.9558 - unet3plus_output_sup1_activation_IoU: 0.3948 - unet3plus_output_sup2_activation_accuracy: 0.9234 - unet3plus_output_sup2_activation_IoU: 0.1118 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0589 - unet3plus_output_final_activation_accuracy: 0.9909 - unet3plus_output_final_activation_IoU: 0.9519 - val_loss: 0.7057 - val_unet3plus_output_sup0_activation_loss: 0.0201 - val_unet3plus_output_sup1_activation_loss: 0.1376 - val_unet3plus_output_sup2_activation_loss: 0.2494 - val_unet3plus_output_sup3_activation_loss: 0.2774 - val_unet3plus_output_final_activation_loss: 0.0213 - val_unet3plus_output_sup0_activation_accuracy: 0.9882 - val_unet3plus_output_sup0_activation_IoU: 0.9171 - val_unet3plus_output_sup1_activation_accuracy: 0.9542 - val_unet3plus_output_sup1_activation_IoU: 0.3849 - val_unet3plus_output_sup2_activation_accuracy: 0.9227 - val_unet3plus_output_sup2_activation_IoU: 0.1099 - val_unet3plus_output_sup3_activation_accuracy: 0.9149 - val_unet3plus_output_sup3_activation_IoU: 0.0584 - val_unet3plus_output_final_activation_accuracy: 0.9882 - val_unet3plus_output_final_activation_IoU: 0.9182 - lr: 1.0000e-05\n",
      "Epoch 36: early stopping\n",
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of convolution filters after each full-scale skip connection: filter_num_skip = [64, 64, 64, 64]\n",
      "\tNumber of channels of full-scale aggregated feature maps: filter_num_aggregate = 320\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tunet3plus_output_sup0_activation\n",
      "\tunet3plus_output_sup1_activation\n",
      "\tunet3plus_output_sup2_activation\n",
      "\tunet3plus_output_sup3_activation\n",
      "\tunet3plus_output_final_activation\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 2.2319 - unet3plus_output_sup0_activation_loss: 0.2987 - unet3plus_output_sup1_activation_loss: 0.5894 - unet3plus_output_sup2_activation_loss: 0.6390 - unet3plus_output_sup3_activation_loss: 0.6579 - unet3plus_output_final_activation_loss: 0.0469 - unet3plus_output_sup0_activation_accuracy: 0.8086 - unet3plus_output_sup0_activation_IoU: 0.2671 - unet3plus_output_sup1_activation_accuracy: 0.7574 - unet3plus_output_sup1_activation_IoU: 0.0957 - unet3plus_output_sup2_activation_accuracy: 0.8793 - unet3plus_output_sup2_activation_IoU: 0.0806 - unet3plus_output_sup3_activation_accuracy: 0.9072 - unet3plus_output_sup3_activation_IoU: 0.0764 - unet3plus_output_final_activation_accuracy: 0.9768 - unet3plus_output_final_activation_IoU: 0.7349\n",
      "Epoch 1: val_loss improved from inf to 1.81839, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 330s 206ms/step - loss: 2.2319 - unet3plus_output_sup0_activation_loss: 0.2987 - unet3plus_output_sup1_activation_loss: 0.5894 - unet3plus_output_sup2_activation_loss: 0.6390 - unet3plus_output_sup3_activation_loss: 0.6579 - unet3plus_output_final_activation_loss: 0.0469 - unet3plus_output_sup0_activation_accuracy: 0.8086 - unet3plus_output_sup0_activation_IoU: 0.2671 - unet3plus_output_sup1_activation_accuracy: 0.7574 - unet3plus_output_sup1_activation_IoU: 0.0957 - unet3plus_output_sup2_activation_accuracy: 0.8793 - unet3plus_output_sup2_activation_IoU: 0.0806 - unet3plus_output_sup3_activation_accuracy: 0.9072 - unet3plus_output_sup3_activation_IoU: 0.0764 - unet3plus_output_final_activation_accuracy: 0.9768 - unet3plus_output_final_activation_IoU: 0.7349 - val_loss: 1.8184 - val_unet3plus_output_sup0_activation_loss: 0.1133 - val_unet3plus_output_sup1_activation_loss: 0.4248 - val_unet3plus_output_sup2_activation_loss: 0.5910 - val_unet3plus_output_sup3_activation_loss: 0.6258 - val_unet3plus_output_final_activation_loss: 0.0634 - val_unet3plus_output_sup0_activation_accuracy: 0.9705 - val_unet3plus_output_sup0_activation_IoU: 0.5842 - val_unet3plus_output_sup1_activation_accuracy: 0.8860 - val_unet3plus_output_sup1_activation_IoU: 0.1350 - val_unet3plus_output_sup2_activation_accuracy: 0.9033 - val_unet3plus_output_sup2_activation_IoU: 0.0835 - val_unet3plus_output_sup3_activation_accuracy: 0.9066 - val_unet3plus_output_sup3_activation_IoU: 0.0768 - val_unet3plus_output_final_activation_accuracy: 0.9748 - val_unet3plus_output_final_activation_IoU: 0.7324 - lr: 1.0000e-04\n",
      "Epoch 2/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.5151 - unet3plus_output_sup0_activation_loss: 0.0400 - unet3plus_output_sup1_activation_loss: 0.3128 - unet3plus_output_sup2_activation_loss: 0.5394 - unet3plus_output_sup3_activation_loss: 0.5958 - unet3plus_output_final_activation_loss: 0.0270 - unet3plus_output_sup0_activation_accuracy: 0.9825 - unet3plus_output_sup0_activation_IoU: 0.7532 - unet3plus_output_sup1_activation_accuracy: 0.9412 - unet3plus_output_sup1_activation_IoU: 0.1925 - unet3plus_output_sup2_activation_accuracy: 0.9164 - unet3plus_output_sup2_activation_IoU: 0.0907 - unet3plus_output_sup3_activation_accuracy: 0.9075 - unet3plus_output_sup3_activation_IoU: 0.0766 - unet3plus_output_final_activation_accuracy: 0.9842 - unet3plus_output_final_activation_IoU: 0.8318\n",
      "Epoch 2: val_loss improved from 1.81839 to 1.51544, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 1.5151 - unet3plus_output_sup0_activation_loss: 0.0400 - unet3plus_output_sup1_activation_loss: 0.3128 - unet3plus_output_sup2_activation_loss: 0.5394 - unet3plus_output_sup3_activation_loss: 0.5958 - unet3plus_output_final_activation_loss: 0.0270 - unet3plus_output_sup0_activation_accuracy: 0.9825 - unet3plus_output_sup0_activation_IoU: 0.7532 - unet3plus_output_sup1_activation_accuracy: 0.9412 - unet3plus_output_sup1_activation_IoU: 0.1925 - unet3plus_output_sup2_activation_accuracy: 0.9164 - unet3plus_output_sup2_activation_IoU: 0.0907 - unet3plus_output_sup3_activation_accuracy: 0.9075 - unet3plus_output_sup3_activation_IoU: 0.0766 - unet3plus_output_final_activation_accuracy: 0.9842 - unet3plus_output_final_activation_IoU: 0.8318 - val_loss: 1.5154 - val_unet3plus_output_sup0_activation_loss: 0.0776 - val_unet3plus_output_sup1_activation_loss: 0.3051 - val_unet3plus_output_sup2_activation_loss: 0.5092 - val_unet3plus_output_sup3_activation_loss: 0.5676 - val_unet3plus_output_final_activation_loss: 0.0558 - val_unet3plus_output_sup0_activation_accuracy: 0.9765 - val_unet3plus_output_sup0_activation_IoU: 0.7358 - val_unet3plus_output_sup1_activation_accuracy: 0.9476 - val_unet3plus_output_sup1_activation_IoU: 0.1931 - val_unet3plus_output_sup2_activation_accuracy: 0.9208 - val_unet3plus_output_sup2_activation_IoU: 0.0921 - val_unet3plus_output_sup3_activation_accuracy: 0.9070 - val_unet3plus_output_sup3_activation_IoU: 0.0772 - val_unet3plus_output_final_activation_accuracy: 0.9776 - val_unet3plus_output_final_activation_IoU: 0.7764 - lr: 1.0000e-04\n",
      "Epoch 3/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.3424 - unet3plus_output_sup0_activation_loss: 0.0271 - unet3plus_output_sup1_activation_loss: 0.2694 - unet3plus_output_sup2_activation_loss: 0.4837 - unet3plus_output_sup3_activation_loss: 0.5411 - unet3plus_output_final_activation_loss: 0.0211 - unet3plus_output_sup0_activation_accuracy: 0.9854 - unet3plus_output_sup0_activation_IoU: 0.8300 - unet3plus_output_sup1_activation_accuracy: 0.9530 - unet3plus_output_sup1_activation_IoU: 0.2172 - unet3plus_output_sup2_activation_accuracy: 0.9224 - unet3plus_output_sup2_activation_IoU: 0.0950 - unet3plus_output_sup3_activation_accuracy: 0.9113 - unet3plus_output_sup3_activation_IoU: 0.0768 - unet3plus_output_final_activation_accuracy: 0.9863 - unet3plus_output_final_activation_IoU: 0.8662\n",
      "Epoch 3: val_loss improved from 1.51544 to 1.33747, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 1.3424 - unet3plus_output_sup0_activation_loss: 0.0271 - unet3plus_output_sup1_activation_loss: 0.2694 - unet3plus_output_sup2_activation_loss: 0.4837 - unet3plus_output_sup3_activation_loss: 0.5411 - unet3plus_output_final_activation_loss: 0.0211 - unet3plus_output_sup0_activation_accuracy: 0.9854 - unet3plus_output_sup0_activation_IoU: 0.8300 - unet3plus_output_sup1_activation_accuracy: 0.9530 - unet3plus_output_sup1_activation_IoU: 0.2172 - unet3plus_output_sup2_activation_accuracy: 0.9224 - unet3plus_output_sup2_activation_IoU: 0.0950 - unet3plus_output_sup3_activation_accuracy: 0.9113 - unet3plus_output_sup3_activation_IoU: 0.0768 - unet3plus_output_final_activation_accuracy: 0.9863 - unet3plus_output_final_activation_IoU: 0.8662 - val_loss: 1.3375 - val_unet3plus_output_sup0_activation_loss: 0.0466 - val_unet3plus_output_sup1_activation_loss: 0.2689 - val_unet3plus_output_sup2_activation_loss: 0.4668 - val_unet3plus_output_sup3_activation_loss: 0.5160 - val_unet3plus_output_final_activation_loss: 0.0392 - val_unet3plus_output_sup0_activation_accuracy: 0.9808 - val_unet3plus_output_sup0_activation_IoU: 0.7759 - val_unet3plus_output_sup1_activation_accuracy: 0.9501 - val_unet3plus_output_sup1_activation_IoU: 0.2111 - val_unet3plus_output_sup2_activation_accuracy: 0.9209 - val_unet3plus_output_sup2_activation_IoU: 0.0932 - val_unet3plus_output_sup3_activation_accuracy: 0.9138 - val_unet3plus_output_sup3_activation_IoU: 0.0769 - val_unet3plus_output_final_activation_accuracy: 0.9813 - val_unet3plus_output_final_activation_IoU: 0.8018 - lr: 1.0000e-04\n",
      "Epoch 4/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.2230 - unet3plus_output_sup0_activation_loss: 0.0216 - unet3plus_output_sup1_activation_loss: 0.2466 - unet3plus_output_sup2_activation_loss: 0.4438 - unet3plus_output_sup3_activation_loss: 0.4929 - unet3plus_output_final_activation_loss: 0.0181 - unet3plus_output_sup0_activation_accuracy: 0.9867 - unet3plus_output_sup0_activation_IoU: 0.8606 - unet3plus_output_sup1_activation_accuracy: 0.9536 - unet3plus_output_sup1_activation_IoU: 0.2285 - unet3plus_output_sup2_activation_accuracy: 0.9226 - unet3plus_output_sup2_activation_IoU: 0.0964 - unet3plus_output_sup3_activation_accuracy: 0.9149 - unet3plus_output_sup3_activation_IoU: 0.0770 - unet3plus_output_final_activation_accuracy: 0.9873 - unet3plus_output_final_activation_IoU: 0.8829\n",
      "Epoch 4: val_loss did not improve from 1.33747\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 1.2230 - unet3plus_output_sup0_activation_loss: 0.0216 - unet3plus_output_sup1_activation_loss: 0.2466 - unet3plus_output_sup2_activation_loss: 0.4438 - unet3plus_output_sup3_activation_loss: 0.4929 - unet3plus_output_final_activation_loss: 0.0181 - unet3plus_output_sup0_activation_accuracy: 0.9867 - unet3plus_output_sup0_activation_IoU: 0.8606 - unet3plus_output_sup1_activation_accuracy: 0.9536 - unet3plus_output_sup1_activation_IoU: 0.2285 - unet3plus_output_sup2_activation_accuracy: 0.9226 - unet3plus_output_sup2_activation_IoU: 0.0964 - unet3plus_output_sup3_activation_accuracy: 0.9149 - unet3plus_output_sup3_activation_IoU: 0.0770 - unet3plus_output_final_activation_accuracy: 0.9873 - unet3plus_output_final_activation_IoU: 0.8829 - val_loss: 1.3494 - val_unet3plus_output_sup0_activation_loss: 0.0872 - val_unet3plus_output_sup1_activation_loss: 0.2772 - val_unet3plus_output_sup2_activation_loss: 0.4333 - val_unet3plus_output_sup3_activation_loss: 0.4737 - val_unet3plus_output_final_activation_loss: 0.0779 - val_unet3plus_output_sup0_activation_accuracy: 0.9721 - val_unet3plus_output_sup0_activation_IoU: 0.6618 - val_unet3plus_output_sup1_activation_accuracy: 0.9434 - val_unet3plus_output_sup1_activation_IoU: 0.1837 - val_unet3plus_output_sup2_activation_accuracy: 0.9202 - val_unet3plus_output_sup2_activation_IoU: 0.0912 - val_unet3plus_output_sup3_activation_accuracy: 0.9141 - val_unet3plus_output_sup3_activation_IoU: 0.0763 - val_unet3plus_output_final_activation_accuracy: 0.9737 - val_unet3plus_output_final_activation_IoU: 0.6913 - lr: 1.0000e-04\n",
      "Epoch 5/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.1288 - unet3plus_output_sup0_activation_loss: 0.0202 - unet3plus_output_sup1_activation_loss: 0.2274 - unet3plus_output_sup2_activation_loss: 0.4092 - unet3plus_output_sup3_activation_loss: 0.4541 - unet3plus_output_final_activation_loss: 0.0178 - unet3plus_output_sup0_activation_accuracy: 0.9869 - unet3plus_output_sup0_activation_IoU: 0.8697 - unet3plus_output_sup1_activation_accuracy: 0.9537 - unet3plus_output_sup1_activation_IoU: 0.2378 - unet3plus_output_sup2_activation_accuracy: 0.9227 - unet3plus_output_sup2_activation_IoU: 0.0972 - unet3plus_output_sup3_activation_accuracy: 0.9150 - unet3plus_output_sup3_activation_IoU: 0.0763 - unet3plus_output_final_activation_accuracy: 0.9875 - unet3plus_output_final_activation_IoU: 0.8856\n",
      "Epoch 5: val_loss improved from 1.33747 to 1.10719, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 1.1288 - unet3plus_output_sup0_activation_loss: 0.0202 - unet3plus_output_sup1_activation_loss: 0.2274 - unet3plus_output_sup2_activation_loss: 0.4092 - unet3plus_output_sup3_activation_loss: 0.4541 - unet3plus_output_final_activation_loss: 0.0178 - unet3plus_output_sup0_activation_accuracy: 0.9869 - unet3plus_output_sup0_activation_IoU: 0.8697 - unet3plus_output_sup1_activation_accuracy: 0.9537 - unet3plus_output_sup1_activation_IoU: 0.2378 - unet3plus_output_sup2_activation_accuracy: 0.9227 - unet3plus_output_sup2_activation_IoU: 0.0972 - unet3plus_output_sup3_activation_accuracy: 0.9150 - unet3plus_output_sup3_activation_IoU: 0.0763 - unet3plus_output_final_activation_accuracy: 0.9875 - unet3plus_output_final_activation_IoU: 0.8856 - val_loss: 1.1072 - val_unet3plus_output_sup0_activation_loss: 0.0269 - val_unet3plus_output_sup1_activation_loss: 0.2232 - val_unet3plus_output_sup2_activation_loss: 0.3948 - val_unet3plus_output_sup3_activation_loss: 0.4372 - val_unet3plus_output_final_activation_loss: 0.0251 - val_unet3plus_output_sup0_activation_accuracy: 0.9844 - val_unet3plus_output_sup0_activation_IoU: 0.8496 - val_unet3plus_output_sup1_activation_accuracy: 0.9519 - val_unet3plus_output_sup1_activation_IoU: 0.2434 - val_unet3plus_output_sup2_activation_accuracy: 0.9218 - val_unet3plus_output_sup2_activation_IoU: 0.0984 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0764 - val_unet3plus_output_final_activation_accuracy: 0.9848 - val_unet3plus_output_final_activation_IoU: 0.8616 - lr: 1.0000e-04\n",
      "Epoch 6/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 1.0429 - unet3plus_output_sup0_activation_loss: 0.0179 - unet3plus_output_sup1_activation_loss: 0.2098 - unet3plus_output_sup2_activation_loss: 0.3786 - unet3plus_output_sup3_activation_loss: 0.4204 - unet3plus_output_final_activation_loss: 0.0162 - unet3plus_output_sup0_activation_accuracy: 0.9876 - unet3plus_output_sup0_activation_IoU: 0.8832 - unet3plus_output_sup1_activation_accuracy: 0.9541 - unet3plus_output_sup1_activation_IoU: 0.2490 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0982 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0754 - unet3plus_output_final_activation_accuracy: 0.9880 - unet3plus_output_final_activation_IoU: 0.8949\n",
      "Epoch 6: val_loss improved from 1.10719 to 1.06925, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 1.0429 - unet3plus_output_sup0_activation_loss: 0.0179 - unet3plus_output_sup1_activation_loss: 0.2098 - unet3plus_output_sup2_activation_loss: 0.3786 - unet3plus_output_sup3_activation_loss: 0.4204 - unet3plus_output_final_activation_loss: 0.0162 - unet3plus_output_sup0_activation_accuracy: 0.9876 - unet3plus_output_sup0_activation_IoU: 0.8832 - unet3plus_output_sup1_activation_accuracy: 0.9541 - unet3plus_output_sup1_activation_IoU: 0.2490 - unet3plus_output_sup2_activation_accuracy: 0.9228 - unet3plus_output_sup2_activation_IoU: 0.0982 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0754 - unet3plus_output_final_activation_accuracy: 0.9880 - unet3plus_output_final_activation_IoU: 0.8949 - val_loss: 1.0693 - val_unet3plus_output_sup0_activation_loss: 0.0420 - val_unet3plus_output_sup1_activation_loss: 0.2129 - val_unet3plus_output_sup2_activation_loss: 0.3678 - val_unet3plus_output_sup3_activation_loss: 0.4064 - val_unet3plus_output_final_activation_loss: 0.0401 - val_unet3plus_output_sup0_activation_accuracy: 0.9818 - val_unet3plus_output_sup0_activation_IoU: 0.8222 - val_unet3plus_output_sup1_activation_accuracy: 0.9510 - val_unet3plus_output_sup1_activation_IoU: 0.2410 - val_unet3plus_output_sup2_activation_accuracy: 0.9215 - val_unet3plus_output_sup2_activation_IoU: 0.0966 - val_unet3plus_output_sup3_activation_accuracy: 0.9142 - val_unet3plus_output_sup3_activation_IoU: 0.0745 - val_unet3plus_output_final_activation_accuracy: 0.9819 - val_unet3plus_output_final_activation_IoU: 0.8290 - lr: 1.0000e-04\n",
      "Epoch 7/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.9697 - unet3plus_output_sup0_activation_loss: 0.0163 - unet3plus_output_sup1_activation_loss: 0.1948 - unet3plus_output_sup2_activation_loss: 0.3523 - unet3plus_output_sup3_activation_loss: 0.3913 - unet3plus_output_final_activation_loss: 0.0150 - unet3plus_output_sup0_activation_accuracy: 0.9880 - unet3plus_output_sup0_activation_IoU: 0.8914 - unet3plus_output_sup1_activation_accuracy: 0.9543 - unet3plus_output_sup1_activation_IoU: 0.2599 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.0991 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0744 - unet3plus_output_final_activation_accuracy: 0.9883 - unet3plus_output_final_activation_IoU: 0.9006\n",
      "Epoch 7: val_loss improved from 1.06925 to 0.98321, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.9697 - unet3plus_output_sup0_activation_loss: 0.0163 - unet3plus_output_sup1_activation_loss: 0.1948 - unet3plus_output_sup2_activation_loss: 0.3523 - unet3plus_output_sup3_activation_loss: 0.3913 - unet3plus_output_final_activation_loss: 0.0150 - unet3plus_output_sup0_activation_accuracy: 0.9880 - unet3plus_output_sup0_activation_IoU: 0.8914 - unet3plus_output_sup1_activation_accuracy: 0.9543 - unet3plus_output_sup1_activation_IoU: 0.2599 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.0991 - unet3plus_output_sup3_activation_accuracy: 0.9151 - unet3plus_output_sup3_activation_IoU: 0.0744 - unet3plus_output_final_activation_accuracy: 0.9883 - unet3plus_output_final_activation_IoU: 0.9006 - val_loss: 0.9832 - val_unet3plus_output_sup0_activation_loss: 0.0336 - val_unet3plus_output_sup1_activation_loss: 0.1969 - val_unet3plus_output_sup2_activation_loss: 0.3434 - val_unet3plus_output_sup3_activation_loss: 0.3793 - val_unet3plus_output_final_activation_loss: 0.0300 - val_unet3plus_output_sup0_activation_accuracy: 0.9830 - val_unet3plus_output_sup0_activation_IoU: 0.8380 - val_unet3plus_output_sup1_activation_accuracy: 0.9515 - val_unet3plus_output_sup1_activation_IoU: 0.2536 - val_unet3plus_output_sup2_activation_accuracy: 0.9217 - val_unet3plus_output_sup2_activation_IoU: 0.0978 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0738 - val_unet3plus_output_final_activation_accuracy: 0.9836 - val_unet3plus_output_final_activation_IoU: 0.8483 - lr: 1.0000e-04\n",
      "Epoch 8/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.9076 - unet3plus_output_sup0_activation_loss: 0.0152 - unet3plus_output_sup1_activation_loss: 0.1820 - unet3plus_output_sup2_activation_loss: 0.3299 - unet3plus_output_sup3_activation_loss: 0.3664 - unet3plus_output_final_activation_loss: 0.0142 - unet3plus_output_sup0_activation_accuracy: 0.9884 - unet3plus_output_sup0_activation_IoU: 0.8982 - unet3plus_output_sup1_activation_accuracy: 0.9545 - unet3plus_output_sup1_activation_IoU: 0.2711 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.0999 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0732 - unet3plus_output_final_activation_accuracy: 0.9887 - unet3plus_output_final_activation_IoU: 0.9059\n",
      "Epoch 8: val_loss improved from 0.98321 to 0.94972, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.9076 - unet3plus_output_sup0_activation_loss: 0.0152 - unet3plus_output_sup1_activation_loss: 0.1820 - unet3plus_output_sup2_activation_loss: 0.3299 - unet3plus_output_sup3_activation_loss: 0.3664 - unet3plus_output_final_activation_loss: 0.0142 - unet3plus_output_sup0_activation_accuracy: 0.9884 - unet3plus_output_sup0_activation_IoU: 0.8982 - unet3plus_output_sup1_activation_accuracy: 0.9545 - unet3plus_output_sup1_activation_IoU: 0.2711 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.0999 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0732 - unet3plus_output_final_activation_accuracy: 0.9887 - unet3plus_output_final_activation_IoU: 0.9059 - val_loss: 0.9497 - val_unet3plus_output_sup0_activation_loss: 0.0409 - val_unet3plus_output_sup1_activation_loss: 0.1905 - val_unet3plus_output_sup2_activation_loss: 0.3231 - val_unet3plus_output_sup3_activation_loss: 0.3565 - val_unet3plus_output_final_activation_loss: 0.0388 - val_unet3plus_output_sup0_activation_accuracy: 0.9829 - val_unet3plus_output_sup0_activation_IoU: 0.8389 - val_unet3plus_output_sup1_activation_accuracy: 0.9510 - val_unet3plus_output_sup1_activation_IoU: 0.2588 - val_unet3plus_output_sup2_activation_accuracy: 0.9216 - val_unet3plus_output_sup2_activation_IoU: 0.0979 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0725 - val_unet3plus_output_final_activation_accuracy: 0.9831 - val_unet3plus_output_final_activation_IoU: 0.8438 - lr: 1.0000e-04\n",
      "Epoch 9/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.8572 - unet3plus_output_sup0_activation_loss: 0.0149 - unet3plus_output_sup1_activation_loss: 0.1716 - unet3plus_output_sup2_activation_loss: 0.3111 - unet3plus_output_sup3_activation_loss: 0.3455 - unet3plus_output_final_activation_loss: 0.0141 - unet3plus_output_sup0_activation_accuracy: 0.9885 - unet3plus_output_sup0_activation_IoU: 0.8997 - unet3plus_output_sup1_activation_accuracy: 0.9545 - unet3plus_output_sup1_activation_IoU: 0.2817 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.1006 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0719 - unet3plus_output_final_activation_accuracy: 0.9887 - unet3plus_output_final_activation_IoU: 0.9061\n",
      "Epoch 9: val_loss did not improve from 0.94972\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.8572 - unet3plus_output_sup0_activation_loss: 0.0149 - unet3plus_output_sup1_activation_loss: 0.1716 - unet3plus_output_sup2_activation_loss: 0.3111 - unet3plus_output_sup3_activation_loss: 0.3455 - unet3plus_output_final_activation_loss: 0.0141 - unet3plus_output_sup0_activation_accuracy: 0.9885 - unet3plus_output_sup0_activation_IoU: 0.8997 - unet3plus_output_sup1_activation_accuracy: 0.9545 - unet3plus_output_sup1_activation_IoU: 0.2817 - unet3plus_output_sup2_activation_accuracy: 0.9229 - unet3plus_output_sup2_activation_IoU: 0.1006 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0719 - unet3plus_output_final_activation_accuracy: 0.9887 - unet3plus_output_final_activation_IoU: 0.9061 - val_loss: 0.9716 - val_unet3plus_output_sup0_activation_loss: 0.0666 - val_unet3plus_output_sup1_activation_loss: 0.1920 - val_unet3plus_output_sup2_activation_loss: 0.3094 - val_unet3plus_output_sup3_activation_loss: 0.3377 - val_unet3plus_output_final_activation_loss: 0.0660 - val_unet3plus_output_sup0_activation_accuracy: 0.9779 - val_unet3plus_output_sup0_activation_IoU: 0.7733 - val_unet3plus_output_sup1_activation_accuracy: 0.9487 - val_unet3plus_output_sup1_activation_IoU: 0.2543 - val_unet3plus_output_sup2_activation_accuracy: 0.9209 - val_unet3plus_output_sup2_activation_IoU: 0.0948 - val_unet3plus_output_sup3_activation_accuracy: 0.9143 - val_unet3plus_output_sup3_activation_IoU: 0.0705 - val_unet3plus_output_final_activation_accuracy: 0.9775 - val_unet3plus_output_final_activation_IoU: 0.7738 - lr: 1.0000e-04\n",
      "Epoch 10/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.8129 - unet3plus_output_sup0_activation_loss: 0.0139 - unet3plus_output_sup1_activation_loss: 0.1624 - unet3plus_output_sup2_activation_loss: 0.2953 - unet3plus_output_sup3_activation_loss: 0.3280 - unet3plus_output_final_activation_loss: 0.0133 - unet3plus_output_sup0_activation_accuracy: 0.9888 - unet3plus_output_sup0_activation_IoU: 0.9062 - unet3plus_output_sup1_activation_accuracy: 0.9547 - unet3plus_output_sup1_activation_IoU: 0.2937 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1016 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0706 - unet3plus_output_final_activation_accuracy: 0.9890 - unet3plus_output_final_activation_IoU: 0.9113\n",
      "Epoch 10: val_loss improved from 0.94972 to 0.87886, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.8129 - unet3plus_output_sup0_activation_loss: 0.0139 - unet3plus_output_sup1_activation_loss: 0.1624 - unet3plus_output_sup2_activation_loss: 0.2953 - unet3plus_output_sup3_activation_loss: 0.3280 - unet3plus_output_final_activation_loss: 0.0133 - unet3plus_output_sup0_activation_accuracy: 0.9888 - unet3plus_output_sup0_activation_IoU: 0.9062 - unet3plus_output_sup1_activation_accuracy: 0.9547 - unet3plus_output_sup1_activation_IoU: 0.2937 - unet3plus_output_sup2_activation_accuracy: 0.9230 - unet3plus_output_sup2_activation_IoU: 0.1016 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0706 - unet3plus_output_final_activation_accuracy: 0.9890 - unet3plus_output_final_activation_IoU: 0.9113 - val_loss: 0.8789 - val_unet3plus_output_sup0_activation_loss: 0.0458 - val_unet3plus_output_sup1_activation_loss: 0.1732 - val_unet3plus_output_sup2_activation_loss: 0.2928 - val_unet3plus_output_sup3_activation_loss: 0.3221 - val_unet3plus_output_final_activation_loss: 0.0449 - val_unet3plus_output_sup0_activation_accuracy: 0.9809 - val_unet3plus_output_sup0_activation_IoU: 0.8112 - val_unet3plus_output_sup1_activation_accuracy: 0.9504 - val_unet3plus_output_sup1_activation_IoU: 0.2739 - val_unet3plus_output_sup2_activation_accuracy: 0.9213 - val_unet3plus_output_sup2_activation_IoU: 0.0967 - val_unet3plus_output_sup3_activation_accuracy: 0.9143 - val_unet3plus_output_sup3_activation_IoU: 0.0687 - val_unet3plus_output_final_activation_accuracy: 0.9805 - val_unet3plus_output_final_activation_IoU: 0.8104 - lr: 1.0000e-04\n",
      "Epoch 11/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7756 - unet3plus_output_sup0_activation_loss: 0.0127 - unet3plus_output_sup1_activation_loss: 0.1547 - unet3plus_output_sup2_activation_loss: 0.2823 - unet3plus_output_sup3_activation_loss: 0.3137 - unet3plus_output_final_activation_loss: 0.0122 - unet3plus_output_sup0_activation_accuracy: 0.9892 - unet3plus_output_sup0_activation_IoU: 0.9134 - unet3plus_output_sup1_activation_accuracy: 0.9549 - unet3plus_output_sup1_activation_IoU: 0.3061 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1028 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0692 - unet3plus_output_final_activation_accuracy: 0.9893 - unet3plus_output_final_activation_IoU: 0.9181\n",
      "Epoch 11: val_loss improved from 0.87886 to 0.84243, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.7756 - unet3plus_output_sup0_activation_loss: 0.0127 - unet3plus_output_sup1_activation_loss: 0.1547 - unet3plus_output_sup2_activation_loss: 0.2823 - unet3plus_output_sup3_activation_loss: 0.3137 - unet3plus_output_final_activation_loss: 0.0122 - unet3plus_output_sup0_activation_accuracy: 0.9892 - unet3plus_output_sup0_activation_IoU: 0.9134 - unet3plus_output_sup1_activation_accuracy: 0.9549 - unet3plus_output_sup1_activation_IoU: 0.3061 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1028 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0692 - unet3plus_output_final_activation_accuracy: 0.9893 - unet3plus_output_final_activation_IoU: 0.9181 - val_loss: 0.8424 - val_unet3plus_output_sup0_activation_loss: 0.0443 - val_unet3plus_output_sup1_activation_loss: 0.1678 - val_unet3plus_output_sup2_activation_loss: 0.2828 - val_unet3plus_output_sup3_activation_loss: 0.3095 - val_unet3plus_output_final_activation_loss: 0.0380 - val_unet3plus_output_sup0_activation_accuracy: 0.9823 - val_unet3plus_output_sup0_activation_IoU: 0.8212 - val_unet3plus_output_sup1_activation_accuracy: 0.9505 - val_unet3plus_output_sup1_activation_IoU: 0.2847 - val_unet3plus_output_sup2_activation_accuracy: 0.9210 - val_unet3plus_output_sup2_activation_IoU: 0.0961 - val_unet3plus_output_sup3_activation_accuracy: 0.9143 - val_unet3plus_output_sup3_activation_IoU: 0.0673 - val_unet3plus_output_final_activation_accuracy: 0.9828 - val_unet3plus_output_final_activation_IoU: 0.8297 - lr: 1.0000e-04\n",
      "Epoch 12/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7477 - unet3plus_output_sup0_activation_loss: 0.0124 - unet3plus_output_sup1_activation_loss: 0.1488 - unet3plus_output_sup2_activation_loss: 0.2720 - unet3plus_output_sup3_activation_loss: 0.3024 - unet3plus_output_final_activation_loss: 0.0121 - unet3plus_output_sup0_activation_accuracy: 0.9893 - unet3plus_output_sup0_activation_IoU: 0.9149 - unet3plus_output_sup1_activation_accuracy: 0.9549 - unet3plus_output_sup1_activation_IoU: 0.3174 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1037 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0677 - unet3plus_output_final_activation_accuracy: 0.9894 - unet3plus_output_final_activation_IoU: 0.9181\n",
      "Epoch 12: val_loss improved from 0.84243 to 0.76823, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.7477 - unet3plus_output_sup0_activation_loss: 0.0124 - unet3plus_output_sup1_activation_loss: 0.1488 - unet3plus_output_sup2_activation_loss: 0.2720 - unet3plus_output_sup3_activation_loss: 0.3024 - unet3plus_output_final_activation_loss: 0.0121 - unet3plus_output_sup0_activation_accuracy: 0.9893 - unet3plus_output_sup0_activation_IoU: 0.9149 - unet3plus_output_sup1_activation_accuracy: 0.9549 - unet3plus_output_sup1_activation_IoU: 0.3174 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1037 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0677 - unet3plus_output_final_activation_accuracy: 0.9894 - unet3plus_output_final_activation_IoU: 0.9181 - val_loss: 0.7682 - val_unet3plus_output_sup0_activation_loss: 0.0240 - val_unet3plus_output_sup1_activation_loss: 0.1514 - val_unet3plus_output_sup2_activation_loss: 0.2698 - val_unet3plus_output_sup3_activation_loss: 0.2991 - val_unet3plus_output_final_activation_loss: 0.0239 - val_unet3plus_output_sup0_activation_accuracy: 0.9864 - val_unet3plus_output_sup0_activation_IoU: 0.8875 - val_unet3plus_output_sup1_activation_accuracy: 0.9533 - val_unet3plus_output_sup1_activation_IoU: 0.3167 - val_unet3plus_output_sup2_activation_accuracy: 0.9221 - val_unet3plus_output_sup2_activation_IoU: 0.1027 - val_unet3plus_output_sup3_activation_accuracy: 0.9145 - val_unet3plus_output_sup3_activation_IoU: 0.0667 - val_unet3plus_output_final_activation_accuracy: 0.9864 - val_unet3plus_output_final_activation_IoU: 0.8907 - lr: 1.0000e-04\n",
      "Epoch 13/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7249 - unet3plus_output_sup0_activation_loss: 0.0118 - unet3plus_output_sup1_activation_loss: 0.1439 - unet3plus_output_sup2_activation_loss: 0.2640 - unet3plus_output_sup3_activation_loss: 0.2936 - unet3plus_output_final_activation_loss: 0.0116 - unet3plus_output_sup0_activation_accuracy: 0.9895 - unet3plus_output_sup0_activation_IoU: 0.9188 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3293 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1048 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0663 - unet3plus_output_final_activation_accuracy: 0.9896 - unet3plus_output_final_activation_IoU: 0.9214\n",
      "Epoch 13: val_loss did not improve from 0.76823\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.7249 - unet3plus_output_sup0_activation_loss: 0.0118 - unet3plus_output_sup1_activation_loss: 0.1439 - unet3plus_output_sup2_activation_loss: 0.2640 - unet3plus_output_sup3_activation_loss: 0.2936 - unet3plus_output_final_activation_loss: 0.0116 - unet3plus_output_sup0_activation_accuracy: 0.9895 - unet3plus_output_sup0_activation_IoU: 0.9188 - unet3plus_output_sup1_activation_accuracy: 0.9550 - unet3plus_output_sup1_activation_IoU: 0.3293 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1048 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0663 - unet3plus_output_final_activation_accuracy: 0.9896 - unet3plus_output_final_activation_IoU: 0.9214 - val_loss: 0.8043 - val_unet3plus_output_sup0_activation_loss: 0.0467 - val_unet3plus_output_sup1_activation_loss: 0.1569 - val_unet3plus_output_sup2_activation_loss: 0.2649 - val_unet3plus_output_sup3_activation_loss: 0.2919 - val_unet3plus_output_final_activation_loss: 0.0440 - val_unet3plus_output_sup0_activation_accuracy: 0.9819 - val_unet3plus_output_sup0_activation_IoU: 0.8330 - val_unet3plus_output_sup1_activation_accuracy: 0.9509 - val_unet3plus_output_sup1_activation_IoU: 0.3096 - val_unet3plus_output_sup2_activation_accuracy: 0.9215 - val_unet3plus_output_sup2_activation_IoU: 0.1009 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0644 - val_unet3plus_output_final_activation_accuracy: 0.9820 - val_unet3plus_output_final_activation_IoU: 0.8381 - lr: 1.0000e-04\n",
      "Epoch 14/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.7063 - unet3plus_output_sup0_activation_loss: 0.0107 - unet3plus_output_sup1_activation_loss: 0.1400 - unet3plus_output_sup2_activation_loss: 0.2579 - unet3plus_output_sup3_activation_loss: 0.2870 - unet3plus_output_final_activation_loss: 0.0106 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9245 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3412 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1060 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0648 - unet3plus_output_final_activation_accuracy: 0.9898 - unet3plus_output_final_activation_IoU: 0.9268\n",
      "Epoch 14: val_loss did not improve from 0.76823\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.7063 - unet3plus_output_sup0_activation_loss: 0.0107 - unet3plus_output_sup1_activation_loss: 0.1400 - unet3plus_output_sup2_activation_loss: 0.2579 - unet3plus_output_sup3_activation_loss: 0.2870 - unet3plus_output_final_activation_loss: 0.0106 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9245 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3412 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1060 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0648 - unet3plus_output_final_activation_accuracy: 0.9898 - unet3plus_output_final_activation_IoU: 0.9268 - val_loss: 0.7878 - val_unet3plus_output_sup0_activation_loss: 0.0477 - val_unet3plus_output_sup1_activation_loss: 0.1516 - val_unet3plus_output_sup2_activation_loss: 0.2595 - val_unet3plus_output_sup3_activation_loss: 0.2867 - val_unet3plus_output_final_activation_loss: 0.0423 - val_unet3plus_output_sup0_activation_accuracy: 0.9840 - val_unet3plus_output_sup0_activation_IoU: 0.8633 - val_unet3plus_output_sup1_activation_accuracy: 0.9519 - val_unet3plus_output_sup1_activation_IoU: 0.3282 - val_unet3plus_output_sup2_activation_accuracy: 0.9218 - val_unet3plus_output_sup2_activation_IoU: 0.1022 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0625 - val_unet3plus_output_final_activation_accuracy: 0.9838 - val_unet3plus_output_final_activation_IoU: 0.8631 - lr: 1.0000e-04\n",
      "Epoch 15/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6980 - unet3plus_output_sup0_activation_loss: 0.0117 - unet3plus_output_sup1_activation_loss: 0.1381 - unet3plus_output_sup2_activation_loss: 0.2540 - unet3plus_output_sup3_activation_loss: 0.2826 - unet3plus_output_final_activation_loss: 0.0116 - unet3plus_output_sup0_activation_accuracy: 0.9894 - unet3plus_output_sup0_activation_IoU: 0.9187 - unet3plus_output_sup1_activation_accuracy: 0.9549 - unet3plus_output_sup1_activation_IoU: 0.3497 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1065 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0632 - unet3plus_output_final_activation_accuracy: 0.9895 - unet3plus_output_final_activation_IoU: 0.9204\n",
      "Epoch 15: val_loss did not improve from 0.76823\n",
      "1581/1581 [==============================] - 324s 205ms/step - loss: 0.6980 - unet3plus_output_sup0_activation_loss: 0.0117 - unet3plus_output_sup1_activation_loss: 0.1381 - unet3plus_output_sup2_activation_loss: 0.2540 - unet3plus_output_sup3_activation_loss: 0.2826 - unet3plus_output_final_activation_loss: 0.0116 - unet3plus_output_sup0_activation_accuracy: 0.9894 - unet3plus_output_sup0_activation_IoU: 0.9187 - unet3plus_output_sup1_activation_accuracy: 0.9549 - unet3plus_output_sup1_activation_IoU: 0.3497 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1065 - unet3plus_output_sup3_activation_accuracy: 0.9152 - unet3plus_output_sup3_activation_IoU: 0.0632 - unet3plus_output_final_activation_accuracy: 0.9895 - unet3plus_output_final_activation_IoU: 0.9204 - val_loss: 0.8036 - val_unet3plus_output_sup0_activation_loss: 0.0536 - val_unet3plus_output_sup1_activation_loss: 0.1560 - val_unet3plus_output_sup2_activation_loss: 0.2566 - val_unet3plus_output_sup3_activation_loss: 0.2827 - val_unet3plus_output_final_activation_loss: 0.0548 - val_unet3plus_output_sup0_activation_accuracy: 0.9809 - val_unet3plus_output_sup0_activation_IoU: 0.8192 - val_unet3plus_output_sup1_activation_accuracy: 0.9503 - val_unet3plus_output_sup1_activation_IoU: 0.3218 - val_unet3plus_output_sup2_activation_accuracy: 0.9214 - val_unet3plus_output_sup2_activation_IoU: 0.1009 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0619 - val_unet3plus_output_final_activation_accuracy: 0.9808 - val_unet3plus_output_final_activation_IoU: 0.8200 - lr: 1.0000e-04\n",
      "Epoch 16/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6856 - unet3plus_output_sup0_activation_loss: 0.0099 - unet3plus_output_sup1_activation_loss: 0.1354 - unet3plus_output_sup2_activation_loss: 0.2509 - unet3plus_output_sup3_activation_loss: 0.2795 - unet3plus_output_final_activation_loss: 0.0098 - unet3plus_output_sup0_activation_accuracy: 0.9900 - unet3plus_output_sup0_activation_IoU: 0.9289 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3616 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1080 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0620 - unet3plus_output_final_activation_accuracy: 0.9900 - unet3plus_output_final_activation_IoU: 0.9302\n",
      "Epoch 16: val_loss improved from 0.76823 to 0.76002, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6856 - unet3plus_output_sup0_activation_loss: 0.0099 - unet3plus_output_sup1_activation_loss: 0.1354 - unet3plus_output_sup2_activation_loss: 0.2509 - unet3plus_output_sup3_activation_loss: 0.2795 - unet3plus_output_final_activation_loss: 0.0098 - unet3plus_output_sup0_activation_accuracy: 0.9900 - unet3plus_output_sup0_activation_IoU: 0.9289 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3616 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1080 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0620 - unet3plus_output_final_activation_accuracy: 0.9900 - unet3plus_output_final_activation_IoU: 0.9302 - val_loss: 0.7600 - val_unet3plus_output_sup0_activation_loss: 0.0401 - val_unet3plus_output_sup1_activation_loss: 0.1472 - val_unet3plus_output_sup2_activation_loss: 0.2536 - val_unet3plus_output_sup3_activation_loss: 0.2803 - val_unet3plus_output_final_activation_loss: 0.0387 - val_unet3plus_output_sup0_activation_accuracy: 0.9844 - val_unet3plus_output_sup0_activation_IoU: 0.8671 - val_unet3plus_output_sup1_activation_accuracy: 0.9520 - val_unet3plus_output_sup1_activation_IoU: 0.3460 - val_unet3plus_output_sup2_activation_accuracy: 0.9219 - val_unet3plus_output_sup2_activation_IoU: 0.1055 - val_unet3plus_output_sup3_activation_accuracy: 0.9145 - val_unet3plus_output_sup3_activation_IoU: 0.0606 - val_unet3plus_output_final_activation_accuracy: 0.9845 - val_unet3plus_output_final_activation_IoU: 0.8699 - lr: 1.0000e-04\n",
      "Epoch 17/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6832 - unet3plus_output_sup0_activation_loss: 0.0106 - unet3plus_output_sup1_activation_loss: 0.1348 - unet3plus_output_sup2_activation_loss: 0.2495 - unet3plus_output_sup3_activation_loss: 0.2778 - unet3plus_output_final_activation_loss: 0.0105 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9253 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3685 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1084 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0608 - unet3plus_output_final_activation_accuracy: 0.9898 - unet3plus_output_final_activation_IoU: 0.9265\n",
      "Epoch 17: val_loss improved from 0.76002 to 0.71617, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 327s 207ms/step - loss: 0.6832 - unet3plus_output_sup0_activation_loss: 0.0106 - unet3plus_output_sup1_activation_loss: 0.1348 - unet3plus_output_sup2_activation_loss: 0.2495 - unet3plus_output_sup3_activation_loss: 0.2778 - unet3plus_output_final_activation_loss: 0.0105 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9253 - unet3plus_output_sup1_activation_accuracy: 0.9551 - unet3plus_output_sup1_activation_IoU: 0.3685 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1084 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0608 - unet3plus_output_final_activation_accuracy: 0.9898 - unet3plus_output_final_activation_IoU: 0.9265 - val_loss: 0.7162 - val_unet3plus_output_sup0_activation_loss: 0.0222 - val_unet3plus_output_sup1_activation_loss: 0.1414 - val_unet3plus_output_sup2_activation_loss: 0.2515 - val_unet3plus_output_sup3_activation_loss: 0.2791 - val_unet3plus_output_final_activation_loss: 0.0220 - val_unet3plus_output_sup0_activation_accuracy: 0.9858 - val_unet3plus_output_sup0_activation_IoU: 0.8754 - val_unet3plus_output_sup1_activation_accuracy: 0.9525 - val_unet3plus_output_sup1_activation_IoU: 0.3582 - val_unet3plus_output_sup2_activation_accuracy: 0.9221 - val_unet3plus_output_sup2_activation_IoU: 0.1066 - val_unet3plus_output_sup3_activation_accuracy: 0.9145 - val_unet3plus_output_sup3_activation_IoU: 0.0594 - val_unet3plus_output_final_activation_accuracy: 0.9859 - val_unet3plus_output_final_activation_IoU: 0.8783 - lr: 1.0000e-04\n",
      "Epoch 18/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6799 - unet3plus_output_sup0_activation_loss: 0.0102 - unet3plus_output_sup1_activation_loss: 0.1341 - unet3plus_output_sup2_activation_loss: 0.2487 - unet3plus_output_sup3_activation_loss: 0.2769 - unet3plus_output_final_activation_loss: 0.0101 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9271 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3754 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1091 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0599 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9280\n",
      "Epoch 18: val_loss improved from 0.71617 to 0.71066, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.6799 - unet3plus_output_sup0_activation_loss: 0.0102 - unet3plus_output_sup1_activation_loss: 0.1341 - unet3plus_output_sup2_activation_loss: 0.2487 - unet3plus_output_sup3_activation_loss: 0.2769 - unet3plus_output_final_activation_loss: 0.0101 - unet3plus_output_sup0_activation_accuracy: 0.9898 - unet3plus_output_sup0_activation_IoU: 0.9271 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3754 - unet3plus_output_sup2_activation_accuracy: 0.9231 - unet3plus_output_sup2_activation_IoU: 0.1091 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0599 - unet3plus_output_final_activation_accuracy: 0.9899 - unet3plus_output_final_activation_IoU: 0.9280 - val_loss: 0.7107 - val_unet3plus_output_sup0_activation_loss: 0.0210 - val_unet3plus_output_sup1_activation_loss: 0.1393 - val_unet3plus_output_sup2_activation_loss: 0.2507 - val_unet3plus_output_sup3_activation_loss: 0.2786 - val_unet3plus_output_final_activation_loss: 0.0210 - val_unet3plus_output_sup0_activation_accuracy: 0.9873 - val_unet3plus_output_sup0_activation_IoU: 0.9014 - val_unet3plus_output_sup1_activation_accuracy: 0.9535 - val_unet3plus_output_sup1_activation_IoU: 0.3697 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.1067 - val_unet3plus_output_sup3_activation_accuracy: 0.9145 - val_unet3plus_output_sup3_activation_IoU: 0.0582 - val_unet3plus_output_final_activation_accuracy: 0.9873 - val_unet3plus_output_final_activation_IoU: 0.9023 - lr: 1.0000e-04\n",
      "Epoch 19/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6752 - unet3plus_output_sup0_activation_loss: 0.0089 - unet3plus_output_sup1_activation_loss: 0.1330 - unet3plus_output_sup2_activation_loss: 0.2480 - unet3plus_output_sup3_activation_loss: 0.2764 - unet3plus_output_final_activation_loss: 0.0088 - unet3plus_output_sup0_activation_accuracy: 0.9902 - unet3plus_output_sup0_activation_IoU: 0.9344 - unet3plus_output_sup1_activation_accuracy: 0.9554 - unet3plus_output_sup1_activation_IoU: 0.3822 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1100 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0595 - unet3plus_output_final_activation_accuracy: 0.9903 - unet3plus_output_final_activation_IoU: 0.9349\n",
      "Epoch 19: val_loss improved from 0.71066 to 0.70817, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 326s 206ms/step - loss: 0.6752 - unet3plus_output_sup0_activation_loss: 0.0089 - unet3plus_output_sup1_activation_loss: 0.1330 - unet3plus_output_sup2_activation_loss: 0.2480 - unet3plus_output_sup3_activation_loss: 0.2764 - unet3plus_output_final_activation_loss: 0.0088 - unet3plus_output_sup0_activation_accuracy: 0.9902 - unet3plus_output_sup0_activation_IoU: 0.9344 - unet3plus_output_sup1_activation_accuracy: 0.9554 - unet3plus_output_sup1_activation_IoU: 0.3822 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1100 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0595 - unet3plus_output_final_activation_accuracy: 0.9903 - unet3plus_output_final_activation_IoU: 0.9349 - val_loss: 0.7082 - val_unet3plus_output_sup0_activation_loss: 0.0206 - val_unet3plus_output_sup1_activation_loss: 0.1384 - val_unet3plus_output_sup2_activation_loss: 0.2503 - val_unet3plus_output_sup3_activation_loss: 0.2782 - val_unet3plus_output_final_activation_loss: 0.0206 - val_unet3plus_output_sup0_activation_accuracy: 0.9875 - val_unet3plus_output_sup0_activation_IoU: 0.9081 - val_unet3plus_output_sup1_activation_accuracy: 0.9537 - val_unet3plus_output_sup1_activation_IoU: 0.3767 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.1089 - val_unet3plus_output_sup3_activation_accuracy: 0.9146 - val_unet3plus_output_sup3_activation_IoU: 0.0590 - val_unet3plus_output_final_activation_accuracy: 0.9876 - val_unet3plus_output_final_activation_IoU: 0.9092 - lr: 1.0000e-04\n",
      "Epoch 20/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6767 - unet3plus_output_sup0_activation_loss: 0.0096 - unet3plus_output_sup1_activation_loss: 0.1333 - unet3plus_output_sup2_activation_loss: 0.2480 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0095 - unet3plus_output_sup0_activation_accuracy: 0.9900 - unet3plus_output_sup0_activation_IoU: 0.9307 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3844 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1101 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0591 - unet3plus_output_final_activation_accuracy: 0.9900 - unet3plus_output_final_activation_IoU: 0.9310\n",
      "Epoch 20: val_loss did not improve from 0.70817\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6767 - unet3plus_output_sup0_activation_loss: 0.0096 - unet3plus_output_sup1_activation_loss: 0.1333 - unet3plus_output_sup2_activation_loss: 0.2480 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0095 - unet3plus_output_sup0_activation_accuracy: 0.9900 - unet3plus_output_sup0_activation_IoU: 0.9307 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3844 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1101 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0591 - unet3plus_output_final_activation_accuracy: 0.9900 - unet3plus_output_final_activation_IoU: 0.9310 - val_loss: 0.7144 - val_unet3plus_output_sup0_activation_loss: 0.0235 - val_unet3plus_output_sup1_activation_loss: 0.1397 - val_unet3plus_output_sup2_activation_loss: 0.2505 - val_unet3plus_output_sup3_activation_loss: 0.2781 - val_unet3plus_output_final_activation_loss: 0.0226 - val_unet3plus_output_sup0_activation_accuracy: 0.9870 - val_unet3plus_output_sup0_activation_IoU: 0.9019 - val_unet3plus_output_sup1_activation_accuracy: 0.9534 - val_unet3plus_output_sup1_activation_IoU: 0.3796 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.1092 - val_unet3plus_output_sup3_activation_accuracy: 0.9146 - val_unet3plus_output_sup3_activation_IoU: 0.0590 - val_unet3plus_output_final_activation_accuracy: 0.9872 - val_unet3plus_output_final_activation_IoU: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 21/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6736 - unet3plus_output_sup0_activation_loss: 0.0085 - unet3plus_output_sup1_activation_loss: 0.1327 - unet3plus_output_sup2_activation_loss: 0.2478 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0084 - unet3plus_output_sup0_activation_accuracy: 0.9903 - unet3plus_output_sup0_activation_IoU: 0.9369 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3881 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1106 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0589 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9371\n",
      "Epoch 21: val_loss did not improve from 0.70817\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6736 - unet3plus_output_sup0_activation_loss: 0.0085 - unet3plus_output_sup1_activation_loss: 0.1327 - unet3plus_output_sup2_activation_loss: 0.2478 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0084 - unet3plus_output_sup0_activation_accuracy: 0.9903 - unet3plus_output_sup0_activation_IoU: 0.9369 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3881 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1106 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0589 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9371 - val_loss: 0.7183 - val_unet3plus_output_sup0_activation_loss: 0.0234 - val_unet3plus_output_sup1_activation_loss: 0.1401 - val_unet3plus_output_sup2_activation_loss: 0.2506 - val_unet3plus_output_sup3_activation_loss: 0.2782 - val_unet3plus_output_final_activation_loss: 0.0260 - val_unet3plus_output_sup0_activation_accuracy: 0.9874 - val_unet3plus_output_sup0_activation_IoU: 0.9064 - val_unet3plus_output_sup1_activation_accuracy: 0.9535 - val_unet3plus_output_sup1_activation_IoU: 0.3769 - val_unet3plus_output_sup2_activation_accuracy: 0.9223 - val_unet3plus_output_sup2_activation_IoU: 0.1077 - val_unet3plus_output_sup3_activation_accuracy: 0.9145 - val_unet3plus_output_sup3_activation_IoU: 0.0576 - val_unet3plus_output_final_activation_accuracy: 0.9873 - val_unet3plus_output_final_activation_IoU: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 22/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6730 - unet3plus_output_sup0_activation_loss: 0.0083 - unet3plus_output_sup1_activation_loss: 0.1325 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0083 - unet3plus_output_sup0_activation_accuracy: 0.9904 - unet3plus_output_sup0_activation_IoU: 0.9381 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3898 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1109 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9381\n",
      "Epoch 22: val_loss did not improve from 0.70817\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6730 - unet3plus_output_sup0_activation_loss: 0.0083 - unet3plus_output_sup1_activation_loss: 0.1325 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0083 - unet3plus_output_sup0_activation_accuracy: 0.9904 - unet3plus_output_sup0_activation_IoU: 0.9381 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3898 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1109 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0588 - unet3plus_output_final_activation_accuracy: 0.9904 - unet3plus_output_final_activation_IoU: 0.9381 - val_loss: 0.7349 - val_unet3plus_output_sup0_activation_loss: 0.0316 - val_unet3plus_output_sup1_activation_loss: 0.1420 - val_unet3plus_output_sup2_activation_loss: 0.2516 - val_unet3plus_output_sup3_activation_loss: 0.2784 - val_unet3plus_output_final_activation_loss: 0.0312 - val_unet3plus_output_sup0_activation_accuracy: 0.9857 - val_unet3plus_output_sup0_activation_IoU: 0.8782 - val_unet3plus_output_sup1_activation_accuracy: 0.9528 - val_unet3plus_output_sup1_activation_IoU: 0.3719 - val_unet3plus_output_sup2_activation_accuracy: 0.9219 - val_unet3plus_output_sup2_activation_IoU: 0.1049 - val_unet3plus_output_sup3_activation_accuracy: 0.9144 - val_unet3plus_output_sup3_activation_IoU: 0.0572 - val_unet3plus_output_final_activation_accuracy: 0.9857 - val_unet3plus_output_final_activation_IoU: 0.8805 - lr: 1.0000e-04\n",
      "Epoch 23/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6764 - unet3plus_output_sup0_activation_loss: 0.0095 - unet3plus_output_sup1_activation_loss: 0.1332 - unet3plus_output_sup2_activation_loss: 0.2480 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0094 - unet3plus_output_sup0_activation_accuracy: 0.9900 - unet3plus_output_sup0_activation_IoU: 0.9311 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3886 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1104 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0585 - unet3plus_output_final_activation_accuracy: 0.9900 - unet3plus_output_final_activation_IoU: 0.9315\n",
      "Epoch 23: val_loss improved from 0.70817 to 0.70789, saving model to B1GC-Kfoldno2-unet3plus_VL-V1-BFRHL.keras\n",
      "1581/1581 [==============================] - 325s 206ms/step - loss: 0.6764 - unet3plus_output_sup0_activation_loss: 0.0095 - unet3plus_output_sup1_activation_loss: 0.1332 - unet3plus_output_sup2_activation_loss: 0.2480 - unet3plus_output_sup3_activation_loss: 0.2763 - unet3plus_output_final_activation_loss: 0.0094 - unet3plus_output_sup0_activation_accuracy: 0.9900 - unet3plus_output_sup0_activation_IoU: 0.9311 - unet3plus_output_sup1_activation_accuracy: 0.9552 - unet3plus_output_sup1_activation_IoU: 0.3886 - unet3plus_output_sup2_activation_accuracy: 0.9232 - unet3plus_output_sup2_activation_IoU: 0.1104 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0585 - unet3plus_output_final_activation_accuracy: 0.9900 - unet3plus_output_final_activation_IoU: 0.9315 - val_loss: 0.7079 - val_unet3plus_output_sup0_activation_loss: 0.0202 - val_unet3plus_output_sup1_activation_loss: 0.1391 - val_unet3plus_output_sup2_activation_loss: 0.2506 - val_unet3plus_output_sup3_activation_loss: 0.2781 - val_unet3plus_output_final_activation_loss: 0.0200 - val_unet3plus_output_sup0_activation_accuracy: 0.9870 - val_unet3plus_output_sup0_activation_IoU: 0.8951 - val_unet3plus_output_sup1_activation_accuracy: 0.9533 - val_unet3plus_output_sup1_activation_IoU: 0.3784 - val_unet3plus_output_sup2_activation_accuracy: 0.9222 - val_unet3plus_output_sup2_activation_IoU: 0.1085 - val_unet3plus_output_sup3_activation_accuracy: 0.9145 - val_unet3plus_output_sup3_activation_IoU: 0.0580 - val_unet3plus_output_final_activation_accuracy: 0.9870 - val_unet3plus_output_final_activation_IoU: 0.8953 - lr: 1.0000e-04\n",
      "Epoch 24/60\n",
      "1581/1581 [==============================] - ETA: 0s - loss: 0.6726 - unet3plus_output_sup0_activation_loss: 0.0081 - unet3plus_output_sup1_activation_loss: 0.1325 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0081 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9387 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3911 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1109 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9393\n",
      "Epoch 24: val_loss did not improve from 0.70789\n",
      "1581/1581 [==============================] - 325s 205ms/step - loss: 0.6726 - unet3plus_output_sup0_activation_loss: 0.0081 - unet3plus_output_sup1_activation_loss: 0.1325 - unet3plus_output_sup2_activation_loss: 0.2477 - unet3plus_output_sup3_activation_loss: 0.2762 - unet3plus_output_final_activation_loss: 0.0081 - unet3plus_output_sup0_activation_accuracy: 0.9905 - unet3plus_output_sup0_activation_IoU: 0.9387 - unet3plus_output_sup1_activation_accuracy: 0.9555 - unet3plus_output_sup1_activation_IoU: 0.3911 - unet3plus_output_sup2_activation_accuracy: 0.9233 - unet3plus_output_sup2_activation_IoU: 0.1109 - unet3plus_output_sup3_activation_accuracy: 0.9153 - unet3plus_output_sup3_activation_IoU: 0.0586 - unet3plus_output_final_activation_accuracy: 0.9905 - unet3plus_output_final_activation_IoU: 0.9393 - val_loss: 0.7199 - val_unet3plus_output_sup0_activation_loss: 0.0255 - val_unet3plus_output_sup1_activation_loss: 0.1404 - val_unet3plus_output_sup2_activation_loss: 0.2508 - val_unet3plus_output_sup3_activation_loss: 0.2783 - val_unet3plus_output_final_activation_loss: 0.0250 - val_unet3plus_output_sup0_activation_accuracy: 0.9874 - val_unet3plus_output_sup0_activation_IoU: 0.9084 - val_unet3plus_output_sup1_activation_accuracy: 0.9535 - val_unet3plus_output_sup1_activation_IoU: 0.3812 - val_unet3plus_output_sup2_activation_accuracy: 0.9222 - val_unet3plus_output_sup2_activation_IoU: 0.1083 - val_unet3plus_output_sup3_activation_accuracy: 0.9145 - val_unet3plus_output_sup3_activation_IoU: 0.0576 - val_unet3plus_output_final_activation_accuracy: 0.9874 - val_unet3plus_output_final_activation_IoU: 0.9103 - lr: 1.0000e-04\n",
      "Epoch 25/60\n",
      " 514/1581 [========>.....................] - ETA: 3:21 - loss: 0.6739 - unet3plus_output_sup0_activation_loss: 0.0073 - unet3plus_output_sup1_activation_loss: 0.1327 - unet3plus_output_sup2_activation_loss: 0.2489 - unet3plus_output_sup3_activation_loss: 0.2777 - unet3plus_output_final_activation_loss: 0.0072 - unet3plus_output_sup0_activation_accuracy: 0.9906 - unet3plus_output_sup0_activation_IoU: 0.9421 - unet3plus_output_sup1_activation_accuracy: 0.9553 - unet3plus_output_sup1_activation_IoU: 0.3929 - unet3plus_output_sup2_activation_accuracy: 0.9227 - unet3plus_output_sup2_activation_IoU: 0.1114 - unet3plus_output_sup3_activation_accuracy: 0.9147 - unet3plus_output_sup3_activation_IoU: 0.0589 - unet3plus_output_final_activation_accuracy: 0.9907 - unet3plus_output_final_activation_IoU: 0.9425"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Fit data to model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m Unet_plus_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#append evaluation values for every fold to a list\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Increase fold number\u001b[39;00m\n\u001b[0;32m     49\u001b[0m fold_no \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras_unet_collection.losses import focal_tversky\n",
    "\n",
    "\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.0000001, verbose=1),\n",
    "    ModelCheckpoint(f'B1GC-Kfoldno{fold_no}-unet3plus_VL-V1-BFRHL.keras', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1GC-Kfoldno{fold_no}-unet3plus_VL-V1-BFRHL.csv', separator=',', append=False)]\n",
    "\n",
    "  # Define the model architecture\n",
    "  # unet_plus_2d require depth >= 2\n",
    "  model = unet_3plus_2d((256, 256, 3), n_labels=num_labels, filter_num_down=[64, 128, 256, 512, 1024], \n",
    "                             filter_num_skip='auto', filter_num_aggregate='auto', \n",
    "                             stack_num_down=2, stack_num_up=2, activation='ReLU', output_activation='Sigmoid',\n",
    "                             batch_norm=True, pool=True, unpool=False, deep_supervision=True, name='unet3plus')\n",
    "  # Compile the model\n",
    "  model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate = 1e-4), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  # Create a generator\n",
    "  train_datagen = ImageDataGenerator()\n",
    "  val_datagen = ImageDataGenerator()\n",
    "\n",
    "  # Create flow from directory or from arrays\n",
    "  train_generator = train_datagen.flow(image_dataset[train], mask_dataset[train], batch_size=1)\n",
    "  val_generator = val_datagen.flow(image_dataset[test], mask_dataset[test], batch_size=1)\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  Unet_plus_history = model.fit(train_generator, \n",
    "                    verbose=1,\n",
    "                    batch_size = 1,\n",
    "                    validation_data=val_generator, \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "  \n",
    "  #append evaluation values for every fold to a list\n",
    "  #acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  #loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  #IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "  # clear session\n",
    "  clear_session()\n",
    "\n",
    "fold_no = 1\n",
    "#determine best fold\n",
    "best_fold(\"unet3plus_BFlh\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with Trans_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.3340 - IoU: 0.6415 - dice_score: 0.7494\n",
      "Epoch 1: val_loss improved from inf to 0.91395, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 61s 240ms/step - loss: 0.3340 - IoU: 0.6415 - dice_score: 0.7494 - val_loss: 0.9139 - val_IoU: 0.0600 - val_dice_score: 0.1131 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.2680 - IoU: 0.7213 - dice_score: 0.8060\n",
      "Epoch 2: val_loss did not improve from 0.91395\n",
      "228/228 [==============================] - 47s 207ms/step - loss: 0.2680 - IoU: 0.7213 - dice_score: 0.8060 - val_loss: 0.9663 - val_IoU: 0.0229 - val_dice_score: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.2445 - IoU: 0.7523 - dice_score: 0.8213\n",
      "Epoch 3: val_loss improved from 0.91395 to 0.88240, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 242ms/step - loss: 0.2445 - IoU: 0.7523 - dice_score: 0.8213 - val_loss: 0.8824 - val_IoU: 0.0942 - val_dice_score: 0.1467 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1982 - IoU: 0.7997 - dice_score: 0.8667\n",
      "Epoch 4: val_loss improved from 0.88240 to 0.83634, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 239ms/step - loss: 0.1982 - IoU: 0.7997 - dice_score: 0.8667 - val_loss: 0.8363 - val_IoU: 0.1352 - val_dice_score: 0.2012 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1644 - IoU: 0.8365 - dice_score: 0.8975\n",
      "Epoch 5: val_loss improved from 0.83634 to 0.63233, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 240ms/step - loss: 0.1644 - IoU: 0.8365 - dice_score: 0.8975 - val_loss: 0.6323 - val_IoU: 0.3202 - val_dice_score: 0.4398 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1593 - IoU: 0.8422 - dice_score: 0.9004\n",
      "Epoch 6: val_loss did not improve from 0.63233\n",
      "228/228 [==============================] - 49s 211ms/step - loss: 0.1593 - IoU: 0.8422 - dice_score: 0.9004 - val_loss: 0.7326 - val_IoU: 0.2322 - val_dice_score: 0.3199 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1455 - IoU: 0.8580 - dice_score: 0.9121\n",
      "Epoch 7: val_loss did not improve from 0.63233\n",
      "228/228 [==============================] - 48s 212ms/step - loss: 0.1455 - IoU: 0.8580 - dice_score: 0.9121 - val_loss: 0.6475 - val_IoU: 0.3025 - val_dice_score: 0.4250 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1314 - IoU: 0.8741 - dice_score: 0.9234\n",
      "Epoch 8: val_loss improved from 0.63233 to 0.43420, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 243ms/step - loss: 0.1314 - IoU: 0.8741 - dice_score: 0.9234 - val_loss: 0.4342 - val_IoU: 0.5147 - val_dice_score: 0.6613 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1266 - IoU: 0.8799 - dice_score: 0.9265\n",
      "Epoch 9: val_loss improved from 0.43420 to 0.38632, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 241ms/step - loss: 0.1266 - IoU: 0.8799 - dice_score: 0.9265 - val_loss: 0.3863 - val_IoU: 0.5656 - val_dice_score: 0.7128 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1229 - IoU: 0.8837 - dice_score: 0.9297\n",
      "Epoch 10: val_loss improved from 0.38632 to 0.37946, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 240ms/step - loss: 0.1229 - IoU: 0.8837 - dice_score: 0.9297 - val_loss: 0.3795 - val_IoU: 0.5723 - val_dice_score: 0.7210 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1126 - IoU: 0.8951 - dice_score: 0.9382\n",
      "Epoch 11: val_loss did not improve from 0.37946\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.1126 - IoU: 0.8951 - dice_score: 0.9382 - val_loss: 0.3937 - val_IoU: 0.5543 - val_dice_score: 0.7091 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1106 - IoU: 0.8974 - dice_score: 0.9397\n",
      "Epoch 12: val_loss improved from 0.37946 to 0.24079, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 242ms/step - loss: 0.1106 - IoU: 0.8974 - dice_score: 0.9397 - val_loss: 0.2408 - val_IoU: 0.7399 - val_dice_score: 0.8476 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1049 - IoU: 0.9035 - dice_score: 0.9439\n",
      "Epoch 13: val_loss improved from 0.24079 to 0.17752, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 239ms/step - loss: 0.1049 - IoU: 0.9035 - dice_score: 0.9439 - val_loss: 0.1775 - val_IoU: 0.8190 - val_dice_score: 0.8936 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1063 - IoU: 0.9017 - dice_score: 0.9430\n",
      "Epoch 14: val_loss did not improve from 0.17752\n",
      "228/228 [==============================] - 49s 212ms/step - loss: 0.1063 - IoU: 0.9017 - dice_score: 0.9430 - val_loss: 0.1811 - val_IoU: 0.8147 - val_dice_score: 0.8927 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1047 - IoU: 0.9037 - dice_score: 0.9444\n",
      "Epoch 15: val_loss improved from 0.17752 to 0.17483, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 239ms/step - loss: 0.1047 - IoU: 0.9037 - dice_score: 0.9444 - val_loss: 0.1748 - val_IoU: 0.8221 - val_dice_score: 0.8963 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1004 - IoU: 0.9088 - dice_score: 0.9468\n",
      "Epoch 16: val_loss improved from 0.17483 to 0.16592, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 56s 242ms/step - loss: 0.1004 - IoU: 0.9088 - dice_score: 0.9468 - val_loss: 0.1659 - val_IoU: 0.8324 - val_dice_score: 0.9031 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1010 - IoU: 0.9078 - dice_score: 0.9463\n",
      "Epoch 17: val_loss improved from 0.16592 to 0.16320, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 239ms/step - loss: 0.1010 - IoU: 0.9078 - dice_score: 0.9463 - val_loss: 0.1632 - val_IoU: 0.8357 - val_dice_score: 0.9041 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.1019 - IoU: 0.9066 - dice_score: 0.9455\n",
      "Epoch 18: val_loss improved from 0.16320 to 0.15287, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 240ms/step - loss: 0.1019 - IoU: 0.9066 - dice_score: 0.9455 - val_loss: 0.1529 - val_IoU: 0.8478 - val_dice_score: 0.9146 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0985 - IoU: 0.9104 - dice_score: 0.9483\n",
      "Epoch 19: val_loss improved from 0.15287 to 0.13632, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 235ms/step - loss: 0.0985 - IoU: 0.9104 - dice_score: 0.9483 - val_loss: 0.1363 - val_IoU: 0.8673 - val_dice_score: 0.9257 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0936 - IoU: 0.9161 - dice_score: 0.9518\n",
      "Epoch 20: val_loss improved from 0.13632 to 0.13344, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 236ms/step - loss: 0.0936 - IoU: 0.9161 - dice_score: 0.9518 - val_loss: 0.1334 - val_IoU: 0.8705 - val_dice_score: 0.9279 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0900 - IoU: 0.9201 - dice_score: 0.9539\n",
      "Epoch 21: val_loss improved from 0.13344 to 0.12257, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0900 - IoU: 0.9201 - dice_score: 0.9539 - val_loss: 0.1226 - val_IoU: 0.8830 - val_dice_score: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0881 - IoU: 0.9218 - dice_score: 0.9552\n",
      "Epoch 22: val_loss improved from 0.12257 to 0.11557, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 236ms/step - loss: 0.0881 - IoU: 0.9218 - dice_score: 0.9552 - val_loss: 0.1156 - val_IoU: 0.8911 - val_dice_score: 0.9405 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0832 - IoU: 0.9274 - dice_score: 0.9590\n",
      "Epoch 23: val_loss did not improve from 0.11557\n",
      "228/228 [==============================] - 47s 206ms/step - loss: 0.0832 - IoU: 0.9274 - dice_score: 0.9590 - val_loss: 0.1157 - val_IoU: 0.8911 - val_dice_score: 0.9405 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0814 - IoU: 0.9291 - dice_score: 0.9601\n",
      "Epoch 24: val_loss did not improve from 0.11557\n",
      "228/228 [==============================] - 47s 206ms/step - loss: 0.0814 - IoU: 0.9291 - dice_score: 0.9601 - val_loss: 0.1190 - val_IoU: 0.8872 - val_dice_score: 0.9377 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0790 - IoU: 0.9317 - dice_score: 0.9617\n",
      "Epoch 25: val_loss improved from 0.11557 to 0.11546, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 57s 250ms/step - loss: 0.0790 - IoU: 0.9317 - dice_score: 0.9617 - val_loss: 0.1155 - val_IoU: 0.8908 - val_dice_score: 0.9389 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0760 - IoU: 0.9352 - dice_score: 0.9638\n",
      "Epoch 26: val_loss improved from 0.11546 to 0.11519, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 239ms/step - loss: 0.0760 - IoU: 0.9352 - dice_score: 0.9638 - val_loss: 0.1152 - val_IoU: 0.8913 - val_dice_score: 0.9404 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0729 - IoU: 0.9384 - dice_score: 0.9660\n",
      "Epoch 27: val_loss did not improve from 0.11519\n",
      "228/228 [==============================] - 47s 207ms/step - loss: 0.0729 - IoU: 0.9384 - dice_score: 0.9660 - val_loss: 0.1247 - val_IoU: 0.8802 - val_dice_score: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0689 - IoU: 0.9427 - dice_score: 0.9686\n",
      "Epoch 28: val_loss did not improve from 0.11519\n",
      "228/228 [==============================] - 47s 206ms/step - loss: 0.0689 - IoU: 0.9427 - dice_score: 0.9686 - val_loss: 0.1237 - val_IoU: 0.8811 - val_dice_score: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0685 - IoU: 0.9431 - dice_score: 0.9689\n",
      "Epoch 29: val_loss improved from 0.11519 to 0.11493, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 239ms/step - loss: 0.0685 - IoU: 0.9431 - dice_score: 0.9689 - val_loss: 0.1149 - val_IoU: 0.8906 - val_dice_score: 0.9395 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0679 - IoU: 0.9440 - dice_score: 0.9695\n",
      "Epoch 30: val_loss improved from 0.11493 to 0.11064, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 239ms/step - loss: 0.0679 - IoU: 0.9440 - dice_score: 0.9695 - val_loss: 0.1106 - val_IoU: 0.8960 - val_dice_score: 0.9429 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0684 - IoU: 0.9434 - dice_score: 0.9695\n",
      "Epoch 31: val_loss improved from 0.11064 to 0.10408, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0684 - IoU: 0.9434 - dice_score: 0.9695 - val_loss: 0.1041 - val_IoU: 0.9045 - val_dice_score: 0.9487 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0641 - IoU: 0.9483 - dice_score: 0.9725\n",
      "Epoch 32: val_loss did not improve from 0.10408\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0641 - IoU: 0.9483 - dice_score: 0.9725 - val_loss: 0.1092 - val_IoU: 0.8986 - val_dice_score: 0.9452 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0618 - IoU: 0.9508 - dice_score: 0.9740\n",
      "Epoch 33: val_loss improved from 0.10408 to 0.10150, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0618 - IoU: 0.9508 - dice_score: 0.9740 - val_loss: 0.1015 - val_IoU: 0.9068 - val_dice_score: 0.9497 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0673 - IoU: 0.9447 - dice_score: 0.9703\n",
      "Epoch 34: val_loss did not improve from 0.10150\n",
      "228/228 [==============================] - 48s 211ms/step - loss: 0.0673 - IoU: 0.9447 - dice_score: 0.9703 - val_loss: 0.1021 - val_IoU: 0.9069 - val_dice_score: 0.9497 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0630 - IoU: 0.9494 - dice_score: 0.9731\n",
      "Epoch 35: val_loss did not improve from 0.10150\n",
      "228/228 [==============================] - 49s 213ms/step - loss: 0.0630 - IoU: 0.9494 - dice_score: 0.9731 - val_loss: 0.1047 - val_IoU: 0.9040 - val_dice_score: 0.9486 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0622 - IoU: 0.9502 - dice_score: 0.9736\n",
      "Epoch 36: val_loss did not improve from 0.10150\n",
      "228/228 [==============================] - 48s 212ms/step - loss: 0.0622 - IoU: 0.9502 - dice_score: 0.9736 - val_loss: 0.1022 - val_IoU: 0.9063 - val_dice_score: 0.9495 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0583 - IoU: 0.9544 - dice_score: 0.9760\n",
      "Epoch 37: val_loss did not improve from 0.10150\n",
      "228/228 [==============================] - 48s 211ms/step - loss: 0.0583 - IoU: 0.9544 - dice_score: 0.9760 - val_loss: 0.1051 - val_IoU: 0.9034 - val_dice_score: 0.9482 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0598 - IoU: 0.9528 - dice_score: 0.9752\n",
      "Epoch 38: val_loss did not improve from 0.10150\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0598 - IoU: 0.9528 - dice_score: 0.9752 - val_loss: 0.1114 - val_IoU: 0.8958 - val_dice_score: 0.9428 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0599 - IoU: 0.9529 - dice_score: 0.9750\n",
      "Epoch 39: val_loss improved from 0.10150 to 0.09303, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 242ms/step - loss: 0.0599 - IoU: 0.9529 - dice_score: 0.9750 - val_loss: 0.0930 - val_IoU: 0.9168 - val_dice_score: 0.9556 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0574 - IoU: 0.9554 - dice_score: 0.9765\n",
      "Epoch 40: val_loss did not improve from 0.09303\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0574 - IoU: 0.9554 - dice_score: 0.9765 - val_loss: 0.0979 - val_IoU: 0.9111 - val_dice_score: 0.9523 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0562 - IoU: 0.9567 - dice_score: 0.9773\n",
      "Epoch 41: val_loss improved from 0.09303 to 0.09209, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 240ms/step - loss: 0.0562 - IoU: 0.9567 - dice_score: 0.9773 - val_loss: 0.0921 - val_IoU: 0.9181 - val_dice_score: 0.9564 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0551 - IoU: 0.9578 - dice_score: 0.9780\n",
      "Epoch 42: val_loss improved from 0.09209 to 0.09090, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 240ms/step - loss: 0.0551 - IoU: 0.9578 - dice_score: 0.9780 - val_loss: 0.0909 - val_IoU: 0.9198 - val_dice_score: 0.9576 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0533 - IoU: 0.9596 - dice_score: 0.9790\n",
      "Epoch 43: val_loss did not improve from 0.09090\n",
      "228/228 [==============================] - 49s 212ms/step - loss: 0.0533 - IoU: 0.9596 - dice_score: 0.9790 - val_loss: 0.0952 - val_IoU: 0.9147 - val_dice_score: 0.9546 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0554 - IoU: 0.9573 - dice_score: 0.9776\n",
      "Epoch 44: val_loss did not improve from 0.09090\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0554 - IoU: 0.9573 - dice_score: 0.9776 - val_loss: 0.0974 - val_IoU: 0.9121 - val_dice_score: 0.9531 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0548 - IoU: 0.9578 - dice_score: 0.9778\n",
      "Epoch 45: val_loss did not improve from 0.09090\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0548 - IoU: 0.9578 - dice_score: 0.9778 - val_loss: 0.0948 - val_IoU: 0.9151 - val_dice_score: 0.9549 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0537 - IoU: 0.9591 - dice_score: 0.9787\n",
      "Epoch 46: val_loss improved from 0.09090 to 0.08753, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0537 - IoU: 0.9591 - dice_score: 0.9787 - val_loss: 0.0875 - val_IoU: 0.9229 - val_dice_score: 0.9591 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0545 - IoU: 0.9581 - dice_score: 0.9781\n",
      "Epoch 47: val_loss did not improve from 0.08753\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0545 - IoU: 0.9581 - dice_score: 0.9781 - val_loss: 0.0898 - val_IoU: 0.9204 - val_dice_score: 0.9576 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0541 - IoU: 0.9584 - dice_score: 0.9782\n",
      "Epoch 48: val_loss improved from 0.08753 to 0.08641, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0541 - IoU: 0.9584 - dice_score: 0.9782 - val_loss: 0.0864 - val_IoU: 0.9241 - val_dice_score: 0.9597 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0511 - IoU: 0.9617 - dice_score: 0.9800\n",
      "Epoch 49: val_loss improved from 0.08641 to 0.08135, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 238ms/step - loss: 0.0511 - IoU: 0.9617 - dice_score: 0.9800 - val_loss: 0.0814 - val_IoU: 0.9294 - val_dice_score: 0.9625 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0513 - IoU: 0.9615 - dice_score: 0.9799\n",
      "Epoch 50: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0513 - IoU: 0.9615 - dice_score: 0.9799 - val_loss: 0.0824 - val_IoU: 0.9291 - val_dice_score: 0.9626 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0497 - IoU: 0.9631 - dice_score: 0.9808\n",
      "Epoch 51: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0497 - IoU: 0.9631 - dice_score: 0.9808 - val_loss: 0.0960 - val_IoU: 0.9144 - val_dice_score: 0.9548 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0538 - IoU: 0.9586 - dice_score: 0.9784\n",
      "Epoch 52: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0538 - IoU: 0.9586 - dice_score: 0.9784 - val_loss: 0.0839 - val_IoU: 0.9273 - val_dice_score: 0.9616 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0545 - IoU: 0.9578 - dice_score: 0.9779\n",
      "Epoch 53: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0545 - IoU: 0.9578 - dice_score: 0.9779 - val_loss: 0.0824 - val_IoU: 0.9285 - val_dice_score: 0.9622 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0539 - IoU: 0.9584 - dice_score: 0.9782\n",
      "Epoch 54: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0539 - IoU: 0.9584 - dice_score: 0.9782 - val_loss: 0.0967 - val_IoU: 0.9126 - val_dice_score: 0.9531 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0536 - IoU: 0.9586 - dice_score: 0.9782\n",
      "Epoch 55: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0536 - IoU: 0.9586 - dice_score: 0.9782 - val_loss: 0.0936 - val_IoU: 0.9160 - val_dice_score: 0.9549 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0515 - IoU: 0.9610 - dice_score: 0.9795\n",
      "Epoch 56: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0515 - IoU: 0.9610 - dice_score: 0.9795 - val_loss: 0.0863 - val_IoU: 0.9242 - val_dice_score: 0.9594 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0505 - IoU: 0.9620 - dice_score: 0.9801\n",
      "Epoch 57: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0505 - IoU: 0.9620 - dice_score: 0.9801 - val_loss: 0.0904 - val_IoU: 0.9205 - val_dice_score: 0.9579 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0506 - IoU: 0.9620 - dice_score: 0.9802\n",
      "Epoch 58: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0506 - IoU: 0.9620 - dice_score: 0.9802 - val_loss: 0.0856 - val_IoU: 0.9259 - val_dice_score: 0.9610 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0511 - IoU: 0.9614 - dice_score: 0.9799\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.08135\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0511 - IoU: 0.9614 - dice_score: 0.9799 - val_loss: 0.0828 - val_IoU: 0.9283 - val_dice_score: 0.9621 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0566 - IoU: 0.9564 - dice_score: 0.9773\n",
      "Epoch 60: val_loss improved from 0.08135 to 0.06952, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0566 - IoU: 0.9564 - dice_score: 0.9773 - val_loss: 0.0695 - val_IoU: 0.9426 - val_dice_score: 0.9700 - lr: 1.0000e-05\n",
      "Epoch 61/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0506 - IoU: 0.9623 - dice_score: 0.9804\n",
      "Epoch 61: val_loss improved from 0.06952 to 0.06538, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0506 - IoU: 0.9623 - dice_score: 0.9804 - val_loss: 0.0654 - val_IoU: 0.9469 - val_dice_score: 0.9724 - lr: 1.0000e-05\n",
      "Epoch 62/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0475 - IoU: 0.9653 - dice_score: 0.9820\n",
      "Epoch 62: val_loss improved from 0.06538 to 0.06402, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 240ms/step - loss: 0.0475 - IoU: 0.9653 - dice_score: 0.9820 - val_loss: 0.0640 - val_IoU: 0.9482 - val_dice_score: 0.9730 - lr: 1.0000e-05\n",
      "Epoch 63/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0453 - IoU: 0.9673 - dice_score: 0.9830\n",
      "Epoch 63: val_loss improved from 0.06402 to 0.06317, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0453 - IoU: 0.9673 - dice_score: 0.9830 - val_loss: 0.0632 - val_IoU: 0.9490 - val_dice_score: 0.9735 - lr: 1.0000e-05\n",
      "Epoch 64/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0438 - IoU: 0.9687 - dice_score: 0.9838\n",
      "Epoch 64: val_loss improved from 0.06317 to 0.06283, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0438 - IoU: 0.9687 - dice_score: 0.9838 - val_loss: 0.0628 - val_IoU: 0.9493 - val_dice_score: 0.9736 - lr: 1.0000e-05\n",
      "Epoch 65/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0426 - IoU: 0.9698 - dice_score: 0.9843\n",
      "Epoch 65: val_loss improved from 0.06283 to 0.06247, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 236ms/step - loss: 0.0426 - IoU: 0.9698 - dice_score: 0.9843 - val_loss: 0.0625 - val_IoU: 0.9497 - val_dice_score: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 66/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0417 - IoU: 0.9707 - dice_score: 0.9848\n",
      "Epoch 66: val_loss improved from 0.06247 to 0.06244, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0417 - IoU: 0.9707 - dice_score: 0.9848 - val_loss: 0.0624 - val_IoU: 0.9496 - val_dice_score: 0.9737 - lr: 1.0000e-05\n",
      "Epoch 67/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0408 - IoU: 0.9714 - dice_score: 0.9852\n",
      "Epoch 67: val_loss did not improve from 0.06244\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0408 - IoU: 0.9714 - dice_score: 0.9852 - val_loss: 0.0625 - val_IoU: 0.9495 - val_dice_score: 0.9736 - lr: 1.0000e-05\n",
      "Epoch 68/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0401 - IoU: 0.9721 - dice_score: 0.9855\n",
      "Epoch 68: val_loss did not improve from 0.06244\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0401 - IoU: 0.9721 - dice_score: 0.9855 - val_loss: 0.0626 - val_IoU: 0.9494 - val_dice_score: 0.9735 - lr: 1.0000e-05\n",
      "Epoch 69/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0395 - IoU: 0.9726 - dice_score: 0.9858\n",
      "Epoch 69: val_loss did not improve from 0.06244\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0395 - IoU: 0.9726 - dice_score: 0.9858 - val_loss: 0.0625 - val_IoU: 0.9494 - val_dice_score: 0.9735 - lr: 1.0000e-05\n",
      "Epoch 70/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0390 - IoU: 0.9731 - dice_score: 0.9861\n",
      "Epoch 70: val_loss improved from 0.06244 to 0.06238, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0390 - IoU: 0.9731 - dice_score: 0.9861 - val_loss: 0.0624 - val_IoU: 0.9495 - val_dice_score: 0.9736 - lr: 1.0000e-05\n",
      "Epoch 71/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0385 - IoU: 0.9735 - dice_score: 0.9863\n",
      "Epoch 71: val_loss improved from 0.06238 to 0.06225, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0385 - IoU: 0.9735 - dice_score: 0.9863 - val_loss: 0.0622 - val_IoU: 0.9496 - val_dice_score: 0.9737 - lr: 1.0000e-05\n",
      "Epoch 72/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0381 - IoU: 0.9739 - dice_score: 0.9865\n",
      "Epoch 72: val_loss did not improve from 0.06225\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0381 - IoU: 0.9739 - dice_score: 0.9865 - val_loss: 0.0623 - val_IoU: 0.9495 - val_dice_score: 0.9736 - lr: 1.0000e-05\n",
      "Epoch 73/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0377 - IoU: 0.9742 - dice_score: 0.9867\n",
      "Epoch 73: val_loss improved from 0.06225 to 0.06202, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0377 - IoU: 0.9742 - dice_score: 0.9867 - val_loss: 0.0620 - val_IoU: 0.9498 - val_dice_score: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 74/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0373 - IoU: 0.9746 - dice_score: 0.9869\n",
      "Epoch 74: val_loss did not improve from 0.06202\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0373 - IoU: 0.9746 - dice_score: 0.9869 - val_loss: 0.0621 - val_IoU: 0.9497 - val_dice_score: 0.9737 - lr: 1.0000e-05\n",
      "Epoch 75/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0369 - IoU: 0.9749 - dice_score: 0.9870\n",
      "Epoch 75: val_loss improved from 0.06202 to 0.06165, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0369 - IoU: 0.9749 - dice_score: 0.9870 - val_loss: 0.0617 - val_IoU: 0.9502 - val_dice_score: 0.9739 - lr: 1.0000e-05\n",
      "Epoch 76/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0366 - IoU: 0.9752 - dice_score: 0.9872\n",
      "Epoch 76: val_loss improved from 0.06165 to 0.06147, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 239ms/step - loss: 0.0366 - IoU: 0.9752 - dice_score: 0.9872 - val_loss: 0.0615 - val_IoU: 0.9503 - val_dice_score: 0.9740 - lr: 1.0000e-05\n",
      "Epoch 77/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0364 - IoU: 0.9754 - dice_score: 0.9873\n",
      "Epoch 77: val_loss did not improve from 0.06147\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0364 - IoU: 0.9754 - dice_score: 0.9873 - val_loss: 0.0619 - val_IoU: 0.9499 - val_dice_score: 0.9737 - lr: 1.0000e-05\n",
      "Epoch 78/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0362 - IoU: 0.9756 - dice_score: 0.9874\n",
      "Epoch 78: val_loss improved from 0.06147 to 0.06132, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0362 - IoU: 0.9756 - dice_score: 0.9874 - val_loss: 0.0613 - val_IoU: 0.9505 - val_dice_score: 0.9741 - lr: 1.0000e-05\n",
      "Epoch 79/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0359 - IoU: 0.9758 - dice_score: 0.9875\n",
      "Epoch 79: val_loss improved from 0.06132 to 0.06084, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0359 - IoU: 0.9758 - dice_score: 0.9875 - val_loss: 0.0608 - val_IoU: 0.9510 - val_dice_score: 0.9744 - lr: 1.0000e-05\n",
      "Epoch 80/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0358 - IoU: 0.9759 - dice_score: 0.9876\n",
      "Epoch 80: val_loss did not improve from 0.06084\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0358 - IoU: 0.9759 - dice_score: 0.9876 - val_loss: 0.0617 - val_IoU: 0.9500 - val_dice_score: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 81/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0356 - IoU: 0.9761 - dice_score: 0.9877\n",
      "Epoch 81: val_loss did not improve from 0.06084\n",
      "228/228 [==============================] - 47s 208ms/step - loss: 0.0356 - IoU: 0.9761 - dice_score: 0.9877 - val_loss: 0.0618 - val_IoU: 0.9499 - val_dice_score: 0.9737 - lr: 1.0000e-05\n",
      "Epoch 82/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0354 - IoU: 0.9762 - dice_score: 0.9877\n",
      "Epoch 82: val_loss improved from 0.06084 to 0.06078, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0354 - IoU: 0.9762 - dice_score: 0.9877 - val_loss: 0.0608 - val_IoU: 0.9511 - val_dice_score: 0.9744 - lr: 1.0000e-05\n",
      "Epoch 83/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0353 - IoU: 0.9764 - dice_score: 0.9878\n",
      "Epoch 83: val_loss improved from 0.06078 to 0.06067, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 237ms/step - loss: 0.0353 - IoU: 0.9764 - dice_score: 0.9878 - val_loss: 0.0607 - val_IoU: 0.9511 - val_dice_score: 0.9744 - lr: 1.0000e-05\n",
      "Epoch 84/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0350 - IoU: 0.9766 - dice_score: 0.9879\n",
      "Epoch 84: val_loss improved from 0.06067 to 0.06050, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0350 - IoU: 0.9766 - dice_score: 0.9879 - val_loss: 0.0605 - val_IoU: 0.9513 - val_dice_score: 0.9745 - lr: 1.0000e-05\n",
      "Epoch 85/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0351 - IoU: 0.9765 - dice_score: 0.9879\n",
      "Epoch 85: val_loss did not improve from 0.06050\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0351 - IoU: 0.9765 - dice_score: 0.9879 - val_loss: 0.0610 - val_IoU: 0.9507 - val_dice_score: 0.9742 - lr: 1.0000e-05\n",
      "Epoch 86/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0348 - IoU: 0.9768 - dice_score: 0.9880\n",
      "Epoch 86: val_loss improved from 0.06050 to 0.06041, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0348 - IoU: 0.9768 - dice_score: 0.9880 - val_loss: 0.0604 - val_IoU: 0.9514 - val_dice_score: 0.9746 - lr: 1.0000e-05\n",
      "Epoch 87/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0345 - IoU: 0.9770 - dice_score: 0.9882\n",
      "Epoch 87: val_loss improved from 0.06041 to 0.06036, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 236ms/step - loss: 0.0345 - IoU: 0.9770 - dice_score: 0.9882 - val_loss: 0.0604 - val_IoU: 0.9514 - val_dice_score: 0.9745 - lr: 1.0000e-05\n",
      "Epoch 88/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0344 - IoU: 0.9771 - dice_score: 0.9882\n",
      "Epoch 88: val_loss did not improve from 0.06036\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0344 - IoU: 0.9771 - dice_score: 0.9882 - val_loss: 0.0604 - val_IoU: 0.9514 - val_dice_score: 0.9745 - lr: 1.0000e-05\n",
      "Epoch 89/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0343 - IoU: 0.9772 - dice_score: 0.9883\n",
      "Epoch 89: val_loss did not improve from 0.06036\n",
      "228/228 [==============================] - 47s 208ms/step - loss: 0.0343 - IoU: 0.9772 - dice_score: 0.9883 - val_loss: 0.0605 - val_IoU: 0.9512 - val_dice_score: 0.9745 - lr: 1.0000e-05\n",
      "Epoch 90/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0342 - IoU: 0.9773 - dice_score: 0.9883\n",
      "Epoch 90: val_loss did not improve from 0.06036\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0342 - IoU: 0.9773 - dice_score: 0.9883 - val_loss: 0.0608 - val_IoU: 0.9509 - val_dice_score: 0.9743 - lr: 1.0000e-05\n",
      "Epoch 91/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0342 - IoU: 0.9773 - dice_score: 0.9883\n",
      "Epoch 91: val_loss improved from 0.06036 to 0.06005, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0342 - IoU: 0.9773 - dice_score: 0.9883 - val_loss: 0.0601 - val_IoU: 0.9517 - val_dice_score: 0.9747 - lr: 1.0000e-05\n",
      "Epoch 92/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0338 - IoU: 0.9776 - dice_score: 0.9885\n",
      "Epoch 92: val_loss did not improve from 0.06005\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0338 - IoU: 0.9776 - dice_score: 0.9885 - val_loss: 0.0607 - val_IoU: 0.9510 - val_dice_score: 0.9743 - lr: 1.0000e-05\n",
      "Epoch 93/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0337 - IoU: 0.9778 - dice_score: 0.9886\n",
      "Epoch 93: val_loss did not improve from 0.06005\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0337 - IoU: 0.9778 - dice_score: 0.9886 - val_loss: 0.0603 - val_IoU: 0.9514 - val_dice_score: 0.9746 - lr: 1.0000e-05\n",
      "Epoch 94/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0336 - IoU: 0.9779 - dice_score: 0.9886\n",
      "Epoch 94: val_loss improved from 0.06005 to 0.05997, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 239ms/step - loss: 0.0336 - IoU: 0.9779 - dice_score: 0.9886 - val_loss: 0.0600 - val_IoU: 0.9519 - val_dice_score: 0.9748 - lr: 1.0000e-05\n",
      "Epoch 95/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0335 - IoU: 0.9779 - dice_score: 0.9887\n",
      "Epoch 95: val_loss did not improve from 0.05997\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0335 - IoU: 0.9779 - dice_score: 0.9887 - val_loss: 0.0604 - val_IoU: 0.9513 - val_dice_score: 0.9745 - lr: 1.0000e-05\n",
      "Epoch 96/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0334 - IoU: 0.9780 - dice_score: 0.9887\n",
      "Epoch 96: val_loss did not improve from 0.05997\n",
      "228/228 [==============================] - 48s 208ms/step - loss: 0.0334 - IoU: 0.9780 - dice_score: 0.9887 - val_loss: 0.0610 - val_IoU: 0.9505 - val_dice_score: 0.9740 - lr: 1.0000e-05\n",
      "Epoch 97/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0334 - IoU: 0.9780 - dice_score: 0.9887\n",
      "Epoch 97: val_loss improved from 0.05997 to 0.05975, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 54s 238ms/step - loss: 0.0334 - IoU: 0.9780 - dice_score: 0.9887 - val_loss: 0.0597 - val_IoU: 0.9520 - val_dice_score: 0.9749 - lr: 1.0000e-05\n",
      "Epoch 98/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0336 - IoU: 0.9778 - dice_score: 0.9886\n",
      "Epoch 98: val_loss improved from 0.05975 to 0.05940, saving model to B1VL-Kfoldno2-transunet_VL_Volume_FTL.h5\n",
      "228/228 [==============================] - 55s 239ms/step - loss: 0.0336 - IoU: 0.9778 - dice_score: 0.9886 - val_loss: 0.0594 - val_IoU: 0.9524 - val_dice_score: 0.9751 - lr: 1.0000e-05\n",
      "Epoch 99/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0332 - IoU: 0.9782 - dice_score: 0.9888\n",
      "Epoch 99: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0332 - IoU: 0.9782 - dice_score: 0.9888 - val_loss: 0.0609 - val_IoU: 0.9507 - val_dice_score: 0.9741 - lr: 1.0000e-05\n",
      "Epoch 100/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0331 - IoU: 0.9783 - dice_score: 0.9888\n",
      "Epoch 100: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0331 - IoU: 0.9783 - dice_score: 0.9888 - val_loss: 0.0599 - val_IoU: 0.9517 - val_dice_score: 0.9747 - lr: 1.0000e-05\n",
      "Epoch 101/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0329 - IoU: 0.9784 - dice_score: 0.9889\n",
      "Epoch 101: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0329 - IoU: 0.9784 - dice_score: 0.9889 - val_loss: 0.0598 - val_IoU: 0.9520 - val_dice_score: 0.9749 - lr: 1.0000e-05\n",
      "Epoch 102/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0329 - IoU: 0.9785 - dice_score: 0.9889\n",
      "Epoch 102: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0329 - IoU: 0.9785 - dice_score: 0.9889 - val_loss: 0.0595 - val_IoU: 0.9523 - val_dice_score: 0.9750 - lr: 1.0000e-05\n",
      "Epoch 103/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0328 - IoU: 0.9785 - dice_score: 0.9890\n",
      "Epoch 103: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0328 - IoU: 0.9785 - dice_score: 0.9890 - val_loss: 0.0611 - val_IoU: 0.9505 - val_dice_score: 0.9740 - lr: 1.0000e-05\n",
      "Epoch 104/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0328 - IoU: 0.9786 - dice_score: 0.9890\n",
      "Epoch 104: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0328 - IoU: 0.9786 - dice_score: 0.9890 - val_loss: 0.0603 - val_IoU: 0.9513 - val_dice_score: 0.9745 - lr: 1.0000e-05\n",
      "Epoch 105/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0327 - IoU: 0.9787 - dice_score: 0.9890\n",
      "Epoch 105: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0327 - IoU: 0.9787 - dice_score: 0.9890 - val_loss: 0.0598 - val_IoU: 0.9520 - val_dice_score: 0.9749 - lr: 1.0000e-05\n",
      "Epoch 106/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0327 - IoU: 0.9786 - dice_score: 0.9890\n",
      "Epoch 106: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0327 - IoU: 0.9786 - dice_score: 0.9890 - val_loss: 0.0598 - val_IoU: 0.9520 - val_dice_score: 0.9749 - lr: 1.0000e-05\n",
      "Epoch 107/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0326 - IoU: 0.9787 - dice_score: 0.9891\n",
      "Epoch 107: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0326 - IoU: 0.9787 - dice_score: 0.9891 - val_loss: 0.0602 - val_IoU: 0.9514 - val_dice_score: 0.9746 - lr: 1.0000e-05\n",
      "Epoch 108/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0330 - IoU: 0.9783 - dice_score: 0.9888\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0330 - IoU: 0.9783 - dice_score: 0.9888 - val_loss: 0.0607 - val_IoU: 0.9508 - val_dice_score: 0.9742 - lr: 1.0000e-05\n",
      "Epoch 109/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0331 - IoU: 0.9782 - dice_score: 0.9888\n",
      "Epoch 109: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0331 - IoU: 0.9782 - dice_score: 0.9888 - val_loss: 0.0598 - val_IoU: 0.9518 - val_dice_score: 0.9747 - lr: 1.0000e-06\n",
      "Epoch 110/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0325 - IoU: 0.9787 - dice_score: 0.9891\n",
      "Epoch 110: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 211ms/step - loss: 0.0325 - IoU: 0.9787 - dice_score: 0.9891 - val_loss: 0.0597 - val_IoU: 0.9519 - val_dice_score: 0.9748 - lr: 1.0000e-06\n",
      "Epoch 111/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0323 - IoU: 0.9789 - dice_score: 0.9892\n",
      "Epoch 111: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0323 - IoU: 0.9789 - dice_score: 0.9892 - val_loss: 0.0596 - val_IoU: 0.9521 - val_dice_score: 0.9749 - lr: 1.0000e-06\n",
      "Epoch 112/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0322 - IoU: 0.9791 - dice_score: 0.9892\n",
      "Epoch 112: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0322 - IoU: 0.9791 - dice_score: 0.9892 - val_loss: 0.0595 - val_IoU: 0.9522 - val_dice_score: 0.9750 - lr: 1.0000e-06\n",
      "Epoch 113/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0321 - IoU: 0.9792 - dice_score: 0.9893\n",
      "Epoch 113: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0321 - IoU: 0.9792 - dice_score: 0.9893 - val_loss: 0.0595 - val_IoU: 0.9522 - val_dice_score: 0.9750 - lr: 1.0000e-06\n",
      "Epoch 114/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0320 - IoU: 0.9792 - dice_score: 0.9893\n",
      "Epoch 114: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0320 - IoU: 0.9792 - dice_score: 0.9893 - val_loss: 0.0595 - val_IoU: 0.9522 - val_dice_score: 0.9750 - lr: 1.0000e-06\n",
      "Epoch 115/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0320 - IoU: 0.9793 - dice_score: 0.9894\n",
      "Epoch 115: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0320 - IoU: 0.9793 - dice_score: 0.9894 - val_loss: 0.0595 - val_IoU: 0.9522 - val_dice_score: 0.9750 - lr: 1.0000e-06\n",
      "Epoch 116/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0319 - IoU: 0.9793 - dice_score: 0.9894\n",
      "Epoch 116: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 210ms/step - loss: 0.0319 - IoU: 0.9793 - dice_score: 0.9894 - val_loss: 0.0595 - val_IoU: 0.9522 - val_dice_score: 0.9750 - lr: 1.0000e-06\n",
      "Epoch 117/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0319 - IoU: 0.9793 - dice_score: 0.9894\n",
      "Epoch 117: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0319 - IoU: 0.9793 - dice_score: 0.9894 - val_loss: 0.0595 - val_IoU: 0.9522 - val_dice_score: 0.9750 - lr: 1.0000e-06\n",
      "Epoch 118/200\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.0319 - IoU: 0.9793 - dice_score: 0.9894\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.05940\n",
      "228/228 [==============================] - 48s 209ms/step - loss: 0.0319 - IoU: 0.9793 - dice_score: 0.9894 - val_loss: 0.0595 - val_IoU: 0.9522 - val_dice_score: 0.9750 - lr: 1.0000e-06\n",
      "Epoch 118: early stopping\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.0502 - IoU: 0.9634 - dice_score: 0.9814\n",
      "2/2 [==============================] - 2s 987ms/step - loss: 0.0502 - IoU: 0.9634 - dice_score: 0.9814\n",
      "2/2 [==============================] - 2s 955ms/step - loss: 0.0502 - IoU: 0.9634 - dice_score: 0.9814\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.3796 - IoU: 0.5852 - dice_score: 0.7103\n",
      "Epoch 1: val_loss improved from inf to 0.94530, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n",
      "229/229 [==============================] - 61s 245ms/step - loss: 0.3796 - IoU: 0.5852 - dice_score: 0.7103 - val_loss: 0.9453 - val_IoU: 0.0375 - val_dice_score: 0.0723 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2954 - IoU: 0.6861 - dice_score: 0.7859\n",
      "Epoch 2: val_loss improved from 0.94530 to 0.91943, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n",
      "229/229 [==============================] - 54s 235ms/step - loss: 0.2954 - IoU: 0.6861 - dice_score: 0.7859 - val_loss: 0.9194 - val_IoU: 0.0594 - val_dice_score: 0.1038 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2533 - IoU: 0.7380 - dice_score: 0.8195\n",
      "Epoch 3: val_loss improved from 0.91943 to 0.85102, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n",
      "229/229 [==============================] - 60s 261ms/step - loss: 0.2533 - IoU: 0.7380 - dice_score: 0.8195 - val_loss: 0.8510 - val_IoU: 0.1265 - val_dice_score: 0.1807 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.2058 - IoU: 0.7906 - dice_score: 0.8618\n",
      "Epoch 4: val_loss improved from 0.85102 to 0.82288, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n",
      "229/229 [==============================] - 56s 245ms/step - loss: 0.2058 - IoU: 0.7906 - dice_score: 0.8618 - val_loss: 0.8229 - val_IoU: 0.1536 - val_dice_score: 0.2123 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1736 - IoU: 0.8265 - dice_score: 0.8899\n",
      "Epoch 5: val_loss did not improve from 0.82288\n",
      "229/229 [==============================] - 49s 213ms/step - loss: 0.1736 - IoU: 0.8265 - dice_score: 0.8899 - val_loss: 0.8671 - val_IoU: 0.1076 - val_dice_score: 0.1649 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1519 - IoU: 0.8508 - dice_score: 0.9077\n",
      "Epoch 6: val_loss improved from 0.82288 to 0.59160, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n",
      "229/229 [==============================] - 58s 251ms/step - loss: 0.1519 - IoU: 0.8508 - dice_score: 0.9077 - val_loss: 0.5916 - val_IoU: 0.3539 - val_dice_score: 0.4895 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1443 - IoU: 0.8592 - dice_score: 0.9141\n",
      "Epoch 7: val_loss improved from 0.59160 to 0.53971, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n",
      "229/229 [==============================] - 58s 253ms/step - loss: 0.1443 - IoU: 0.8592 - dice_score: 0.9141 - val_loss: 0.5397 - val_IoU: 0.4072 - val_dice_score: 0.5455 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1336 - IoU: 0.8718 - dice_score: 0.9213\n",
      "Epoch 8: val_loss improved from 0.53971 to 0.36878, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n",
      "229/229 [==============================] - 59s 258ms/step - loss: 0.1336 - IoU: 0.8718 - dice_score: 0.9213 - val_loss: 0.3688 - val_IoU: 0.5899 - val_dice_score: 0.7267 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.1296 - IoU: 0.8764 - dice_score: 0.9243\n",
      "Epoch 9: val_loss improved from 0.36878 to 0.35753, saving model to B1VL-Kfoldno3-transunet_VL_Volume_FTL.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#fit model to data\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m transunet_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#append evaluation values for every fold to a list\u001b[39;00m\n\u001b[0;32m     44\u001b[0m acc_per_fold\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mevaluate(image_dataset[test], mask_dataset[test])[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\engine\\training.py:1624\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m     }\n\u001b[0;32m   1622\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1624\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:448\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    446\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 448\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:1463\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs_since_last_save \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\callbacks.py:1528\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[1;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\n\u001b[0;32m   1523\u001b[0m             filepath,\n\u001b[0;32m   1524\u001b[0m             overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1525\u001b[0m             options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options,\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[0;32m   1527\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1528\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\engine\\training.py:2698\u001b[0m, in \u001b[0;36mModel.save\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m   2643\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m   2644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\n\u001b[0;32m   2645\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2652\u001b[0m     save_traces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2653\u001b[0m ):\n\u001b[0;32m   2655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\u001b[39;00m\n\u001b[0;32m   2656\u001b[0m \n\u001b[0;32m   2657\u001b[0m \u001b[38;5;124;03m    Please see `tf.keras.models.save_model` or the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2698\u001b[0m     \u001b[43msave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2699\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2702\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2703\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2704\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2705\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2706\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\saving\\save.py:161\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    151\u001b[0m         model, sequential\u001b[38;5;241m.\u001b[39mSequential\n\u001b[0;32m    152\u001b[0m     ):\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) or using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    160\u001b[0m         )\n\u001b[1;32m--> 161\u001b[0m     \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model_to_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mSharedObjectSavingScope():\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\saving\\hdf5_format.py:141\u001b[0m, in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    131\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDF5 format does not save weights of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `optimizer_experimental.Optimizer`, your optimizer will\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be recompiled at loading time.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    137\u001b[0m         include_optimizer\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model\u001b[38;5;241m.\u001b[39moptimizer, optimizer_v1\u001b[38;5;241m.\u001b[39mTFOptimizer)\n\u001b[0;32m    140\u001b[0m     ):\n\u001b[1;32m--> 141\u001b[0m         \u001b[43msave_optimizer_weights_to_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     f\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\saving\\hdf5_format.py:695\u001b[0m, in \u001b[0;36msave_optimizer_weights_to_hdf5_group\u001b[1;34m(hdf5_group, optimizer)\u001b[0m\n\u001b[0;32m    693\u001b[0m     param_dset[()] \u001b[38;5;241m=\u001b[39m val\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     \u001b[43mparam_dset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\h5py\\_hl\\dataset.py:1009\u001b[0m, in \u001b[0;36mDataset.__setitem__\u001b[1;34m(self, args, val)\u001b[0m\n\u001b[0;32m   1007\u001b[0m mspace \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(selection\u001b[38;5;241m.\u001b[39mexpand_shape(mshape))\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fspace \u001b[38;5;129;01min\u001b[39;00m selection\u001b[38;5;241m.\u001b[39mbroadcast(mshape):\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import _model_transunet_2d\n",
    "importlib.reload(_model_transunet_2d)\n",
    "from _model_transunet_2d import transunet_2d\n",
    "from keras_unet_collection.losses import focal_tversky\n",
    "\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  #clear_session()\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=20, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.0000001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-transunet_VL_Volume_FTL.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-transunet_VL_Volume_FTL.csv', separator=',', append=False)]\n",
    "  \n",
    "  #define the model architecture\n",
    "  model = transunet_2d((256, 256, 3), filter_num=[64, 128, 256, 512, 1024], \n",
    "                       backbone='VGG19',\n",
    "                          n_labels=1, stack_num_down=2, stack_num_up=2, \n",
    "                          embed_dim=768, num_mlp=3072, num_heads=12, num_transformer=12, \n",
    "                          activation='ReLU', mlp_activation='GELU', output_activation='Sigmoid', #output activation from Softmax to Sigmoid\n",
    "                          batch_norm=True, pool=True, unpool=False, name='transunet')\n",
    "                          #batchnorm to true, unpool to false\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss=focal_tversky, optimizer=Adam(learning_rate = 1e-4), \n",
    "              metrics=[IoU, dice_score])\n",
    "\n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model to data\n",
    "  transunet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = 1,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "  clear_session()\n",
    "\n",
    "fold_no = 1\n",
    "#determine best fold\n",
    "best_fold(\"transunet_BFlh.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with Swin_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/200\n",
      "1934/1934 [==============================] - ETA: 0s - loss: 0.9919 - IoU: 0.0032 - dice_score: 0.0064\n",
      "Epoch 1: val_loss improved from inf to 0.99168, saving model to K-foldno1-swinunet_VL.keras\n",
      "1934/1934 [==============================] - 276s 138ms/step - loss: 0.9919 - IoU: 0.0032 - dice_score: 0.0064 - val_loss: 0.9917 - val_IoU: 0.0033 - val_dice_score: 0.0067 - lr: 1.0000e-06\n",
      "Epoch 2/200\n",
      "1934/1934 [==============================] - ETA: 0s - loss: 0.9892 - IoU: 0.0043 - dice_score: 0.0087\n",
      "Epoch 2: val_loss improved from 0.99168 to 0.98435, saving model to K-foldno1-swinunet_VL.keras\n",
      "1934/1934 [==============================] - 265s 137ms/step - loss: 0.9892 - IoU: 0.0043 - dice_score: 0.0087 - val_loss: 0.9844 - val_IoU: 0.0064 - val_dice_score: 0.0127 - lr: 1.0000e-06\n",
      "Epoch 3/200\n",
      "1934/1934 [==============================] - ETA: 0s - loss: 0.9719 - IoU: 0.0117 - dice_score: 0.0233\n",
      "Epoch 3: val_loss improved from 0.98435 to 0.96596, saving model to K-foldno1-swinunet_VL.keras\n",
      "1934/1934 [==============================] - 263s 136ms/step - loss: 0.9719 - IoU: 0.0117 - dice_score: 0.0233 - val_loss: 0.9660 - val_IoU: 0.0143 - val_dice_score: 0.0283 - lr: 1.0000e-06\n",
      "Epoch 4/200\n",
      "1559/1934 [=======================>......] - ETA: 47s - loss: 0.9551 - IoU: 0.0196 - dice_score: 0.0389"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#fit model on data\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m swin_unet_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#append evaluation values for every fold to a list\u001b[39;00m\n\u001b[0;32m     50\u001b[0m acc_per_fold\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mevaluate(image_dataset[test], mask_dataset[test])[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DL_Track_US\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import _model_swin_unet_2d\n",
    "importlib.reload(_model_swin_unet_2d)\n",
    "from _model_swin_unet_2d import swin_unet_2d\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  clear_session()\n",
    "  \n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=20, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00000001, verbose=1),\n",
    "    ModelCheckpoint(f'K-foldno{fold_no}-swinunet_VL.keras', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'K-foldno{fold_no}-swinunet_VL.csv', separator=',', append=False)]\n",
    "  \n",
    "  #define the model architecture\n",
    "  #this model requires depth >= 2\n",
    "  model = swin_unet_2d((256, 256, 3), filter_num_begin=64, n_labels=1, depth=4, stack_num_down=2, stack_num_up=2,\n",
    "                            patch_size=(8, 8), num_heads=[4, 8, 16, 16], window_size=[8, 4, 4, 4], num_mlp=512, \n",
    "                            output_activation='Sigmoid', shift_window=False, name='swin_unet') #Guess: Shift_window = False\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss=focal_tversky_loss, optimizer=Adam(learning_rate = 1e-6), \n",
    "              metrics=[IoU, dice_score])\n",
    "\n",
    "  # # Create a generator\n",
    "  # train_datagen = ImageDataGenerator()\n",
    "  # val_datagen = ImageDataGenerator()\n",
    "\n",
    "  # # Create flow from directory or from arrays\n",
    "  # train_generator = train_datagen.flow(image_dataset[train], mask_dataset[train], batch_size=batch_size)\n",
    "  # val_generator = val_datagen.flow(image_dataset[test], mask_dataset[test], batch_size=batch_size)\n",
    "  \n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model on data\n",
    "  swin_unet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = 1,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "#determine best fold\n",
    "best_fold(\"swin_unet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16-UNet with DL-Kfold (build_vgg16_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now working fine as is, but should be run on a restarted kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of aponeurosis images =  314\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      " 72/126 [================>.............] - ETA: 2:42 - loss: 0.6685 - accuracy: 0.5600 - IoU: 0.1895"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 220\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Fit data to model\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Increase fold number\u001b[39;00m\n\u001b[0;32m    225\u001b[0m fold_no \u001b[38;5;241m=\u001b[39m fold_no \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\diego\\miniconda3\\envs\\DeepACSA\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_unet_collection.losses import focal_tversky\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.applications import VGG16 \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "def IoU(y_true, y_pred, dtype=tf.float32):\n",
    "    y_pred = tf.cast(y_pred, dtype)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    y_pred = tf.squeeze(y_pred)\n",
    "    y_true = tf.squeeze(y_true)\n",
    "\n",
    "    y_true_pos = tf.reshape(y_true, [-1])\n",
    "    y_pred_pos = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    area_intersect = tf.reduce_sum(tf.multiply(y_true_pos, y_pred_pos))\n",
    "    \n",
    "    area_true = tf.reduce_sum(y_true_pos)\n",
    "    area_pred = tf.reduce_sum(y_pred_pos)\n",
    "    area_union = area_true + area_pred - area_intersect\n",
    "    \n",
    "    # Return the IoU score\n",
    "    return tf.math.divide_no_nan(area_intersect, area_union)\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n",
    "    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n",
    "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
    "\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
    "    tv = tversky(y_true, y_pred)\n",
    "    return K.pow((1 - tv), gamma)\n",
    "\n",
    "\n",
    "# Plot sample of model prediction\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(30, 20))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='Greys_r')\n",
    "    \n",
    "    ax[0].set_title('US-image', c=\"white\" )\n",
    "    ax[0].grid(False)\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), cmap='Greys_r')\n",
    "    ax[1].set_title('Aponeurosis', c=\"white\")\n",
    "    ax[1].grid(False)\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[2].set_title('Apo-Predicted', c=\"white\")\n",
    "    ax[2].grid(False)\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=0.5, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[3].set_title('Apo-Picture binary', c=\"white\")\n",
    "    ax[3].grid(False)\n",
    "    \n",
    "    plt.savefig(str(ix)+\"Pred_area.tif\")\n",
    "\n",
    "\n",
    "def conv_block(inputs, num_filters): # found in model_training\n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(x) #NOTE Carla was right, this should be x. This is a bug! \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs) #32\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_vgg16_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    vgg16 = VGG16(include_top=False, weights=\"imagenet\", input_tensor = inputs)\n",
    "    #vgg16.summary()\n",
    "    \n",
    "    \"\"\" Encoder \"\"\"\n",
    "    \n",
    "    # skip connections\n",
    "    s1 = vgg16.get_layer(\"block1_conv2\").output # 256\n",
    "    s2 = vgg16.get_layer(\"block2_conv2\").output # 128\n",
    "    s3 = vgg16.get_layer(\"block3_conv3\").output # 64\n",
    "    s4 = vgg16.get_layer(\"block4_conv3\").output # 32\n",
    "\n",
    "    \"\"\" Bottleneck/Bridge \"\"\"\n",
    "    \n",
    "    b1 = vgg16.get_layer(\"block5_conv3\").output # 16\n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "    \n",
    "    \"\"\" Outputs \"\"\"\n",
    "    outputs = Conv2D(1, (1, 1), padding = \"same\", activation=\"sigmoid\")(d4) #binary segmentation\n",
    "    model = Model(inputs, outputs, name = \"VGG16_U-Net\")\n",
    "    return model\n",
    "\n",
    "# Images will be re-scaled\n",
    "im_width = 256\n",
    "im_height = 256\n",
    "border = 5\n",
    "\n",
    "image_directory = \"C:/Users/diego/Bac Sport and Computer Science/BA-arbeit/RepoBA/BA-IFSS/data/analyzed_data/008/MZP1/volume\"\n",
    "mask_directory = \"C:/Users/diego/Bac Sport and Computer Science/BA-arbeit/RepoBA/BA-IFSS/data/analyzed_data/008/MZP1/mask\"  \n",
    "\n",
    "# list of all images in the path\n",
    "ids = os.listdir(image_directory)\n",
    "print(\"Total no. of aponeurosis images = \", len(ids))\n",
    "\n",
    "image_dataset = []\n",
    "for path in glob.glob(image_directory):\n",
    "    for img_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img_to_array(img)\n",
    "        img = img/255.0\n",
    "        image_dataset.append(img)  \n",
    "image_dataset = np.array(image_dataset)\n",
    "\n",
    "mask_dataset = []\n",
    "for path in glob.glob(mask_directory):\n",
    "    for mask_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        mask = cv2.resize(mask, (256,256))\n",
    "        mask = img_to_array(mask)\n",
    "        mask = mask/255.0\n",
    "        mask_dataset.append(mask)        \n",
    "mask_dataset = np.array(mask_dataset)\n",
    "\n",
    "# Define some hyperparameters\n",
    "batch_size = 2\n",
    "epochs = 100\n",
    "num_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "#compile the model\n",
    "VGG16_UNet = build_vgg16_unet((256,256,3)) #input_shape is (256, 256, 3)\n",
    "model= VGG16_UNet\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss=\"binary_crossentropy\", metrics= [\"accuracy\", IoU])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=20, verbose=1),\n",
    "        ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.0000001, verbose=1),\n",
    "        ModelCheckpoint(f'K-foldno{fold_no}-VGG16-V1-VL-BFRHL-256.keras', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "        CSVLogger(f'K-foldno{fold_no}-VGG16-V1-VL-BFRHL-256.csv', separator=',', append=False)\n",
    "    ]\n",
    "\n",
    "    # # Create a generator\n",
    "    train_datagen = ImageDataGenerator()\n",
    "    val_datagen = ImageDataGenerator()\n",
    "\n",
    "    # Create flow from directory or from arrays\n",
    "    train_generator = train_datagen.flow(image_dataset[train], mask_dataset[train], batch_size=batch_size)\n",
    "    val_generator = val_datagen.flow(image_dataset[test], mask_dataset[test], batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    results = model.fit(train_generator, batch_size=batch_size, epochs=epochs,\n",
    "                        callbacks=callbacks, validation_data=val_generator)\n",
    "\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net with DL-Kfold (get_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: 'C:/Users/carla/Documents/Master_Thesis/Example_Images/FALLMUD/NeilCronin/training_images_small/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m image_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/carla/Documents/Master_Thesis/Example_Images/FALLMUD/NeilCronin/training_images_small/\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m##VGG16 need seperate induction of path\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# list of all images in the path\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal no. of aponeurosis images = \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ids))\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m#X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m#y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: 'C:/Users/carla/Documents/Master_Thesis/Example_Images/FALLMUD/NeilCronin/training_images_small/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Conv2DTranspose, Input, concatenate, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Convolution block\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), /\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),/\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Create u-net model\n",
    "def get_unet(input_img, n_filters = 64, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    \n",
    "    # Contracting Path\n",
    "    # c is output tensor of conv layers\n",
    "    # p ist output tensor of max pool layers\n",
    "    # u is output tensor of up-sampling (transposed) layers\n",
    "    # Batchnorm standardizes/normalizes the output of each layer where applied in order to avoid huge weights using \n",
    "    # z-scores \n",
    "    \n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Compute Intersection over union (IoU), a measure of labelling accuracy\n",
    "# NOTE: This is sometimes also called Jaccard score\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = tf.reduce_sum(tf.abs(y_true * y_pred), axis=-1)\n",
    "    union = tf.reduce_sum(y_true,-1) + tf.reduce_sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \n",
    "    return 1 - dice_score(y_true, y_pred)\n",
    "\n",
    "def dice_bce_score(y_true, y_pred, smooth=1):    \n",
    "    \n",
    "    BCE =  K.binary_crossentropy(y_true, y_pred)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)    \n",
    "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(y_true, -1) + K.sum(y_pred, -1) + smooth)\n",
    "    Dice_BCE = BCE + dice_loss\n",
    "    \n",
    "    return Dice_BCE\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.8, gamma=2):    \n",
    "      \n",
    "    BCE = K.binary_crossentropy(y_true, y_pred)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    return focal_loss\n",
    "\n",
    "# Plot sample of model prediction\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(30, 20))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='Greys_r')\n",
    "    \n",
    "    ax[0].set_title('US-image', c=\"white\" )\n",
    "    ax[0].grid(False)\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), cmap='Greys_r')\n",
    "    ax[1].set_title('Aponeurosis', c=\"white\")\n",
    "    ax[1].grid(False)\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[2].set_title('Apo-Predicted', c=\"white\")\n",
    "    ax[2].grid(False)\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=0.5, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[3].set_title('Apo-Picture binary', c=\"white\")\n",
    "    ax[3].grid(False)\n",
    "    \n",
    "    plt.savefig(str(ix)+\"Pred_area.tif\")\n",
    "\n",
    "# Save all predictions on disk \n",
    "def save_pred_area(binary_preds): \n",
    "    for i in range(len(binary_preds)): \n",
    "        fig, (ax1)= plt.subplots(1, 1, figsize = (15, 15))\n",
    "        ax1.imshow(binary_preds[i], cmap=\"Greys_r\", interpolation=\"bilinear\")\n",
    "        ax1.set_title(\"Predicted Area\")\n",
    "        plt.savefig(str(i)+\"Pred_area.tif\") # Saves images to directory of notebook\n",
    "\n",
    "# Images will be re-scaled\n",
    "im_width = 512\n",
    "im_height = 512\n",
    "border = 5\n",
    "\n",
    "mask_directory = 'C:/Users/carla/Documents/Master_Thesis/Example_Images/FALLMUD/NeilCronin/training_fascicle_masks_small/' ##VGG16 needs seperate induction of path\n",
    "image_directory = 'C:/Users/carla/Documents/Master_Thesis/Example_Images/FALLMUD/NeilCronin/training_images_small/'   ##VGG16 need seperate induction of path\n",
    "\n",
    "# list of all images in the path\n",
    "ids = os.listdir(image_directory)\n",
    "print(\"Total no. of aponeurosis images = \", len(ids))\n",
    "#X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "#y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "\n",
    "image_dataset = []\n",
    "for path in glob.glob(image_directory):\n",
    "    for img_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img_to_array(img)\n",
    "        img = img/255.0\n",
    "        image_dataset.append(img)  \n",
    "image_dataset = np.array(image_dataset)\n",
    "\n",
    "mask_dataset = []\n",
    "for path in glob.glob(mask_directory):\n",
    "    for mask_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        mask = cv2.resize(mask, (256,256))\n",
    "        mask = img_to_array(mask)\n",
    "        mask = mask/255.0\n",
    "        mask_dataset.append(mask)        \n",
    "mask_dataset = np.array(mask_dataset)\n",
    "\n",
    "# Define some hyperparameters\n",
    "batch_size = 2\n",
    "epochs = 1 #60\n",
    "num_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "# Define per-fold score containers \n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "#compile the model\n",
    "input_shape = (256,256,3)\n",
    "input_img = Input(input_shape)\n",
    "\n",
    "UNet = get_unet(input_img)\n",
    "model_apo = UNet\n",
    "model_apo.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\", IoU])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=8, verbose=1),\n",
    "        ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1),\n",
    "        ModelCheckpoint(f'K-foldno{fold_no}-UNet-V1-BFlh-256.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "        CSVLogger(f'K-foldno{fold_no}-UNet-V1-BFlh-256.csv', separator=',', append=False)\n",
    "    ]\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    results = model_apo.fit(image_dataset[train], mask_dataset[train], batch_size=batch_size, epochs=epochs,\n",
    "                        callbacks=callbacks, validation_data=(image_dataset[test], mask_dataset[test]))\n",
    "\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "fold_no = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with ResUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import _model_resunet_a_2d\n",
    "importlib.reload(_model_resunet_a_2d)\n",
    "from _model_resunet_a_2d import resunet_a_2d\n",
    "\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'K-foldno{fold_no}-resunet-VL-256.keras', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'K-foldno{fold_no}-resunet-VL-256.csv', separator=',', append=False)]\n",
    "\n",
    "  # Define the model architecture\n",
    "  # resunet requires depth >= 2\n",
    "  resunet = resunet_a_2d((256, 256, 3), [64, 128, 256, 512], \n",
    "                            dilation_num=[1, 3, 15, 31], \n",
    "                            n_labels=1, aspp_num_down=256, aspp_num_up=128, \n",
    "                            activation='ReLU', output_activation='Sigmoid', \n",
    "                            batch_norm=True, pool=\"max\", unpool='nearest', name='resunet')\n",
    "  \n",
    "  # Compile the model\n",
    "  resunet.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  resunet_history = resunet.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(resunet.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(resunet.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(resunet.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "#create csv file with per fold and mean scores\n",
    "rows = [f\"acc_per_fold: {acc_per_fold}\",\n",
    "        f\"loss_per_fold: {loss_per_fold}\",\n",
    "        f\"IoU_per_fold: {IoU_per_fold}\",\n",
    "        \"mean_values:\", f\"> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})\", f\"> Loss: {np.mean(loss_per_fold)}\", f\"> IoU: {np.mean(loss_per_fold)}\"]\n",
    "\n",
    "np.savetxt(\"Unet2Plus.csv\",\n",
    "           rows, delimiter=\", \", fmt =\"% s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with r2_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import _model_r2_unet_2d\n",
    "importlib.reload(_model_r2_unet_2d)\n",
    "from _model_r2_unet_2d import r2_unet_2d\n",
    "\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'K-foldno{fold_no}-model_r2-VL-256.keras', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'K-foldno{fold_no}-model_r2-VL-256.csv', separator=',', append=False)]                                  # Give the CSV file a name (.csv)\n",
    "\n",
    "  #define the model architecture\n",
    "  #r2_unet_2d requires depth >= 2\n",
    "  model = r2_unet_2d((256, 256, 3), [64, 128, 256, 512], n_labels=num_labels,\n",
    "                          stack_num_down=2, stack_num_up=1, recur_num=2,\n",
    "                          activation='ReLU', output_activation='Sigmoid', \n",
    "                          batch_norm=True, pool='max', unpool='nearest', name='r2unet')\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model on data\n",
    "  resunet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide average scores for K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]} - IoU: {IoU_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print(f'> IoU: {np.mean(IoU_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Predicted masks have been saved to C:/Users/admin/Documents/DL_Track/Unet_collection/output/images\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_unet_collection.losses import focal_tversky\n",
    "\n",
    "# Set up directories\n",
    "input_directory = 'C:/Users/admin/Documents/DL_Track/Unet_collection/test_images/'  # Directory containing input images\n",
    "output_directory = 'C:/Users/admin/Documents/DL_Track/Unet_collection/output/images'  # Directory where predicted masks will be saved\n",
    "# model = keras.models.load_model('C:/Users/admin/Documents/DL_Track/IFSS_Net/IFSS_Net/IFSS_epoch08_FocalTversky_512.h5')\n",
    "model = keras.models.load_model('C:/Users/admin/Documents/DL_Track/Unet_collection/VGG16-Unet-FocalTversky_epoch100_unaugmented.keras', custom_objects={\"focal_tversky\": focal_tversky, \"IoU\": IoU, \"dice_score\": dice_score})\n",
    "#model = keras.models.load_model('C:/Users/admin/Documents/DL_Track/Models_DL_Track/Final_models/model-fasc-VGG16-BCE-512.h5', custom_objects={\"IoU\": IoU})\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to load and preprocess images from a directory\n",
    "def load_and_preprocess_image(image_path, target_size=(512, 512)):\n",
    "    img = load_img(image_path, target_size=target_size)  # Load the image with target size\n",
    "    img_array = img_to_array(img)  # Convert the image to a numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize the image to [0, 1] range\n",
    "    return img_array\n",
    "\n",
    "# Function to save predicted masks with Pillow\n",
    "def save_predicted_mask(predicted_mask, image_name):\n",
    "    # Apply a threshold to make the mask binary (0 or 1)\n",
    "    binary_mask = (predicted_mask >= 0.05).astype(np.uint8)  # Apply threshold and convert to 0 and 1\n",
    "    \n",
    "    # Rescale the binary mask to [0, 255] for saving as an image\n",
    "    binary_mask = binary_mask * 255\n",
    "    \n",
    "    # Convert the numpy array to a Pillow image\n",
    "    binary_mask_image = Image.fromarray(np.squeeze(binary_mask))\n",
    "    \n",
    "    # Save the image using Pillow\n",
    "    output_path = os.path.join(output_directory, os.path.splitext(image_name)[0] + '.tif')  # Save as .tif\n",
    "    binary_mask_image.save(output_path)\n",
    "\n",
    "# Load images, predict masks, and save results\n",
    "for image_name in os.listdir(input_directory):\n",
    "    if image_name.endswith('.tif') or image_name.endswith('.png'):  # Change to your file extension\n",
    "        image_path = os.path.join(input_directory, image_name)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        input_image = load_and_preprocess_image(image_path)\n",
    "        \n",
    "        # Use the model to predict the mask\n",
    "        predicted_mask = model.predict(input_image)[0]  # [0] to remove batch dimension\n",
    "        \n",
    "        # Save the predicted mask\n",
    "        save_predicted_mask(predicted_mask, image_name)\n",
    "\n",
    "print(\"Predicted masks have been saved to\", output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepACSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
